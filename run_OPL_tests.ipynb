{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "from abc import ABCMeta\n",
    "\n",
    "\n",
    "from obp.ope import (\n",
    "    RegressionModel,\n",
    "    DirectMethod as DM,\n",
    ")\n",
    "\n",
    "from my_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simluation_data_from_pi,\n",
    "    get_train_data,\n",
    "    CFModel,\n",
    "    CustomCFDataset,\n",
    "    NeighborhoodModel\n",
    ")\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(dataset, policy):\n",
    "    return np.array([np.sum(dataset['q_x_a'] * policy.squeeze(), axis=1).mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPWPolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(IPWPolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        # q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        # dm_grads = (scores * policy_prob.detach() * torch.log(policy_prob)).sum(dim=1)\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # reinforce trick step\n",
    "        # reinforce_grad = ((iw * (original_policy_rewards - q_hat_at_position) * log_pi) / iw.sum()) + dm_grads\n",
    "        reinforce_grad = iw * original_policy_rewards * log_pi\n",
    "        \n",
    "        return reinforce_grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNDRPolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(SNDRPolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        dm_reward = (scores * policy_prob.detach()).sum(dim=1)\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # reinforce trick step\n",
    "        r_hat = ((iw * (original_policy_rewards - q_hat_at_position)) / iw.sum()) + dm_reward\n",
    "        reinforce_grad = r_hat * log_pi\n",
    "        return reinforce_grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opl_results_dict(reg_results, conv_results):\n",
    "    reward = conv_results[:, 0]\n",
    "    return    dict(\n",
    "                policy_rewards=np.mean(reward),\n",
    "                ipw=np.mean(abs(conv_results[: ,3] - reward)),\n",
    "                reg_dm=np.mean(abs(reg_results - reward)),\n",
    "                conv_dm=np.mean(abs(conv_results[: ,1] - reward)),\n",
    "                conv_dr=np.mean(abs(conv_results[: ,2] - reward)),\n",
    "                conv_sndr=np.mean(abs(conv_results[: ,4] - reward)),\n",
    "\n",
    "                ipw_var=np.var(conv_results[: ,3]),\n",
    "                reg_dm_var=np.var(reg_results),\n",
    "                conv_dm_var=np.var(conv_results[: ,1]),\n",
    "                conv_dr_var=np.var(conv_results[: ,2]),\n",
    "                conv_sndr_var=np.var(conv_results[: ,4]),\n",
    "\n",
    "                                \n",
    "                # ipw_p_err=np.mean(abs(conv_results[: ,3] - reward) / reward) * 100,\n",
    "                # reg_dm_p_err=np.mean(abs(reg_results - reward) / reward) * 100,\n",
    "                # conv_dm_p_err=np.mean(abs(conv_results[: ,1] - reward) / reward) * 100,\n",
    "                # conv_dr_p_err=np.mean(abs(conv_results[: ,2] - reward) / reward) * 100,\n",
    "                # conv_sndr_p_err=np.mean(abs(conv_results[: ,4] - reward) / reward) * 100,\n",
    "                \n",
    "                action_diff_to_real=np.mean(conv_results[: ,5]),\n",
    "                action_delta=np.mean(conv_results[: ,6]),\n",
    "                context_diff_to_real=np.mean(conv_results[: ,7]),\n",
    "                context_delta=np.mean(conv_results[: ,8])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the training function\n",
    "def train(model, train_loader, neighborhood_model, num_epochs=1, lr=0.0001):\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # here we can change the learning rate\n",
    "    criterion = SNDRPolicyLoss()\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    tq = tqdm(range(num_epochs))\n",
    "    for epoch in tq:\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for user_idx, action_idx, rewards, original_prob in train_loader:\n",
    "            # Move data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                user_idx = user_idx.to(device) \n",
    "                action_idx = action_idx.to(device)\n",
    "                rewards = rewards.to(device)\n",
    "                original_prob = original_prob.to(device) \n",
    "            \n",
    "            # Forward pass\n",
    "            policy = model(user_idx)\n",
    "            pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "            \n",
    "            scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()))\n",
    "            \n",
    "            loss = criterion(\n",
    "                              pscore,\n",
    "                              scores,\n",
    "                              policy, \n",
    "                              rewards, \n",
    "                              action_idx.type(torch.long), \n",
    "                              )\n",
    "            \n",
    "            # Zero the gradients Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()                        \n",
    "            optimizer.step()\n",
    "            \n",
    "            # update neighborhood\n",
    "            # action_emb, context_emb = model.get_params()\n",
    "            \n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "            # Print statistics after each epoch\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            tq.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "        # neighborhood_model.update(action_emb.detach().numpy(), context_emb.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  num_epochs,\n",
    "                  lr=0.001\n",
    "                  ):\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    \n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    first = True\n",
    "    zero = True\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simluation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            dataset[\"n_users\"],\n",
    "                                                            dataset[\"n_actions\"],\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            \n",
    "            regression_model = RegressionModel(\n",
    "                                                n_actions=n_actions,\n",
    "                                                action_context=our_x,\n",
    "                                                base_model=LogisticRegression(random_state=12345)\n",
    "                                                )\n",
    "            \n",
    "            regression_model.fit(train_data['x'], \n",
    "                        train_data['a'],\n",
    "                        train_data['r'],\n",
    "                        original_policy_prob[train_data['x_idx'],\n",
    "                        train_data['a']].squeeze()\n",
    "                        )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob[train_data['x_idx']]\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=False)\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            train(model, train_loader, neighberhoodmodel, num_epochs=num_epochs, lr=lr)\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.4 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [1, 2, 3, 4, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: -0.9668: 100%|██████████| 1/1 [00:00<00:00, 30.35it/s]\n",
      "Epoch [1/1], Loss: -1.1029: 100%|██████████| 1/1 [00:00<00:00, 28.65it/s]\n",
      "Epoch [1/1], Loss: -0.9270: 100%|██████████| 1/1 [00:00<00:00, 21.80it/s]\n",
      "Epoch [1/1], Loss: -0.8795: 100%|██████████| 1/1 [00:00<00:00, 13.19it/s]\n",
      "Epoch [1/1], Loss: -1.0203: 100%|██████████| 1/1 [00:00<00:00, 11.39it/s]\n",
      "Epoch [1/1], Loss: -0.9509: 100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\n",
      "Epoch [1/1], Loss: -0.9584: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=1, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1744</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.5188</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1745</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.5155</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.0076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1747</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.7722</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1748</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.5119</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.7735</td>\n",
       "      <td>0.0169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.5093</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>0.0216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1753</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.5061</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.7742</td>\n",
       "      <td>0.0251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1761</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.4991</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>0.0425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1775</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.4862</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.7987</td>\n",
       "      <td>0.0824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0           0.1744 0.0473  0.0266   0.0031   0.0287     0.0272   \n",
       "1           0.1745 0.0483  0.0268   0.0030   0.0277     0.0275   \n",
       "2           0.1747 0.0173  0.0324   0.0193   0.0303     0.0319   \n",
       "3           0.1748 0.0207  0.0213   0.0084   0.0088     0.0087   \n",
       "4           0.1750 0.0037  0.0256   0.0201   0.0130     0.0122   \n",
       "5           0.1753 0.0252  0.0162   0.0051   0.0278     0.0287   \n",
       "10          0.1761 0.0054  0.0225   0.0103   0.0033     0.0046   \n",
       "20          0.1775 0.0186  0.0358   0.0135   0.0235     0.0246   \n",
       "\n",
       "    action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                0.5188        0.0000                0.7710         0.0000  \n",
       "1                0.5155        0.0117                0.7715         0.0076  \n",
       "2                0.5130        0.0185                0.7722         0.0127  \n",
       "3                0.5119        0.0246                0.7735         0.0169  \n",
       "4                0.5093        0.0286                0.7745         0.0216  \n",
       "5                0.5061        0.0351                0.7742         0.0251  \n",
       "10               0.4991        0.0556                0.7795         0.0425  \n",
       "20               0.4862        0.1060                0.7987         0.0824  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: -0.9665: 100%|██████████| 1/1 [00:00<00:00, 30.38it/s]\n",
      "Epoch [1/1], Loss: -1.0999: 100%|██████████| 1/1 [00:00<00:00, 18.92it/s]\n",
      "Epoch [1/1], Loss: -0.9267: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Epoch [1/1], Loss: -0.8775: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "Epoch [1/1], Loss: -1.0125: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Epoch [1/1], Loss: -0.9388: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\n",
      "Epoch [1/1], Loss: -0.9220: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1744</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5188</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1744</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1745</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5176</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.7712</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1745</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5173</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1745</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5168</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.7716</td>\n",
       "      <td>0.0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5161</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1748</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5144</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.7723</td>\n",
       "      <td>0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1751</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5105</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.7744</td>\n",
       "      <td>0.0147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1744 0.0473  0.0266   0.0031   0.0287     0.0272   0.0000   \n",
       "1           0.1744 0.0475  0.0266   0.0031   0.0285     0.0272   0.0000   \n",
       "2           0.1745 0.0166  0.0322   0.0187   0.0302     0.0314   0.0000   \n",
       "3           0.1745 0.0158  0.0210   0.0075   0.0049     0.0052   0.0000   \n",
       "4           0.1745 0.0009  0.0253   0.0192   0.0091     0.0085   0.0000   \n",
       "5           0.1746 0.0236  0.0161   0.0050   0.0270     0.0270   0.0000   \n",
       "10          0.1748 0.0060  0.0217   0.0091   0.0037     0.0042   0.0000   \n",
       "20          0.1751 0.0114  0.0342   0.0124   0.0151     0.0152   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.5188   \n",
       "1       0.0000       0.0000       0.0000         0.0000               0.5181   \n",
       "2       0.0000       0.0000       0.0000         0.0000               0.5176   \n",
       "3       0.0000       0.0000       0.0000         0.0000               0.5173   \n",
       "4       0.0000       0.0000       0.0000         0.0000               0.5168   \n",
       "5       0.0000       0.0000       0.0000         0.0000               0.5161   \n",
       "10      0.0000       0.0000       0.0000         0.0000               0.5144   \n",
       "20      0.0000       0.0000       0.0000         0.0000               0.5105   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.7710         0.0000  \n",
       "1         0.0023                0.7711         0.0015  \n",
       "2         0.0037                0.7712         0.0025  \n",
       "3         0.0049                0.7715         0.0034  \n",
       "4         0.0057                0.7716         0.0043  \n",
       "5         0.0070                0.7715         0.0050  \n",
       "10        0.0108                0.7723         0.0083  \n",
       "20        0.0198                0.7744         0.0147  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.0445: 100%|██████████| 10/10 [00:00<00:00, 31.56it/s]\n",
      "Epoch [10/10], Loss: -1.2650: 100%|██████████| 10/10 [00:00<00:00, 24.34it/s]\n",
      "Epoch [10/10], Loss: -1.0220: 100%|██████████| 10/10 [00:00<00:00, 15.65it/s]\n",
      "Epoch [10/10], Loss: -0.9963: 100%|██████████| 10/10 [00:00<00:00, 11.38it/s]\n",
      "Epoch [10/10], Loss: -1.3594: 100%|██████████| 10/10 [00:00<00:00, 10.80it/s]\n",
      "Epoch [10/10], Loss: -1.7357: 100%|██████████| 10/10 [00:01<00:00,  5.12it/s]\n",
      "Epoch [10/10], Loss: -7.5669: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=10, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1744</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5188</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5050</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.0506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1765</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.0795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1773</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4968</td>\n",
       "      <td>0.1405</td>\n",
       "      <td>0.8004</td>\n",
       "      <td>0.1064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.1681</td>\n",
       "      <td>0.8189</td>\n",
       "      <td>0.1385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1792</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4909</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.8370</td>\n",
       "      <td>0.1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1740</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>0.3934</td>\n",
       "      <td>1.0519</td>\n",
       "      <td>0.4319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1608</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0416</td>\n",
       "      <td>0.9077</td>\n",
       "      <td>1.8231</td>\n",
       "      <td>1.2566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1744 0.0473  0.0266   0.0031   0.0287     0.0272   0.0000   \n",
       "1           0.1752 0.0533  0.0276   0.0021   0.0214     0.0283   0.0000   \n",
       "2           0.1765 0.0159  0.0336   0.0245   0.0291     0.0317   0.0000   \n",
       "3           0.1773 0.0591  0.0235   0.0164   0.0349     0.0385   0.0000   \n",
       "4           0.1780 0.0276  0.0268   0.0310   0.0385     0.0430   0.0000   \n",
       "5           0.1792 0.0336  0.0066   0.0114   0.0335     0.0413   0.0000   \n",
       "10          0.1740 0.0707  0.0088   0.0273   0.0548     0.0673   0.0000   \n",
       "20          0.1608 0.0010  0.0255   0.0390   0.0556     0.0358   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.5188   \n",
       "1       0.0000       0.0000       0.0000         0.0000               0.5050   \n",
       "2       0.0000       0.0000       0.0000         0.0000               0.4947   \n",
       "3       0.0000       0.0000       0.0000         0.0000               0.4968   \n",
       "4       0.0000       0.0000       0.0000         0.0000               0.4910   \n",
       "5       0.0000       0.0000       0.0000         0.0000               0.4909   \n",
       "10      0.0000       0.0000       0.0000         0.0000               0.5840   \n",
       "20      0.0000       0.0000       0.0000         0.0000               1.0416   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.7710         0.0000  \n",
       "1         0.0683                0.7778         0.0506  \n",
       "2         0.1073                0.7865         0.0795  \n",
       "3         0.1405                0.8004         0.1064  \n",
       "4         0.1681                0.8189         0.1385  \n",
       "5         0.2128                0.8370         0.1792  \n",
       "10        0.3934                1.0519         0.4319  \n",
       "20        0.9077                1.8231         1.2566  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.9529: 100%|██████████| 10/10 [00:00<00:00, 66.40it/s]\n",
      "Epoch [10/10], Loss: -3.4946: 100%|██████████| 10/10 [00:00<00:00, 26.15it/s]\n",
      "Epoch [10/10], Loss: -3.0818: 100%|██████████| 10/10 [00:00<00:00, 15.55it/s]\n",
      "Epoch [10/10], Loss: -5.4128: 100%|██████████| 10/10 [00:00<00:00, 13.93it/s]\n"
     ]
    }
   ],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1744</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5188</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1795</td>\n",
       "      <td>0.2188</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1243</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6322</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.4703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1816</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1107</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8454</td>\n",
       "      <td>0.8170</td>\n",
       "      <td>1.2693</td>\n",
       "      <td>0.8229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1808</td>\n",
       "      <td>0.1679</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.1221</td>\n",
       "      <td>1.0931</td>\n",
       "      <td>1.6491</td>\n",
       "      <td>1.2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4486</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>2.1657</td>\n",
       "      <td>1.7289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1744 0.0473  0.0266   0.0031   0.0287     0.0272   0.0000   \n",
       "1          0.1795 0.2188  0.0337   0.0153   0.0236     0.1243   0.0000   \n",
       "2          0.1816 0.1632  0.0273   0.0351   0.0202     0.1107   0.0000   \n",
       "3          0.1808 0.1679  0.0273   0.0573   0.0683     0.0804   0.0000   \n",
       "4          0.1786 0.1385  0.0199   0.0472   0.0397     0.0303   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.5188   \n",
       "1      0.0000       0.0000       0.0000         0.0000               0.6322   \n",
       "2      0.0000       0.0000       0.0000         0.0000               0.8454   \n",
       "3      0.0000       0.0000       0.0000         0.0000               1.1221   \n",
       "4      0.0000       0.0000       0.0000         0.0000               1.4486   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.7710         0.0000  \n",
       "1        0.4893                0.9652         0.4703  \n",
       "2        0.8170                1.2693         0.8229  \n",
       "3        1.0931                1.6491         1.2093  \n",
       "4        1.4419                2.1657         1.7289  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
