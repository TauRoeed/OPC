{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "from abc import ABCMeta\n",
    "import optuna\n",
    "\n",
    "\n",
    "from from_saito import (\n",
    "    DirectMethod as DM,\n",
    ")\n",
    "\n",
    "from my_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    CFModel,\n",
    "    CustomCFDataset,\n",
    "    NeighborhoodModel,\n",
    "    # BPRModel\n",
    ")\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRModel(nn.Module):\n",
    "    def __init__(self, num_users, num_actions, embedding_dim, \n",
    "                 initial_user_embeddings=None, initial_actions_embeddings=None):\n",
    "        super(BPRModel, self).__init__()\n",
    "\n",
    "        self.actions = torch.arange(num_actions)\n",
    "        self.users = torch.arange(num_users)\n",
    "        \n",
    "        # Initialize user and actions embeddings\n",
    "        if initial_user_embeddings is None:\n",
    "            self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        else:\n",
    "            # If initial embeddings are provided, set them as the embeddings\n",
    "            self.user_embeddings = nn.Embedding.from_pretrained(initial_user_embeddings, freeze=False)\n",
    "        \n",
    "        if initial_actions_embeddings is None:\n",
    "            self.actions_embeddings = nn.Embedding(num_actions, embedding_dim)\n",
    "        else:\n",
    "            # If initial embeddings are provided, set them as the embeddings\n",
    "            self.actions_embeddings = nn.Embedding.from_pretrained(initial_actions_embeddings, freeze=False)\n",
    "\n",
    "\n",
    "    def forward(self, user_ids, pos_action_ids, neg_action_ids):\n",
    "        user_embeds = self.user_embeddings(user_ids)\n",
    "        pos_action_embeds = self.actions_embeddings(pos_action_ids)\n",
    "        neg_action_embeds = self.actions_embeddings(neg_action_ids)\n",
    "\n",
    "        # Compute dot product between user and action embeddings\n",
    "        pos_scores = (user_embeds * pos_action_embeds).sum(dim=1)\n",
    "        neg_scores = (user_embeds * neg_action_embeds).sum(dim=1)\n",
    "\n",
    "        return pos_scores, neg_scores\n",
    "    \n",
    "    def calc_scores(self, user_ids):\n",
    "        # Ensure user_ids is on the same device as the model\n",
    "        device = self.user_embeddings.weight.device\n",
    "        user_ids = user_ids.to(device)\n",
    "\n",
    "        # Get user embeddings\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "\n",
    "        # Ensure self.actions is on the same device\n",
    "        actions = self.actions.to(device)\n",
    "\n",
    "        # Get action embeddings\n",
    "        actions_embedding = self.actions_embeddings(actions)\n",
    "\n",
    "        # Compute dot product scores\n",
    "        scores = user_embedding @ actions_embedding.T\n",
    "\n",
    "        # Return softmaxed scores\n",
    "        return F.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        \n",
    "    def to(self, device):\n",
    "        # Move the module itself\n",
    "        super().to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        self.users = self.users.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `calc_reward` Function\n",
    "Calculates the expected reward of a policy by computing the weighted average of true reward probabilities.\n",
    "\n",
    "### Parameters\n",
    "- `dataset` (dict): Contains dataset information including `q_x_a`, the true reward probabilities for each user-action pair\n",
    "- `policy` (numpy.ndarray): Policy probabilities with shape [n_users, n_actions, 1]\n",
    "\n",
    "### Returns\n",
    "- `numpy.ndarray`: A single-element array containing the expected policy reward\n",
    "\n",
    "### Mathematical Formulation\n",
    "Implements: $R_{gt} = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}{\\pi_{ij} \\cdot p_{ij}}$\n",
    "\n",
    "Where:\n",
    "- $\\pi_{ij}$ is the policy probability for user $i$ choosing action $j$\n",
    "- $p_{ij}$ is the true reward probability for user $i$ choosing action $j$ (stored in `q_x_a`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(dataset, policy):\n",
    "    return np.array([np.sum(dataset['q_x_a'] * policy.squeeze(), axis=1).mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_opl_results_dict` Function\n",
    "\n",
    "This function processes evaluation results from various offline policy learning (OPL) estimators and computes summary statistics.\n",
    "\n",
    "### Parameters\n",
    "- **reg_results** (numpy.ndarray): Results from regression-based direct method estimator\n",
    "- **conv_results** (numpy.ndarray): Results from various estimators including true rewards and embeddings quality metrics\n",
    "\n",
    "### Returns\n",
    "- **dict**: A dictionary containing the following metrics:\n",
    "  - `policy_rewards`: Mean true reward of the learned policy\n",
    "  - Error metrics (absolute difference between estimator and true reward):\n",
    "    - `ipw`: Inverse Propensity Weighting estimator error\n",
    "    - `reg_dm`: Regression-based Direct Method estimator error\n",
    "    - `conv_dm`: Convolution-based Direct Method estimator error\n",
    "    - `conv_dr`: Convolution-based Doubly Robust estimator error\n",
    "    - `conv_sndr`: Convolution-based Self-Normalized Doubly Robust estimator error\n",
    "  - Variance metrics for each estimator:\n",
    "    - `ipw_var`, `reg_dm_var`, `conv_dm_var`, `conv_dr_var`, `conv_sndr_var`\n",
    "  - Embedding quality metrics:\n",
    "    - `action_diff_to_real`: RMSE between learned and real action embeddings\n",
    "    - `action_delta`: RMSE between learned and original action embeddings\n",
    "    - `context_diff_to_real`: RMSE between learned and real context embeddings\n",
    "    - `context_delta`: RMSE between learned and original context embeddings\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses the first column of `conv_results` as the ground truth reward\n",
    "- Contains commented-out code for percentage error calculations\n",
    "- Computes absolute errors rather than signed differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opl_results_dict(reg_results, conv_results):\n",
    "    reward = conv_results[:, 0]\n",
    "    return    dict(\n",
    "                policy_rewards=np.mean(reward),\n",
    "                ipw=np.mean(abs(conv_results[: ,3] - reward)),\n",
    "                reg_dm=np.mean(abs(reg_results - reward)),\n",
    "                conv_dm=np.mean(abs(conv_results[: ,1] - reward)),\n",
    "                conv_dr=np.mean(abs(conv_results[: ,2] - reward)),\n",
    "                conv_sndr=np.mean(abs(conv_results[: ,4] - reward)),\n",
    "\n",
    "                ipw_var=np.var(conv_results[: ,3]),\n",
    "                reg_dm_var=np.var(reg_results),\n",
    "                conv_dm_var=np.var(conv_results[: ,1]),\n",
    "                conv_dr_var=np.var(conv_results[: ,2]),\n",
    "                conv_sndr_var=np.var(conv_results[: ,4]),\n",
    "\n",
    "                                \n",
    "                # ipw_p_err=np.mean(abs(conv_results[: ,3] - reward) / reward) * 100,\n",
    "                # reg_dm_p_err=np.mean(abs(reg_results - reward) / reward) * 100,\n",
    "                # conv_dm_p_err=np.mean(abs(conv_results[: ,1] - reward) / reward) * 100,\n",
    "                # conv_dr_p_err=np.mean(abs(conv_results[: ,2] - reward) / reward) * 100,\n",
    "                # conv_sndr_p_err=np.mean(abs(conv_results[: ,4] - reward) / reward) * 100,\n",
    "                \n",
    "                action_diff_to_real=np.mean(conv_results[: ,5]),\n",
    "                action_delta=np.mean(conv_results[: ,6]),\n",
    "                context_diff_to_real=np.mean(conv_results[: ,7]),\n",
    "                context_delta=np.mean(conv_results[: ,8])\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `IPWPolicyLoss` Class\n",
    "\n",
    "This class implements an Inverse Propensity Weighting (IPW) loss function for counterfactual policy learning from offline bandit data.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The loss implements the IPW estimator as a differentiable function:\n",
    "\n",
    "$$\\mathcal{L}_{IPW} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)} \\cdot r_i \\cdot \\log(\\pi_e(a_i|x_i))$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_e(a_i|x_i)$ is the probability of the new policy taking action $a_i$ for context $x_i$\n",
    "- $\\pi_0(a_i|x_i)$ is the propensity score (probability of the logging policy)\n",
    "- $r_i$ is the observed reward\n",
    "- $n$ is the batch size\n",
    "\n",
    "### Parameters\n",
    "- **log_eps** (float): Small constant added to prevent numerical instability in log calculations\n",
    "\n",
    "### Method: `forward`\n",
    "- **pscore** (Tensor): Propensity scores from original logging policy\n",
    "- **scores** (Tensor): Model-estimated reward predictions for each action (not being used)\n",
    "- **policy_prob** (Tensor): Probabilities from current policy being optimized\n",
    "- **original_policy_rewards** (Tensor): Observed rewards from logged data\n",
    "- **original_policy_actions** (Tensor): Actions that were taken in the logged data\n",
    "\n",
    "### Implementation Notes\n",
    "- Importance weights (`iw`) are detached from the computation graph\n",
    "- Uses the REINFORCE policy gradient method\n",
    "- The implementation includes commented-out code for more advanced variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPWPolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(IPWPolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        # q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        # dm_grads = (scores * policy_prob.detach() * torch.log(policy_prob)).sum(dim=1)\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # reinforce trick step\n",
    "        # reinforce_grad = ((iw * (original_policy_rewards - q_hat_at_position) * log_pi) / iw.sum()) + dm_grads\n",
    "        reinforce_grad = iw * original_policy_rewards * log_pi\n",
    "        \n",
    "        return reinforce_grad.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SNDRPolicyLoss` Class\n",
    "\n",
    "This class implements a Self-Normalized Doubly Robust (SNDR) loss function for counterfactual policy learning from offline bandit data.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The loss combines IPW with direct method estimates for variance reduction:\n",
    "\n",
    "$$\\mathcal{L}_{SNDR} = \\frac{1}{n}\\sum_{i=1}^{n} \\left( \\frac{\\sum_{i=1}^{n}\\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)} \\cdot (r_i - \\hat{q}(x_i,a_i))}{\\sum_{i=1}^{n}\\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)}} + \\sum_{a}\\pi_e(a|x_i)\\hat{q}(x_i,a) \\right) \\cdot \\log(\\pi_e(a_i|x_i))$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_e(a_i|x_i)$ is the probability from the new policy\n",
    "- $\\pi_0(a_i|x_i)$ is the propensity score from the logging policy\n",
    "- $r_i$ is the observed reward\n",
    "- $\\hat{q}(x_i,a_i)$ is the estimated reward from a direct model\n",
    "- $n$ is the batch size\n",
    "\n",
    "### Parameters\n",
    "- **log_eps** (float): Small constant added to prevent numerical instability in log calculations\n",
    "\n",
    "### Method: `forward`\n",
    "- **pscore** (Tensor): Propensity scores from original logging policy\n",
    "- **scores** (Tensor): Model-estimated reward predictions for each action\n",
    "- **policy_prob** (Tensor): Probabilities from current policy being optimized\n",
    "- **original_policy_rewards** (Tensor): Observed rewards from logged data\n",
    "- **original_policy_actions** (Tensor): Actions that were taken in the logged data\n",
    "\n",
    "### Implementation Notes\n",
    "- Combines direct method rewards with importance-weighted corrections\n",
    "- Self-normalizes the importance weights by dividing by their sum\n",
    "- Generally provides lower variance estimates than pure IPW approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNDRPolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(SNDRPolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        dm_reward = (scores * policy_prob.detach()).sum(dim=1)\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # reinforce trick step\n",
    "        r_hat = ((iw * (original_policy_rewards - q_hat_at_position)) / iw.sum()) + dm_reward\n",
    "        reinforce_grad = r_hat * log_pi\n",
    "        return reinforce_grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_estimated_policy_rewards(pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        dm_reward = (scores * policy_prob.detach()).sum(dim=1)\n",
    "        \n",
    "        # reinforce trick step\n",
    "        r_hat = ((iw * (original_policy_rewards - q_hat_at_position)) / iw.sum()) + dm_reward\n",
    "\n",
    "        var_hat = r_hat.std()\n",
    "        return r_hat.mean() - scipy.stats.t.ppf(0.95, n - 1) * var_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        num_items = policy_prob.shape[1]\n",
    "        batch_size = scores.size(0)\n",
    "\n",
    "        # Filter to only positive-reward samples (reward == 1)\n",
    "        mask = original_policy_rewards > 0\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=scores.device)\n",
    "\n",
    "        pos_idx = torch.arange(batch_size, device=mask.device)[mask]\n",
    "        pos_actions = original_policy_actions[mask]\n",
    "        pos_scores = scores[pos_idx, pos_actions]\n",
    "        pos_pscore = pscore[mask]\n",
    "\n",
    "        # Sample negative actions not equal to the positive ones\n",
    "        neg_actions = torch.randint(0, num_items, size=(pos_idx.size(0),), device=scores.device)\n",
    "        conflict = neg_actions == pos_actions\n",
    "        \n",
    "        while conflict.any():\n",
    "            neg_actions[conflict] = torch.randint(0, num_items, size=(conflict.sum(),), device=scores.device)\n",
    "            conflict = neg_actions == pos_actions\n",
    "\n",
    "        neg_scores = scores[pos_idx, neg_actions]\n",
    "\n",
    "        # Compute pairwise BPR loss\n",
    "        bpr = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10)\n",
    "\n",
    "        # Importance weighting using inverse propensity score\n",
    "        loss = (bpr / (pos_pscore + 1e-6)).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train` Function\n",
    "\n",
    "This function trains a policy model with Self-Normalized Doubly Robust (SNDR) loss for counterfactual policy learning.\n",
    "\n",
    "### Parameters\n",
    "- **model** (CFModel): The policy model to be trained, which maps users to action probabilities\n",
    "- **train_loader** (DataLoader): PyTorch data loader containing training data with user indices, actions, rewards, and logging policy probabilities\n",
    "- **neighborhood_model** (NeighborhoodModel): Model that provides reward estimates based on neighborhood information\n",
    "- **num_epochs** (int, default=1): Number of training epochs\n",
    "- **lr** (float, default=0.0001): Learning rate for the Adam optimizer\n",
    "- **device** (str or torch.device, default='cpu'): Device to run the training on\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes an Adam optimizer and SNDR loss criterion\n",
    "2. For each epoch:\n",
    "   - Iterates through batches from the data loader\n",
    "   - Moves data to specified device (CPU/GPU)\n",
    "   - Gets policy probabilities by running the model on user indices\n",
    "   - Computes propensity scores from the logging policy\n",
    "   - Gets reward predictions from neighborhood model\n",
    "   - Calculates loss using the SNDR criterion\n",
    "   - Performs backpropagation and optimization\n",
    "   - Tracks and displays running loss statistics\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses `tqdm` for progress visualization\n",
    "- Contains commented-out code for neighborhood model updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the training function\n",
    "def train(model, train_loader, neighborhood_model, num_epochs=1, lr=0.0001, device='cpu'):\n",
    "    model.to(device)\n",
    "\n",
    "    tq = tqdm(range(num_epochs))\n",
    "    for epoch in tq:\n",
    "        run_train_loop(model, train_loader, neighborhood_model, lr=lr, device=device)\n",
    "\n",
    "\n",
    "# 4. Define the training function\n",
    "def run_train_loop(model, train_loader, neighborhood_model, lr=0.0001, device='cpu'):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # here we can change the learning rate\n",
    "    criterion = SNDRPolicyLoss()\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "\n",
    "    for user_idx, action_idx, rewards, original_prob in train_loader:\n",
    "        # Move data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            user_idx = user_idx.to(device) \n",
    "            action_idx = action_idx.to(device)\n",
    "            rewards = rewards.to(device)\n",
    "            original_prob = original_prob.to(device) \n",
    "        \n",
    "        # Forward pass\n",
    "        policy = model(user_idx)\n",
    "        pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "        \n",
    "        scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
    "        \n",
    "        loss = criterion(\n",
    "                            pscore,\n",
    "                            scores,\n",
    "                            policy, \n",
    "                            rewards, \n",
    "                            action_idx.type(torch.long), \n",
    "                            )\n",
    "        \n",
    "        # Zero the gradients Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()                        \n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the training function\n",
    "def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    estimated_rewards = 0.0\n",
    "\n",
    "    for user_idx, action_idx, rewards, original_prob in val_loader:\n",
    "        # Move data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            user_idx = user_idx.to(device) \n",
    "            action_idx = action_idx.to(device)\n",
    "            rewards = rewards.to(device)\n",
    "            original_prob = original_prob.to(device) \n",
    "        \n",
    "        # Forward pass\n",
    "        policy = model(user_idx)\n",
    "        pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "        \n",
    "        scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
    "        \n",
    "        estimated_rewards = calc_estimated_policy_rewards(\n",
    "            pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
    "        )\n",
    "\n",
    "    return estimated_rewards.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bpr(model, loss_fn, data_loader, num_epochs=5, lr=0.0001, device=device):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # here we can change the learning rate\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    tq = tqdm(range(num_epochs))\n",
    "    for epoch in tq:\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for user_idx, action_idx, rewards, original_prob in data_loader:\n",
    "            # Move data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                user_idx = user_idx.to(device) \n",
    "                action_idx = action_idx.to(device)\n",
    "                rewards = rewards.to(device)\n",
    "                original_prob = original_prob.to(device) \n",
    "            \n",
    "            # Forward pass\n",
    "            policy = model.calc_scores(user_idx)\n",
    "            pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "            \n",
    "            # scores = torch.tensor(model.calc_scores(user_idx.numpy()), device=device)\n",
    "            scores = policy.clone()\n",
    "            \n",
    "            loss = loss_fn(\n",
    "                            pscore,\n",
    "                            scores,\n",
    "                            policy, \n",
    "                            rewards, \n",
    "                            action_idx.type(torch.long), \n",
    "                            )\n",
    "            \n",
    "            # Zero the gradients Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()                        \n",
    "            optimizer.step()\n",
    "            \n",
    "            # update neighborhood\n",
    "            # action_emb, context_emb = model.get_params()\n",
    "            \n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "            # Print statistics after each epoch\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            tq.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        neighberhoodmodel = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        for _ in range(epochs):\n",
    "            train(model, train_loader, neighberhoodmodel, num_epochs=epochs, lr=lr, device=device)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = validation_loop(model, val_loader, neighberhoodmodel)\n",
    "        return val_loss\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    \n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    first = True\n",
    "    zero = True\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            dataset[\"n_users\"],\n",
    "                                                            dataset[\"n_actions\"],\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, 5, simulation_data, np.arange(5) + train_size, our_x)\n",
    "\n",
    "            bpr_model = BPRModel(\n",
    "                                dataset[\"n_users\"], \n",
    "                                dataset[\"n_actions\"], \n",
    "                                dataset[\"emb_x\"].shape[1], \n",
    "                                initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                                initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob[train_data['x_idx']]\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob[val_data['x_idx']]\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "            \n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            Bloss = BPRLoss()\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=10)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            train(model, train_loader, neighberhoodmodel, num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.3 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [3, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=10, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
