{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import gym\n",
    "import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  val_size=2000\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            dataset['env'],\n",
    "                                                            pi_0,\n",
    "                                                            train_size + val_size\n",
    "                                                            )\n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "\n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            # bpr_model = BPRModel(\n",
    "            #                     n_users,\n",
    "            #                     n_actions,\n",
    "            #                     emb_x.shape[1], \n",
    "            #                     initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "            #                     initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "            #                     )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "                reg_dm = 0.0\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            # Bloss = BPRLoss()\n",
    "\n",
    "            \n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=15)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            # fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "            reg_dm = 0.0\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 300,\n",
    "                    n_users = 300,\n",
    "                    emb_dim = 3,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.2 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 100\n",
    "num_neighbors = 51\n",
    "num_rounds_list = [8000, 7000, 5000, 6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-17 04:58:38,357] A new study created in memory with name: no-name-fc0f7a79-bd45-4543-ad29-772d5f328f08\n",
      "[I 2025-07-17 04:59:21,348] Trial 0 finished with value: 0.003159249001240238 and parameters: {'lr': 2.4647095179041555e-05, 'num_epochs': 5}. Best is trial 0 with value: 0.003159249001240238.\n",
      "[I 2025-07-17 05:00:04,587] Trial 1 finished with value: 0.0032178486367008836 and parameters: {'lr': 0.00103853334601605, 'num_epochs': 6}. Best is trial 0 with value: 0.003159249001240238.\n",
      "[I 2025-07-17 05:00:47,316] Trial 2 finished with value: 0.003163341501502345 and parameters: {'lr': 0.00010266873633087149, 'num_epochs': 4}. Best is trial 0 with value: 0.003159249001240238.\n",
      "[I 2025-07-17 05:01:29,835] Trial 3 finished with value: -0.014959807193750028 and parameters: {'lr': 0.044065929910662244, 'num_epochs': 2}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[W 2025-07-17 05:02:13,471] Trial 4 failed with parameters: {'lr': 0.055014550620760966, 'num_epochs': 6} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:02:13,472] Trial 4 failed with value nan.\n",
      "[I 2025-07-17 05:02:56,786] Trial 5 finished with value: 0.003162172772823931 and parameters: {'lr': 7.642906708530713e-05, 'num_epochs': 5}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:03:41,145] Trial 6 finished with value: 0.003196525595058711 and parameters: {'lr': 0.0008708965750208377, 'num_epochs': 10}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:04:25,302] Trial 7 finished with value: 0.003167850997538635 and parameters: {'lr': 9.893785931494985e-05, 'num_epochs': 10}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:05:08,723] Trial 8 finished with value: 0.003193393818611655 and parameters: {'lr': 0.0007707496692474522, 'num_epochs': 6}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:05:51,660] Trial 9 finished with value: 0.0031801971853206695 and parameters: {'lr': 0.00044908544482078876, 'num_epochs': 3}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:06:34,710] Trial 10 finished with value: -0.011974104511373535 and parameters: {'lr': 0.010574536333049684, 'num_epochs': 3}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:07:17,271] Trial 11 finished with value: -0.01462844927109191 and parameters: {'lr': 0.07773915454302775, 'num_epochs': 1}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:07:59,863] Trial 12 finished with value: -0.008876667176642527 and parameters: {'lr': 0.040430687889274286, 'num_epochs': 1}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:08:42,676] Trial 13 finished with value: -0.01151777321804048 and parameters: {'lr': 0.0906647125925229, 'num_epochs': 1}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:09:25,224] Trial 14 finished with value: -0.004099421124922774 and parameters: {'lr': 0.011943995862212849, 'num_epochs': 2}. Best is trial 3 with value: -0.014959807193750028.\n",
      "[I 2025-07-17 05:12:47,219] A new study created in memory with name: no-name-33ef8ccf-dc9f-4985-ac70-595ef930564a\n",
      "[I 2025-07-17 05:13:24,551] Trial 0 finished with value: -0.00032498885538606234 and parameters: {'lr': 0.0032780759588936425, 'num_epochs': 9}. Best is trial 0 with value: -0.00032498885538606234.\n",
      "[W 2025-07-17 05:14:01,799] Trial 1 failed with parameters: {'lr': 0.060372720039479334, 'num_epochs': 8} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:14:01,799] Trial 1 failed with value nan.\n",
      "[I 2025-07-17 05:14:38,435] Trial 2 finished with value: 0.005202516074600365 and parameters: {'lr': 0.0003816117697813427, 'num_epochs': 6}. Best is trial 0 with value: -0.00032498885538606234.\n",
      "[I 2025-07-17 05:15:14,450] Trial 3 finished with value: 0.0052440510724014195 and parameters: {'lr': 4.519860680497888e-05, 'num_epochs': 1}. Best is trial 0 with value: -0.00032498885538606234.\n",
      "[W 2025-07-17 05:15:51,318] Trial 4 failed with parameters: {'lr': 0.08007751410036476, 'num_epochs': 7} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:15:51,319] Trial 4 failed with value nan.\n",
      "[I 2025-07-17 05:16:28,044] Trial 5 finished with value: 0.005238020709887811 and parameters: {'lr': 8.392025260768386e-05, 'num_epochs': 6}. Best is trial 0 with value: -0.00032498885538606234.\n",
      "[I 2025-07-17 05:17:05,279] Trial 6 finished with value: 0.005204750807149923 and parameters: {'lr': 0.00038601698637563177, 'num_epochs': 9}. Best is trial 0 with value: -0.00032498885538606234.\n",
      "[I 2025-07-17 05:17:42,129] Trial 7 finished with value: 0.005239987171858444 and parameters: {'lr': 5.3416393536955497e-05, 'num_epochs': 7}. Best is trial 0 with value: -0.00032498885538606234.\n",
      "[I 2025-07-17 05:18:18,369] Trial 8 finished with value: 0.004245097773252543 and parameters: {'lr': 0.004971460016119965, 'num_epochs': 3}. Best is trial 0 with value: -0.00032498885538606234.\n",
      "[I 2025-07-17 05:18:54,156] Trial 9 finished with value: 0.005244333539174329 and parameters: {'lr': 1.9956147449878804e-05, 'num_epochs': 1}. Best is trial 0 with value: -0.00032498885538606234.\n",
      "[I 2025-07-17 05:19:31,001] Trial 10 finished with value: -0.01285920940646856 and parameters: {'lr': 0.028961329448296852, 'num_epochs': 6}. Best is trial 10 with value: -0.01285920940646856.\n",
      "[I 2025-07-17 05:20:07,336] Trial 11 finished with value: 0.0052439829449846456 and parameters: {'lr': 1.5178250836207356e-05, 'num_epochs': 4}. Best is trial 10 with value: -0.01285920940646856.\n",
      "[W 2025-07-17 05:20:43,737] Trial 12 failed with parameters: {'lr': 0.07597018591146971, 'num_epochs': 4} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:20:43,738] Trial 12 failed with value nan.\n",
      "[W 2025-07-17 05:21:20,370] Trial 13 failed with parameters: {'lr': 0.05502772078378998, 'num_epochs': 4} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:21:20,370] Trial 13 failed with value nan.\n",
      "[W 2025-07-17 05:21:56,921] Trial 14 failed with parameters: {'lr': 0.0737488944452758, 'num_epochs': 4} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:21:56,921] Trial 14 failed with value nan.\n",
      "[I 2025-07-17 05:24:59,506] A new study created in memory with name: no-name-50d3ba6a-14d0-416f-bba1-c88d46e0417b\n",
      "[I 2025-07-17 05:25:25,752] Trial 0 finished with value: -0.009962149242057999 and parameters: {'lr': 0.008810249432009391, 'num_epochs': 10}. Best is trial 0 with value: -0.009962149242057999.\n",
      "[I 2025-07-17 05:25:51,915] Trial 1 finished with value: 0.003943696666564436 and parameters: {'lr': 0.001452340827023553, 'num_epochs': 10}. Best is trial 0 with value: -0.009962149242057999.\n",
      "[I 2025-07-17 05:26:16,989] Trial 2 finished with value: 0.003499842347400233 and parameters: {'lr': 0.00018601178618210708, 'num_epochs': 1}. Best is trial 0 with value: -0.009962149242057999.\n",
      "[I 2025-07-17 05:26:43,268] Trial 3 finished with value: -0.012232761810749454 and parameters: {'lr': 0.025856867880491564, 'num_epochs': 10}. Best is trial 3 with value: -0.012232761810749454.\n",
      "[I 2025-07-17 05:27:08,708] Trial 4 finished with value: 0.003494581761327536 and parameters: {'lr': 1.3903880361340965e-05, 'num_epochs': 4}. Best is trial 3 with value: -0.012232761810749454.\n",
      "[I 2025-07-17 05:27:34,059] Trial 5 finished with value: 0.003500028880384372 and parameters: {'lr': 5.285453770450309e-05, 'num_epochs': 4}. Best is trial 3 with value: -0.012232761810749454.\n",
      "[I 2025-07-17 05:27:59,459] Trial 6 finished with value: 0.0035034590314822183 and parameters: {'lr': 5.9912310911628845e-05, 'num_epochs': 4}. Best is trial 3 with value: -0.012232761810749454.\n",
      "[I 2025-07-17 05:28:24,568] Trial 7 finished with value: 0.0035066979569946376 and parameters: {'lr': 0.0004158561581073387, 'num_epochs': 1}. Best is trial 3 with value: -0.012232761810749454.\n",
      "[I 2025-07-17 05:28:50,899] Trial 8 finished with value: 0.0029904482320214157 and parameters: {'lr': 0.0025617932067378405, 'num_epochs': 10}. Best is trial 3 with value: -0.012232761810749454.\n",
      "[I 2025-07-17 05:29:16,410] Trial 9 finished with value: 0.003505141829695845 and parameters: {'lr': 7.101142533189594e-05, 'num_epochs': 4}. Best is trial 3 with value: -0.012232761810749454.\n",
      "[W 2025-07-17 05:29:42,389] Trial 10 failed with parameters: {'lr': 0.08784137085925911, 'num_epochs': 7} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:29:42,389] Trial 10 failed with value nan.\n",
      "[W 2025-07-17 05:30:08,290] Trial 11 failed with parameters: {'lr': 0.09334860233285845, 'num_epochs': 7} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:30:08,291] Trial 11 failed with value nan.\n",
      "[I 2025-07-17 05:30:34,332] Trial 12 finished with value: -0.009509331076036577 and parameters: {'lr': 0.025902888327867087, 'num_epochs': 7}. Best is trial 3 with value: -0.012232761810749454.\n",
      "[W 2025-07-17 05:31:00,417] Trial 13 failed with parameters: {'lr': 0.046989846913717485, 'num_epochs': 8} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:31:00,417] Trial 13 failed with value nan.\n",
      "[W 2025-07-17 05:31:26,491] Trial 14 failed with parameters: {'lr': 0.0635432507439464, 'num_epochs': 8} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:31:26,491] Trial 14 failed with value nan.\n",
      "[I 2025-07-17 05:34:24,735] A new study created in memory with name: no-name-f0c3a575-3004-4891-950e-d7736edc9dfe\n",
      "[I 2025-07-17 05:34:55,739] Trial 0 finished with value: 0.005150446467255593 and parameters: {'lr': 6.460450813570439e-05, 'num_epochs': 2}. Best is trial 0 with value: 0.005150446467255593.\n",
      "[I 2025-07-17 05:35:27,464] Trial 1 finished with value: 0.005096792909317723 and parameters: {'lr': 0.000391566823592592, 'num_epochs': 8}. Best is trial 1 with value: 0.005096792909317723.\n",
      "[I 2025-07-17 05:35:58,534] Trial 2 finished with value: 0.005150916459695358 and parameters: {'lr': 3.101832536757932e-05, 'num_epochs': 3}. Best is trial 1 with value: 0.005096792909317723.\n",
      "[I 2025-07-17 05:36:29,194] Trial 3 finished with value: -0.010471373292803985 and parameters: {'lr': 0.0752985260372532, 'num_epochs': 1}. Best is trial 3 with value: -0.010471373292803985.\n",
      "[I 2025-07-17 05:37:01,141] Trial 4 finished with value: 0.005149749876718372 and parameters: {'lr': 2.3887174555881167e-05, 'num_epochs': 9}. Best is trial 3 with value: -0.010471373292803985.\n",
      "[I 2025-07-17 05:37:32,243] Trial 5 finished with value: -0.0027611028953313937 and parameters: {'lr': 0.010024371945856602, 'num_epochs': 3}. Best is trial 3 with value: -0.010471373292803985.\n",
      "[I 2025-07-17 05:38:03,465] Trial 6 finished with value: 0.005111839722777289 and parameters: {'lr': 0.0007019601385528663, 'num_epochs': 4}. Best is trial 3 with value: -0.010471373292803985.\n",
      "[I 2025-07-17 05:38:34,617] Trial 7 finished with value: -0.010159117448214 and parameters: {'lr': 0.011451034350268504, 'num_epochs': 4}. Best is trial 3 with value: -0.010471373292803985.\n",
      "[I 2025-07-17 05:39:05,265] Trial 8 finished with value: 0.005133002516136374 and parameters: {'lr': 0.0014829678085082633, 'num_epochs': 1}. Best is trial 3 with value: -0.010471373292803985.\n",
      "[I 2025-07-17 05:39:36,226] Trial 9 finished with value: -0.01400408245117975 and parameters: {'lr': 0.03022624441714283, 'num_epochs': 4}. Best is trial 9 with value: -0.01400408245117975.\n",
      "[W 2025-07-17 05:40:07,454] Trial 10 failed with parameters: {'lr': 0.09840425652762001, 'num_epochs': 6} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:40:07,454] Trial 10 failed with value nan.\n",
      "[W 2025-07-17 05:40:38,782] Trial 11 failed with parameters: {'lr': 0.059338177980925916, 'num_epochs': 6} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:40:38,783] Trial 11 failed with value nan.\n",
      "[W 2025-07-17 05:41:10,308] Trial 12 failed with parameters: {'lr': 0.04322881384614808, 'num_epochs': 7} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:41:10,308] Trial 12 failed with value nan.\n",
      "[W 2025-07-17 05:41:41,845] Trial 13 failed with parameters: {'lr': 0.0704345079442542, 'num_epochs': 6} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:41:41,846] Trial 13 failed with value nan.\n",
      "[W 2025-07-17 05:42:13,373] Trial 14 failed with parameters: {'lr': 0.09285542422672056, 'num_epochs': 6} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-17 05:42:13,374] Trial 14 failed with value nan.\n"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3431</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>-0.0053</td>\n",
       "      <td>2.1850</td>\n",
       "      <td>2.1310</td>\n",
       "      <td>3.2564</td>\n",
       "      <td>3.2989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>3.4869</td>\n",
       "      <td>3.4195</td>\n",
       "      <td>5.5357</td>\n",
       "      <td>5.5441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>3.4904</td>\n",
       "      <td>3.4339</td>\n",
       "      <td>5.6245</td>\n",
       "      <td>5.6546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>2.1377</td>\n",
       "      <td>2.0936</td>\n",
       "      <td>3.0524</td>\n",
       "      <td>3.0433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0             0.0126 0.0134  0.0000   0.0133   0.0129     0.0119   \n",
       "8000          0.0125 0.0000  0.0000   0.0169   0.0058    -0.0053   \n",
       "7000          0.0109 0.0000  0.0000   0.0187   0.0122     0.0046   \n",
       "5000          0.0151 0.0207  0.0000   0.0147   0.0156     0.0163   \n",
       "6000          0.0145 0.0000  0.0000   0.0158   0.0061     0.0007   \n",
       "\n",
       "      action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                  0.2138        0.0000                0.3431         0.0000  \n",
       "8000               2.1850        2.1310                3.2564         3.2989  \n",
       "7000               3.4869        3.4195                5.5357         5.5441  \n",
       "5000               3.4904        3.4339                5.6245         5.6546  \n",
       "6000               2.1377        2.0936                3.0524         3.0433  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`pscore` must be 1D array, but got 0D array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df5 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 118\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size, val_size)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[1;32m    117\u001b[0m     policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(softmax(our_x \u001b[38;5;241m@\u001b[39m our_a\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m     conv_results\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighberhoodmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_policy_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m     conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(calc_reward(dataset, policy), conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    120\u001b[0m     conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((emb_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)), np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((original_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))])\n",
      "File \u001b[0;32m/code/simulation_utils.py:214\u001b[0m, in \u001b[0;36meval_policy\u001b[0;34m(model, test_data, original_policy_prob, policy)\u001b[0m\n\u001b[1;32m    211\u001b[0m pscore \u001b[38;5;241m=\u001b[39m original_policy_prob[test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_idx\u001b[39m\u001b[38;5;124m'\u001b[39m], actions]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    213\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(dm\u001b[38;5;241m.\u001b[39mestimate_policy_value(policy, scores))\n\u001b[0;32m--> 214\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_policy_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpscore\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    215\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(ipw\u001b[38;5;241m.\u001b[39mestimate_policy_value(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], policy, pscore\u001b[38;5;241m=\u001b[39mpscore))\n\u001b[1;32m    216\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(sndr\u001b[38;5;241m.\u001b[39mestimate_policy_value(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], policy, scores, pscore\u001b[38;5;241m=\u001b[39mpscore))\n",
      "File \u001b[0;32m/code/estimators.py:1050\u001b[0m, in \u001b[0;36mDoublyRobust.estimate_policy_value\u001b[0;34m(self, reward, action, action_dist, estimated_rewards_by_reg_model, pscore, position, estimated_pscore, **kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     pscore_ \u001b[38;5;241m=\u001b[39m estimated_pscore\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m     \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m     pscore_ \u001b[38;5;241m=\u001b[39m pscore\n\u001b[1;32m   1052\u001b[0m check_ope_inputs(\n\u001b[1;32m   1053\u001b[0m     action_dist\u001b[38;5;241m=\u001b[39maction_dist,\n\u001b[1;32m   1054\u001b[0m     position\u001b[38;5;241m=\u001b[39mposition,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     estimated_rewards_by_reg_model\u001b[38;5;241m=\u001b[39mestimated_rewards_by_reg_model,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/code/saito_helpers.py:410\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, name, expected_dim)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(array)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m expected_dim:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `pscore` must be 1D array, but got 0D array"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
