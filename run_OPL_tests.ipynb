{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# import gym\n",
    "# import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")  # TF32 = big speedup on Ada\n",
    "\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "# from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward,\n",
    "    get_weights_info\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    LinearCFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    train,\n",
    "    validation_loop, \n",
    "    cv_score_model\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    IPWPolicyLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_results(\n",
    "    our_x, \n",
    "    our_a, \n",
    "    emb_x, \n",
    "    emb_a, \n",
    "    original_x, \n",
    "    original_a, \n",
    "    dataset, \n",
    "    val_data, \n",
    "    original_policy_prob, \n",
    "    neighberhoodmodel, \n",
    "    regression_model, \n",
    "    dm\n",
    "):\n",
    "    policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "    policy_reward = calc_reward(dataset, policy)\n",
    "    eval_metrics = eval_policy(neighberhoodmodel, val_data, original_policy_prob, policy)\n",
    "    action_diff_to_real = np.sqrt(np.mean((emb_a - our_a) ** 2))\n",
    "    action_delta = np.sqrt(np.mean((original_a - our_a) ** 2))\n",
    "    context_diff_to_real = np.sqrt(np.mean((emb_x - our_x) ** 2))\n",
    "    context_delta = np.sqrt(np.mean((original_x - our_x) ** 2))\n",
    "\n",
    "    row = np.concatenate([\n",
    "        np.atleast_1d(policy_reward),\n",
    "        np.atleast_1d(eval_metrics),\n",
    "        np.atleast_1d(action_diff_to_real),\n",
    "        np.atleast_1d(action_delta),\n",
    "        np.atleast_1d(context_diff_to_real),\n",
    "        np.atleast_1d(context_delta)\n",
    "    ])\n",
    "    reg_dm = dm.estimate_policy_value(policy[val_data['x_idx']], regression_model.predict(val_data['x']))\n",
    "    reg_results = np.array([reg_dm])\n",
    "    conv_results = np.array([row])\n",
    "    return get_opl_results_dict(reg_results, conv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "    num_runs,\n",
    "    num_neighbors,\n",
    "    train_sizes,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    val_size=2000,\n",
    "    n_trials=10,    \n",
    "    prev_best_params=None\n",
    "):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    def T(x):\n",
    "        return torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    def _mean_dict(dicts):\n",
    "        \"\"\"\n",
    "        Robust mean over a list of dicts with numeric/scalar/1D-array values.\n",
    "        Returns a single dict with elementwise means.\n",
    "        \"\"\"\n",
    "        if not dicts:\n",
    "            return {}\n",
    "        keys = dicts[0].keys()\n",
    "        out = {}\n",
    "        for k in keys:\n",
    "            vals = [d[k] for d in dicts if k in d]\n",
    "            # try to convert each to np.array and average\n",
    "            arrs = [np.asarray(v) for v in vals]\n",
    "            # broadcast to same shape if scalars/1D\n",
    "            stacked = np.stack(arrs, axis=0)\n",
    "            out[k] = np.mean(stacked, axis=0)\n",
    "        return out\n",
    "\n",
    "    # ===== unpack dataset (keep originals safe) =====\n",
    "    our_x_orig, our_a_orig = our_x, our_a\n",
    "    emb_x, emb_a = emb_x, emb_a\n",
    "    original_x, original_a = original_x, original_a\n",
    "    n_users, n_actions, emb_dim = n_users, n_actions, emb_dim\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "    best_hyperparams_by_size = {}\n",
    "    last_best_params = prev_best_params if prev_best_params is not None else None\n",
    "\n",
    "    # ===== baseline (sample size = 0) using get_trial_results =====\n",
    "    pi_0 = softmax(our_x_orig @ our_a_orig.T, axis=1)\n",
    "    original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "\n",
    "    simulation_data = create_simulation_data_from_pi(\n",
    "        dataset, pi_0, val_size, random_state=0\n",
    "    )\n",
    "\n",
    "    # use same data for train/val just to generate the baseline row\n",
    "    train_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size), our_x_orig)\n",
    "    val_data   = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size), our_x_orig)\n",
    "\n",
    "    regression_model = RegressionModel(\n",
    "        n_actions=n_actions, action_context=our_x_orig,\n",
    "        base_model=LogisticRegression(random_state=12345)\n",
    "    )\n",
    "\n",
    "    regression_model.fit(train_data['x'], train_data['a'], train_data['r'])\n",
    "\n",
    "    neighberhoodmodel = NeighborhoodModel(\n",
    "        train_data['x_idx'], train_data['a'],\n",
    "        our_a_orig, our_x_orig, train_data['r'],\n",
    "        num_neighbors=num_neighbors\n",
    "    )\n",
    "\n",
    "    # baseline row produced via get_trial_results\n",
    "    results[0] = get_trial_results(\n",
    "        our_x_orig, our_a_orig, emb_x, emb_a, original_x, original_a,\n",
    "        dataset, val_data, original_policy_prob,\n",
    "        neighberhoodmodel, regression_model, dm\n",
    "    )\n",
    "\n",
    "    # ===== main loop over training sizes =====\n",
    "    for train_size in train_sizes:\n",
    "\n",
    "        # we’ll collect per-run trial dicts generated by get_trial_results\n",
    "        trial_dicts_this_size = []\n",
    "        best_hyperparams_by_size[train_size] = {}\n",
    "\n",
    "        # --- prepare a resampling for Optuna’s objective (shared loaders built per-run inside objective) ---\n",
    "        # We’ll do Optuna per-run (fresh resample + search), then final fit with best params, then get_trial_results.\n",
    "\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            # --- resample for this run ---\n",
    "            pi_0 = softmax(our_x_orig @ our_a_orig.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                dataset, pi_0, train_size + val_size,\n",
    "                random_state=(run + 1) * (train_size + 17)\n",
    "            )\n",
    "\n",
    "            idx_train = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx_train, our_x_orig)\n",
    "            val_idx   = np.arange(val_size) + train_size\n",
    "            val_data  = get_train_data(n_actions, val_size, simulation_data, val_idx, our_x_orig)\n",
    "\n",
    "            num_workers = 4 if torch.cuda.is_available() else 0\n",
    "\n",
    "            cf_dataset = CustomCFDataset(\n",
    "                train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "            )\n",
    "\n",
    "            # val_loader = DataLoader(\n",
    "            #     val_dataset, batch_size=val_size, shuffle=False,\n",
    "            #     pin_memory=torch.cuda.is_available(),\n",
    "            #     num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            # )\n",
    "\n",
    "\n",
    "            # --- Optuna objective bound to this run's data ---\n",
    "            def objective(trial):\n",
    "                print()\n",
    "                print(f\"Trial {trial.number} started\")\n",
    "                lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "                epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "                trial_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "                trial_num_neighbors = trial.suggest_int(\"num_neighbors\", 3, 15)\n",
    "                lr_decay = trial.suggest_float(\"lr_decay\", 0.8, 1.0)\n",
    "\n",
    "                trial_neigh_model = NeighborhoodModel(\n",
    "                    train_data['x_idx'], train_data['a'],\n",
    "                    our_a_orig, our_x_orig, train_data['r'],\n",
    "                    num_neighbors=trial_num_neighbors\n",
    "                )\n",
    "\n",
    "                trial_scores_all = torch.as_tensor(\n",
    "                    trial_neigh_model.predict(all_user_indices),\n",
    "                    device=device, dtype=torch.float32\n",
    "                )\n",
    "\n",
    "                trial_model = LinearCFModel(\n",
    "                    n_users, n_actions, emb_dim,\n",
    "                    initial_user_embeddings=T(our_x_orig),\n",
    "                    initial_actions_embeddings=T(our_a_orig)\n",
    "                ).to(device)\n",
    "\n",
    "                assert (not torch.cuda.is_available()) or next(trial_model.parameters()).is_cuda\n",
    "\n",
    "                final_train_loader = DataLoader(\n",
    "                    cf_dataset, batch_size=trial_batch_size, shuffle=True,\n",
    "                    pin_memory=torch.cuda.is_available(),\n",
    "                    num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "                )\n",
    "\n",
    "                current_lr = lr\n",
    "                for epoch in range(epochs):\n",
    "                    if epoch > 0:\n",
    "                        current_lr *= lr_decay\n",
    "                        \n",
    "                    train(\n",
    "                        trial_model, final_train_loader, trial_scores_all,\n",
    "                        criterion=SNDRPolicyLoss(), num_epochs=1, lr=current_lr, device=str(device)\n",
    "                    )\n",
    "\n",
    "                trial_x, trial_a = trial_model.get_params()\n",
    "                trial_x = trial_x.detach().cpu().numpy()\n",
    "                trial_a = trial_a.detach().cpu().numpy()\n",
    "\n",
    "                pi_i = softmax(trial_x @ trial_a.T, axis=1)\n",
    "                train_actions = train_data['a']\n",
    "                train_users = train_data['x_idx']\n",
    "\n",
    "                print(\"Train wi info: {}\".format(get_weights_info(pi_i[train_users, train_actions], original_policy_prob[train_users, train_actions])))\n",
    "                print(f\"actual reward: {calc_reward(dataset, np.expand_dims(pi_i, -1))}\")\n",
    "\n",
    "                # print(get_weights_info(pi_i, original_policy_prob))\n",
    "                # validation reward for selection\n",
    "                return cv_score_model(val_data, trial_scores_all, pi_i)\n",
    "\n",
    "\n",
    "            # --- run Optuna for this run ---\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            \n",
    "            if last_best_params is not None:\n",
    "                study.enqueue_trial(last_best_params)\n",
    "\n",
    "            study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            last_best_params = best_params  # optional warm-start to next run\n",
    "            best_hyperparams_by_size[train_size][run] = {\n",
    "                \"params\": best_params,\n",
    "                \"reward\": study.best_value\n",
    "            }\n",
    "\n",
    "\n",
    "            # --- final training with best params on this run’s data ---\n",
    "            regression_model = RegressionModel(\n",
    "                n_actions=n_actions, action_context=our_x_orig,\n",
    "                base_model=LogisticRegression(random_state=12345)\n",
    "            )\n",
    "            regression_model.fit(\n",
    "                train_data['x'], train_data['a'], train_data['r'],\n",
    "                original_policy_prob[train_data['x_idx'], train_data['a']].squeeze()\n",
    "            )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a_orig, our_x_orig, train_data['r'],\n",
    "                num_neighbors=best_params['num_neighbors']\n",
    "            )\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            model = LinearCFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x_orig),\n",
    "                initial_actions_embeddings=T(our_a_orig)\n",
    "            ).to(device)\n",
    "            assert (not torch.cuda.is_available()) or next(model.parameters()).is_cuda\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            current_lr = best_params['lr']\n",
    "            for epoch in range(best_params['num_epochs']):\n",
    "                if epoch > 0:\n",
    "                    current_lr *= best_params['lr_decay']\n",
    "                train(\n",
    "                    model, train_loader, scores_all,\n",
    "                    criterion=SNDRPolicyLoss(), num_epochs=1, lr=current_lr, device=str(device)\n",
    "                )\n",
    "\n",
    "            # learned embeddings (do NOT overwrite originals)\n",
    "            learned_x_t, learned_a_t = model.get_params()\n",
    "            learned_x = learned_x_t.detach().cpu().numpy()\n",
    "            learned_a = learned_a_t.detach().cpu().numpy()\n",
    "\n",
    "            # --- produce the per-run result via get_trial_results ---\n",
    "            trial_res = get_trial_results(\n",
    "                learned_x, learned_a,          # learned (policy) embeddings\n",
    "                emb_x, emb_a,                  # ground-truth embedding refs\n",
    "                original_x, original_a,        # original clean refs\n",
    "                dataset,\n",
    "                val_data,                      # use this run's val split\n",
    "                original_policy_prob,\n",
    "                neighberhoodmodel,\n",
    "                regression_model,\n",
    "                dm\n",
    "            )\n",
    "\n",
    "            trial_dicts_this_size.append(trial_res)\n",
    "\n",
    "            # memory hygiene\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # === aggregate per-run results (mean) and store under this train_size ===\n",
    "        results[train_size] = _mean_dict(trial_dicts_this_size)\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index'), best_hyperparams_by_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Item CTR: 0.07066414727263938\n",
      "Optimal greedy CTR: 0.09999926940951757\n",
      "Optimal Stochastic CTR: 0.09995326955796031\n",
      "Our Initial CTR: 0.08610747363354625\n"
     ]
    }
   ],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.1\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 200\n",
    "num_neighbors = 6\n",
    "n_trials_for_optuna = 20\n",
    "# num_rounds_list = [500, 1000, 2000, 10000, 20000]\n",
    "# num_rounds_list = [500, 1000, 2000]\n",
    "num_rounds_list = [15000]\n",
    "\n",
    "\n",
    "# Manually define your best parameters\n",
    "best_params_to_use = {\n",
    "    \"lr\": 0.096,  # Learning rate\n",
    "    \"num_epochs\": 5,  # Number of training epochs\n",
    "    \"batch_size\": 64,  # Batch size for training\n",
    "    \"num_neighbors\": 8,  # Number of neighbors for neighborhood model\n",
    "    \"lr_decay\": 0.85  # Learning rate decay factor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of num_rounds_list: [15000]\n",
      "Num samples is 10000\n",
      "{'gini': np.float64(0.4800649632306703), 'ess': np.float64(4465.928051308972), 'max_wi': np.float64(13.91170994663072), 'min_wi': np.float64(0.0125177671998377)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-28 13:09:50,943] A new study created in memory with name: no-name-30d09f17-1c08-4fcf-9981-f19afc24583b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b99dde1b8242a595d5eb3879d57b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 0 started\n",
      "Train wi info: {'gini': np.float64(0.9995468425318123), 'ess': np.float64(7.451867045676316), 'max_wi': np.float64(3503.585751714663), 'min_wi': np.float64(0.0)}\n",
      "actual reward: [0.08124106]\n",
      "{'gini': np.float64(0.9997834208825459), 'ess': np.float64(2.2656835249631255), 'max_wi': np.float64(1314.97351651244), 'min_wi': np.float64(0.0)}\n",
      "Estimated reward: 0.131126\n",
      "Cross-validated error: 0.013089\n",
      "Final score CI (reward +- 2*error): [0.104948, 0.157304]\n",
      "[I 2025-10-28 13:10:56,666] Trial 0 finished with value: 0.10494769354306584 and parameters: {'lr': 0.096, 'num_epochs': 5, 'batch_size': 64, 'num_neighbors': 8, 'lr_decay': 0.85}. Best is trial 0 with value: 0.10494769354306584.\n",
      "\n",
      "Trial 1 started\n",
      "Train wi info: {'gini': np.float64(0.01849753824127466), 'ess': np.float64(14980.999065138967), 'max_wi': np.float64(1.1560480105967603), 'min_wi': np.float64(0.9182437089968505)}\n",
      "actual reward: [0.08612342]\n",
      "{'gini': np.float64(0.018426560215649295), 'ess': np.float64(9988.009563947755), 'max_wi': np.float64(1.1410783320408082), 'min_wi': np.float64(0.9165028290598999)}\n",
      "Estimated reward: 0.090759\n",
      "Cross-validated error: 0.009083\n",
      "Final score CI (reward +- 2*error): [0.072593, 0.108926]\n",
      "[I 2025-10-28 13:11:59,647] Trial 1 finished with value: 0.07259257229275576 and parameters: {'lr': 0.00018367436760887538, 'num_epochs': 3, 'batch_size': 512, 'num_neighbors': 13, 'lr_decay': 0.8961295169100852}. Best is trial 0 with value: 0.10494769354306584.\n",
      "\n",
      "Trial 2 started\n"
     ]
    }
   ],
   "source": [
    "print(\"Value of num_rounds_list:\", num_rounds_list)\n",
    "\n",
    "# Run the optimization\n",
    "df4, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=10000, n_trials=n_trials_for_optuna, prev_best_params=best_params_to_use)\n",
    "\n",
    "# # Print best hyperparameters for each training size\n",
    "# print(\"\\n=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\")\n",
    "# for train_size, params in best_hyperparams_by_size.items():\n",
    "#     print(f\"\\nTraining Size: {train_size}\")\n",
    "#     # print(f\"Best Reward: {params['reward']:.6f}\")\n",
    "#     print(\"Parameters:\")\n",
    "#     for param_name, value in params['params'].items():\n",
    "#         print(f\"  {param_name}: {value}\")\n",
    "# print(\"===========================\\n\")\n",
    "\n",
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy with delta function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.1\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params, seed=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization\n",
    "df5, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=10000, n_trials=n_trials_for_optuna, prev_best_params=best_params_to_use)\n",
    "\n",
    "# Show the performance metrics\n",
    "df5[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.1\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params, seed=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization\n",
    "df6, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=10000, n_trials=n_trials_for_optuna, prev_best_params=best_params_to_use)\n",
    "\n",
    "# Show the performance metrics\n",
    "df6[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.1\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params, seed=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization\n",
    "df7, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=10000, n_trials=n_trials_for_optuna, prev_best_params=best_params_to_use)\n",
    "\n",
    "# Show the performance metrics\n",
    "df7[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.1\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params, seed=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization\n",
    "df8, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=10000, n_trials=n_trials_for_optuna, prev_best_params=best_params_to_use)\n",
    "\n",
    "# Show the performance metrics\n",
    "df8[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.1\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params, seed=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization\n",
    "df9, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=10000, n_trials=n_trials_for_optuna, prev_best_params=best_params_to_use)\n",
    "\n",
    "# Show the performance metrics\n",
    "df9[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poicy Via argmax(r_hat - error_hat) through cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Via using actual policy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
