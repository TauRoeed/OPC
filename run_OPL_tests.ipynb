{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "from abc import ABCMeta\n",
    "import optuna\n",
    "\n",
    "\n",
    "from from_saito import (\n",
    "    DirectMethod as DM,\n",
    ")\n",
    "\n",
    "from my_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    CFModel,\n",
    "    CustomCFDataset,\n",
    "    NeighborhoodModel,\n",
    "    # BPRModel\n",
    ")\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRModel(nn.Module):\n",
    "    def __init__(self, num_users, num_actions, embedding_dim, \n",
    "                 initial_user_embeddings=None, initial_actions_embeddings=None):\n",
    "        super(BPRModel, self).__init__()\n",
    "\n",
    "        self.actions = torch.arange(num_actions)\n",
    "        self.users = torch.arange(num_users)\n",
    "        \n",
    "        # Initialize user and actions embeddings\n",
    "        if initial_user_embeddings is None:\n",
    "            self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        else:\n",
    "            # If initial embeddings are provided, set them as the embeddings\n",
    "            self.user_embeddings = nn.Embedding.from_pretrained(initial_user_embeddings, freeze=False)\n",
    "        \n",
    "        if initial_actions_embeddings is None:\n",
    "            self.actions_embeddings = nn.Embedding(num_actions, embedding_dim)\n",
    "        else:\n",
    "            # If initial embeddings are provided, set them as the embeddings\n",
    "            self.actions_embeddings = nn.Embedding.from_pretrained(initial_actions_embeddings, freeze=False)\n",
    "\n",
    "\n",
    "    def forward(self, user_ids, pos_action_ids, neg_action_ids):\n",
    "        user_embeds = self.user_embeddings(user_ids)\n",
    "        pos_action_embeds = self.actions_embeddings(pos_action_ids)\n",
    "        neg_action_embeds = self.actions_embeddings(neg_action_ids)\n",
    "\n",
    "        # Compute dot product between user and action embeddings\n",
    "        pos_scores = (user_embeds * pos_action_embeds).sum(dim=1)\n",
    "        neg_scores = (user_embeds * neg_action_embeds).sum(dim=1)\n",
    "\n",
    "        return pos_scores, neg_scores\n",
    "    \n",
    "    def calc_scores(self, user_ids):\n",
    "        # Ensure user_ids is on the same device as the model\n",
    "        device = self.user_embeddings.weight.device\n",
    "        user_ids = user_ids.to(device)\n",
    "\n",
    "        # Get user embeddings\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "\n",
    "        # Ensure self.actions is on the same device\n",
    "        actions = self.actions.to(device)\n",
    "\n",
    "        # Get action embeddings\n",
    "        actions_embedding = self.actions_embeddings(actions)\n",
    "\n",
    "        # Compute dot product scores\n",
    "        scores = user_embedding @ actions_embedding.T\n",
    "\n",
    "        # Return softmaxed scores\n",
    "        return F.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        \n",
    "    def to(self, device):\n",
    "        # Move the module itself\n",
    "        super().to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        self.users = self.users.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `calc_reward` Function\n",
    "Calculates the expected reward of a policy by computing the weighted average of true reward probabilities.\n",
    "\n",
    "### Parameters\n",
    "- `dataset` (dict): Contains dataset information including `q_x_a`, the true reward probabilities for each user-action pair\n",
    "- `policy` (numpy.ndarray): Policy probabilities with shape [n_users, n_actions, 1]\n",
    "\n",
    "### Returns\n",
    "- `numpy.ndarray`: A single-element array containing the expected policy reward\n",
    "\n",
    "### Mathematical Formulation\n",
    "Implements: $R_{gt} = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}{\\pi_{ij} \\cdot p_{ij}}$\n",
    "\n",
    "Where:\n",
    "- $\\pi_{ij}$ is the policy probability for user $i$ choosing action $j$\n",
    "- $p_{ij}$ is the true reward probability for user $i$ choosing action $j$ (stored in `q_x_a`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(dataset, policy):\n",
    "    return np.array([np.sum(dataset['q_x_a'] * policy.squeeze(), axis=1).mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_opl_results_dict` Function\n",
    "\n",
    "This function processes evaluation results from various offline policy learning (OPL) estimators and computes summary statistics.\n",
    "\n",
    "### Parameters\n",
    "- **reg_results** (numpy.ndarray): Results from regression-based direct method estimator\n",
    "- **conv_results** (numpy.ndarray): Results from various estimators including true rewards and embeddings quality metrics\n",
    "\n",
    "### Returns\n",
    "- **dict**: A dictionary containing the following metrics:\n",
    "  - `policy_rewards`: Mean true reward of the learned policy\n",
    "  - Error metrics (absolute difference between estimator and true reward):\n",
    "    - `ipw`: Inverse Propensity Weighting estimator error\n",
    "    - `reg_dm`: Regression-based Direct Method estimator error\n",
    "    - `conv_dm`: Convolution-based Direct Method estimator error\n",
    "    - `conv_dr`: Convolution-based Doubly Robust estimator error\n",
    "    - `conv_sndr`: Convolution-based Self-Normalized Doubly Robust estimator error\n",
    "  - Variance metrics for each estimator:\n",
    "    - `ipw_var`, `reg_dm_var`, `conv_dm_var`, `conv_dr_var`, `conv_sndr_var`\n",
    "  - Embedding quality metrics:\n",
    "    - `action_diff_to_real`: RMSE between learned and real action embeddings\n",
    "    - `action_delta`: RMSE between learned and original action embeddings\n",
    "    - `context_diff_to_real`: RMSE between learned and real context embeddings\n",
    "    - `context_delta`: RMSE between learned and original context embeddings\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses the first column of `conv_results` as the ground truth reward\n",
    "- Contains commented-out code for percentage error calculations\n",
    "- Computes absolute errors rather than signed differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opl_results_dict(reg_results, conv_results):\n",
    "    reward = conv_results[:, 0]\n",
    "    return    dict(\n",
    "                policy_rewards=np.mean(reward),\n",
    "                ipw=np.mean(abs(conv_results[: ,3] - reward)),\n",
    "                reg_dm=np.mean(abs(reg_results - reward)),\n",
    "                conv_dm=np.mean(abs(conv_results[: ,1] - reward)),\n",
    "                conv_dr=np.mean(abs(conv_results[: ,2] - reward)),\n",
    "                conv_sndr=np.mean(abs(conv_results[: ,4] - reward)),\n",
    "\n",
    "                ipw_var=np.var(conv_results[: ,3]),\n",
    "                reg_dm_var=np.var(reg_results),\n",
    "                conv_dm_var=np.var(conv_results[: ,1]),\n",
    "                conv_dr_var=np.var(conv_results[: ,2]),\n",
    "                conv_sndr_var=np.var(conv_results[: ,4]),\n",
    "\n",
    "                                \n",
    "                # ipw_p_err=np.mean(abs(conv_results[: ,3] - reward) / reward) * 100,\n",
    "                # reg_dm_p_err=np.mean(abs(reg_results - reward) / reward) * 100,\n",
    "                # conv_dm_p_err=np.mean(abs(conv_results[: ,1] - reward) / reward) * 100,\n",
    "                # conv_dr_p_err=np.mean(abs(conv_results[: ,2] - reward) / reward) * 100,\n",
    "                # conv_sndr_p_err=np.mean(abs(conv_results[: ,4] - reward) / reward) * 100,\n",
    "                \n",
    "                action_diff_to_real=np.mean(conv_results[: ,5]),\n",
    "                action_delta=np.mean(conv_results[: ,6]),\n",
    "                context_diff_to_real=np.mean(conv_results[: ,7]),\n",
    "                context_delta=np.mean(conv_results[: ,8])\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `IPWPolicyLoss` Class\n",
    "\n",
    "This class implements an Inverse Propensity Weighting (IPW) loss function for counterfactual policy learning from offline bandit data.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The loss implements the IPW estimator as a differentiable function:\n",
    "\n",
    "$$\\mathcal{L}_{IPW} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)} \\cdot r_i \\cdot \\log(\\pi_e(a_i|x_i))$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_e(a_i|x_i)$ is the probability of the new policy taking action $a_i$ for context $x_i$\n",
    "- $\\pi_0(a_i|x_i)$ is the propensity score (probability of the logging policy)\n",
    "- $r_i$ is the observed reward\n",
    "- $n$ is the batch size\n",
    "\n",
    "### Parameters\n",
    "- **log_eps** (float): Small constant added to prevent numerical instability in log calculations\n",
    "\n",
    "### Method: `forward`\n",
    "- **pscore** (Tensor): Propensity scores from original logging policy\n",
    "- **scores** (Tensor): Model-estimated reward predictions for each action (not being used)\n",
    "- **policy_prob** (Tensor): Probabilities from current policy being optimized\n",
    "- **original_policy_rewards** (Tensor): Observed rewards from logged data\n",
    "- **original_policy_actions** (Tensor): Actions that were taken in the logged data\n",
    "\n",
    "### Implementation Notes\n",
    "- Importance weights (`iw`) are detached from the computation graph\n",
    "- Uses the REINFORCE policy gradient method\n",
    "- The implementation includes commented-out code for more advanced variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPWPolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(IPWPolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        # q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        # dm_grads = (scores * policy_prob.detach() * torch.log(policy_prob)).sum(dim=1)\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # reinforce trick step\n",
    "        # reinforce_grad = ((iw * (original_policy_rewards - q_hat_at_position) * log_pi) / iw.sum()) + dm_grads\n",
    "        reinforce_grad = iw * original_policy_rewards * log_pi\n",
    "        \n",
    "        return reinforce_grad.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SNDRPolicyLoss` Class\n",
    "\n",
    "This class implements a Self-Normalized Doubly Robust (SNDR) loss function for counterfactual policy learning from offline bandit data.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The loss combines IPW with direct method estimates for variance reduction:\n",
    "\n",
    "$$\\mathcal{L}_{SNDR} = \\frac{1}{n}\\sum_{i=1}^{n} \\left( \\frac{\\sum_{i=1}^{n}\\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)} \\cdot (r_i - \\hat{q}(x_i,a_i))}{\\sum_{i=1}^{n}\\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)}} + \\sum_{a}\\pi_e(a|x_i)\\hat{q}(x_i,a) \\right) \\cdot \\log(\\pi_e(a_i|x_i))$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_e(a_i|x_i)$ is the probability from the new policy\n",
    "- $\\pi_0(a_i|x_i)$ is the propensity score from the logging policy\n",
    "- $r_i$ is the observed reward\n",
    "- $\\hat{q}(x_i,a_i)$ is the estimated reward from a direct model\n",
    "- $n$ is the batch size\n",
    "\n",
    "### Parameters\n",
    "- **log_eps** (float): Small constant added to prevent numerical instability in log calculations\n",
    "\n",
    "### Method: `forward`\n",
    "- **pscore** (Tensor): Propensity scores from original logging policy\n",
    "- **scores** (Tensor): Model-estimated reward predictions for each action\n",
    "- **policy_prob** (Tensor): Probabilities from current policy being optimized\n",
    "- **original_policy_rewards** (Tensor): Observed rewards from logged data\n",
    "- **original_policy_actions** (Tensor): Actions that were taken in the logged data\n",
    "\n",
    "### Implementation Notes\n",
    "- Combines direct method rewards with importance-weighted corrections\n",
    "- Self-normalizes the importance weights by dividing by their sum\n",
    "- Generally provides lower variance estimates than pure IPW approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNDRPolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(SNDRPolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        dm_reward = (scores * policy_prob.detach()).sum(dim=1)\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # reinforce trick step\n",
    "        r_hat = ((iw * (original_policy_rewards - q_hat_at_position)) / iw.sum()) + dm_reward\n",
    "        reinforce_grad = r_hat * log_pi\n",
    "        return reinforce_grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_estimated_policy_rewards(pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        dm_reward = (scores * policy_prob.detach()).sum(dim=1)\n",
    "        \n",
    "        # reinforce trick step\n",
    "        r_hat = ((iw * (original_policy_rewards - q_hat_at_position)) / iw.sum()) + dm_reward\n",
    "\n",
    "        var_hat = r_hat.std()\n",
    "        return r_hat.mean() - scipy.stats.t.ppf(0.95, n - 1) * var_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        num_items = policy_prob.shape[1]\n",
    "        batch_size = scores.size(0)\n",
    "\n",
    "        # Filter to only positive-reward samples (reward == 1)\n",
    "        mask = original_policy_rewards > 0\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=scores.device)\n",
    "\n",
    "        pos_idx = torch.arange(batch_size, device=mask.device)[mask]\n",
    "        pos_actions = original_policy_actions[mask]\n",
    "        pos_scores = scores[pos_idx, pos_actions]\n",
    "        pos_pscore = pscore[mask]\n",
    "\n",
    "        # Sample negative actions not equal to the positive ones\n",
    "        neg_actions = torch.randint(0, num_items, size=(pos_idx.size(0),), device=scores.device)\n",
    "        conflict = neg_actions == pos_actions\n",
    "        \n",
    "        while conflict.any():\n",
    "            neg_actions[conflict] = torch.randint(0, num_items, size=(conflict.sum(),), device=scores.device)\n",
    "            conflict = neg_actions == pos_actions\n",
    "\n",
    "        neg_scores = scores[pos_idx, neg_actions]\n",
    "\n",
    "        # Compute pairwise BPR loss\n",
    "        bpr = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10)\n",
    "\n",
    "        # Importance weighting using inverse propensity score\n",
    "        loss = (bpr / (pos_pscore + 1e-6)).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train` Function\n",
    "\n",
    "This function trains a policy model with Self-Normalized Doubly Robust (SNDR) loss for counterfactual policy learning.\n",
    "\n",
    "### Parameters\n",
    "- **model** (CFModel): The policy model to be trained, which maps users to action probabilities\n",
    "- **train_loader** (DataLoader): PyTorch data loader containing training data with user indices, actions, rewards, and logging policy probabilities\n",
    "- **neighborhood_model** (NeighborhoodModel): Model that provides reward estimates based on neighborhood information\n",
    "- **num_epochs** (int, default=1): Number of training epochs\n",
    "- **lr** (float, default=0.0001): Learning rate for the Adam optimizer\n",
    "- **device** (str or torch.device, default='cpu'): Device to run the training on\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes an Adam optimizer and SNDR loss criterion\n",
    "2. For each epoch:\n",
    "   - Iterates through batches from the data loader\n",
    "   - Moves data to specified device (CPU/GPU)\n",
    "   - Gets policy probabilities by running the model on user indices\n",
    "   - Computes propensity scores from the logging policy\n",
    "   - Gets reward predictions from neighborhood model\n",
    "   - Calculates loss using the SNDR criterion\n",
    "   - Performs backpropagation and optimization\n",
    "   - Tracks and displays running loss statistics\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses `tqdm` for progress visualization\n",
    "- Contains commented-out code for neighborhood model updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the training function\n",
    "def train(model, train_loader, neighborhood_model, num_epochs=1, lr=0.0001, device='cpu'):\n",
    "    model.to(device)\n",
    "\n",
    "    tq = tqdm(range(num_epochs))\n",
    "    for epoch in tq:\n",
    "        run_train_loop(model, train_loader, neighborhood_model, lr=lr, device=device)\n",
    "\n",
    "\n",
    "# 4. Define the training function\n",
    "def run_train_loop(model, train_loader, neighborhood_model, lr=0.0001, device='cpu'):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # here we can change the learning rate\n",
    "    criterion = SNDRPolicyLoss()\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "\n",
    "    for user_idx, action_idx, rewards, original_prob in train_loader:\n",
    "        # Move data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            user_idx = user_idx.to(device) \n",
    "            action_idx = action_idx.to(device)\n",
    "            rewards = rewards.to(device)\n",
    "            original_prob = original_prob.to(device) \n",
    "        \n",
    "        # Forward pass\n",
    "        policy = model(user_idx)\n",
    "        pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "        \n",
    "        scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
    "        \n",
    "        loss = criterion(\n",
    "                            pscore,\n",
    "                            scores,\n",
    "                            policy, \n",
    "                            rewards, \n",
    "                            action_idx.type(torch.long), \n",
    "                            )\n",
    "        \n",
    "        # Zero the gradients Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()                        \n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the training function\n",
    "def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    estimated_rewards = 0.0\n",
    "\n",
    "    for user_idx, action_idx, rewards, original_prob in val_loader:\n",
    "        # Move data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            user_idx = user_idx.to(device) \n",
    "            action_idx = action_idx.to(device)\n",
    "            rewards = rewards.to(device)\n",
    "            original_prob = original_prob.to(device) \n",
    "        \n",
    "        # Forward pass\n",
    "        policy = model(user_idx)\n",
    "        pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "        \n",
    "        scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
    "        \n",
    "        estimated_rewards = calc_estimated_policy_rewards(\n",
    "            pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
    "        )\n",
    "\n",
    "    return estimated_rewards.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bpr(model, loss_fn, data_loader, num_epochs=5, lr=0.0001, device=device):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # here we can change the learning rate\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    tq = tqdm(range(num_epochs))\n",
    "    for epoch in tq:\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for user_idx, action_idx, rewards, original_prob in data_loader:\n",
    "            # Move data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                user_idx = user_idx.to(device) \n",
    "                action_idx = action_idx.to(device)\n",
    "                rewards = rewards.to(device)\n",
    "                original_prob = original_prob.to(device) \n",
    "            \n",
    "            # Forward pass\n",
    "            policy = model.calc_scores(user_idx)\n",
    "            pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "            \n",
    "            # scores = torch.tensor(model.calc_scores(user_idx.numpy()), device=device)\n",
    "            scores = policy.clone()\n",
    "            \n",
    "            loss = loss_fn(\n",
    "                            pscore,\n",
    "                            scores,\n",
    "                            policy, \n",
    "                            rewards, \n",
    "                            action_idx.type(torch.long), \n",
    "                            )\n",
    "            \n",
    "            # Zero the gradients Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()                        \n",
    "            optimizer.step()\n",
    "            \n",
    "            # update neighborhood\n",
    "            # action_emb, context_emb = model.get_params()\n",
    "            \n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "            # Print statistics after each epoch\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            tq.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        neighberhoodmodel = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        for _ in range(epochs):\n",
    "            train(model, train_loader, neighberhoodmodel, num_epochs=epochs, lr=lr, device=device)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = validation_loop(model, val_loader, neighberhoodmodel)\n",
    "        return val_loss\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    \n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    first = True\n",
    "    zero = True\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            dataset[\"n_users\"],\n",
    "                                                            dataset[\"n_actions\"],\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, 5, simulation_data, np.arange(5) + train_size, our_x)\n",
    "\n",
    "            bpr_model = BPRModel(\n",
    "                                dataset[\"n_users\"], \n",
    "                                dataset[\"n_actions\"], \n",
    "                                dataset[\"emb_x\"].shape[1], \n",
    "                                initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                                initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob[train_data['x_idx']]\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob[val_data['x_idx']]\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "            \n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            Bloss = BPRLoss()\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=30)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            train(model, train_loader, neighberhoodmodel, num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.3 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [1, 2, 3, 4, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-19 07:55:07,315] A new study created in memory with name: no-name-91a342c0-abd2-4ab9-b997-5955b6334a03\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 142.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 175.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 194.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 198.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 198.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 199.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 196.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 195.56it/s]\n",
      "[I 2025-06-19 07:55:20,744] Trial 0 finished with value: 0.03708245235292387 and parameters: {'lr': 4.7522541835759504e-05, 'num_epochs': 8}. Best is trial 0 with value: 0.03708245235292387.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 172.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 184.74it/s]\n",
      "[I 2025-06-19 07:55:20,788] Trial 1 finished with value: 0.0362306011179524 and parameters: {'lr': 0.0036313096779229106, 'num_epochs': 2}. Best is trial 1 with value: 0.0362306011179524.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 177.63it/s]\n",
      "[I 2025-06-19 07:55:20,814] Trial 2 finished with value: 0.03615628139888455 and parameters: {'lr': 0.0009225988990413783, 'num_epochs': 1}. Best is trial 2 with value: 0.03615628139888455.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 156.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 190.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 194.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 193.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 192.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 184.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 192.82it/s]\n",
      "[I 2025-06-19 07:55:21,110] Trial 3 finished with value: -0.12230249671251009 and parameters: {'lr': 0.038950139158815, 'num_epochs': 7}. Best is trial 3 with value: -0.12230249671251009.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 150.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 186.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 190.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 190.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 189.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 172.67it/s]\n",
      "[I 2025-06-19 07:55:21,342] Trial 4 finished with value: -0.12232598271648037 and parameters: {'lr': 0.00017483647283828512, 'num_epochs': 6}. Best is trial 4 with value: -0.12232598271648037.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 186.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 187.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 189.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 150.27it/s]\n",
      "[I 2025-06-19 07:55:21,460] Trial 5 finished with value: -0.12363280751634215 and parameters: {'lr': 0.020279884195198637, 'num_epochs': 4}. Best is trial 5 with value: -0.12363280751634215.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 154.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 154.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 191.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 191.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 189.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 194.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 191.59it/s]\n",
      "[I 2025-06-19 07:55:21,765] Trial 6 finished with value: -0.12422704154524186 and parameters: {'lr': 0.001332242849818992, 'num_epochs': 7}. Best is trial 6 with value: -0.12422704154524186.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 185.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 154.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 155.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 153.94it/s]\n",
      "[I 2025-06-19 07:55:21,892] Trial 7 finished with value: -0.1242352222379633 and parameters: {'lr': 4.235007658148961e-05, 'num_epochs': 4}. Best is trial 7 with value: -0.1242352222379633.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 146.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 144.90it/s]\n",
      "[I 2025-06-19 07:55:21,944] Trial 8 finished with value: -0.12453187891636655 and parameters: {'lr': 0.013687563192210242, 'num_epochs': 2}. Best is trial 8 with value: -0.12453187891636655.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 137.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 140.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 150.01it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 151.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 188.16it/s]\n",
      "[I 2025-06-19 07:55:22,138] Trial 9 finished with value: -0.12467360661360605 and parameters: {'lr': 0.0005803369192196719, 'num_epochs': 5}. Best is trial 9 with value: -0.12467360661360605.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 192.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 193.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 191.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 194.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 186.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 195.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 195.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 194.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 192.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 191.15it/s]\n",
      "[I 2025-06-19 07:55:22,705] Trial 10 finished with value: -0.12497790799138042 and parameters: {'lr': 0.0002686377744249483, 'num_epochs': 10}. Best is trial 10 with value: -0.12497790799138042.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 156.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 177.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 188.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 193.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 154.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 177.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 154.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 157.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 154.76it/s]\n",
      "[I 2025-06-19 07:55:23,237] Trial 11 finished with value: -0.1251574536674395 and parameters: {'lr': 0.00022545003204620012, 'num_epochs': 9}. Best is trial 11 with value: -0.1251574536674395.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 184.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 157.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 190.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 189.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 194.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 188.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 191.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 194.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 194.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 191.09it/s]\n",
      "[I 2025-06-19 07:55:23,816] Trial 12 finished with value: -0.12534164582604065 and parameters: {'lr': 0.0001705941492744156, 'num_epochs': 10}. Best is trial 12 with value: -0.12534164582604065.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 188.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 189.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 190.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 192.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 189.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 192.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 155.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 161.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 155.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 170.03it/s]\n",
      "[I 2025-06-19 07:55:24,425] Trial 13 finished with value: -0.12535552327861488 and parameters: {'lr': 1.477276236062108e-05, 'num_epochs': 10}. Best is trial 13 with value: -0.12535552327861488.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 192.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 175.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 139.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 193.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 161.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 194.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 189.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 191.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 193.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 192.01it/s]\n",
      "[I 2025-06-19 07:55:25,027] Trial 14 finished with value: -0.1253721611985573 and parameters: {'lr': 1.6047614845276225e-05, 'num_epochs': 10}. Best is trial 14 with value: -0.1253721611985573.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 186.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 190.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 189.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 191.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 186.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 190.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 186.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 185.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 188.31it/s]\n",
      "[I 2025-06-19 07:55:25,503] Trial 15 finished with value: -0.12538102255256806 and parameters: {'lr': 1.1492850270582274e-05, 'num_epochs': 9}. Best is trial 15 with value: -0.12538102255256806.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 183.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 183.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 183.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 184.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 181.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 181.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 184.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 185.01it/s]\n",
      "[I 2025-06-19 07:55:25,894] Trial 16 finished with value: -0.12538923592584428 and parameters: {'lr': 1.0295327590243532e-05, 'num_epochs': 8}. Best is trial 16 with value: -0.12538923592584428.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 183.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 182.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 182.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 182.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 186.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 186.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 183.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 182.88it/s]\n",
      "[I 2025-06-19 07:55:26,285] Trial 17 finished with value: -0.1254163526571475 and parameters: {'lr': 3.959274575309425e-05, 'num_epochs': 8}. Best is trial 17 with value: -0.1254163526571475.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 182.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 133.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 147.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 184.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 189.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 155.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 154.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 165.56it/s]\n",
      "[I 2025-06-19 07:55:26,725] Trial 18 finished with value: -0.12543642117601542 and parameters: {'lr': 3.5282328257900244e-05, 'num_epochs': 8}. Best is trial 18 with value: -0.12543642117601542.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 180.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 146.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 149.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 185.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 193.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 192.87it/s]\n",
      "[I 2025-06-19 07:55:26,973] Trial 19 finished with value: -0.12545838519054814 and parameters: {'lr': 5.140040939616358e-05, 'num_epochs': 6}. Best is trial 19 with value: -0.12545838519054814.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 181.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 190.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 191.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 188.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 180.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 182.03it/s]\n",
      "[I 2025-06-19 07:55:27,206] Trial 20 finished with value: -0.1254877236221954 and parameters: {'lr': 7.355460211164561e-05, 'num_epochs': 6}. Best is trial 20 with value: -0.1254877236221954.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 185.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 189.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 152.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 185.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 173.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 153.17it/s]\n",
      "[I 2025-06-19 07:55:27,457] Trial 21 finished with value: -0.1255171894634124 and parameters: {'lr': 7.442009971677554e-05, 'num_epochs': 6}. Best is trial 21 with value: -0.1255171894634124.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 152.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 164.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 153.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 149.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 139.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 152.90it/s]\n",
      "[I 2025-06-19 07:55:27,739] Trial 22 finished with value: -0.1255453252808128 and parameters: {'lr': 7.150247011721452e-05, 'num_epochs': 6}. Best is trial 22 with value: -0.1255453252808128.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 164.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 192.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 188.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 190.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 187.84it/s]\n",
      "[I 2025-06-19 07:55:27,914] Trial 23 finished with value: -0.12556688434013014 and parameters: {'lr': 7.9953222188533e-05, 'num_epochs': 5}. Best is trial 23 with value: -0.12556688434013014.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 184.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 184.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 187.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 177.72it/s]\n",
      "[I 2025-06-19 07:55:28,036] Trial 24 finished with value: -0.12558939607567013 and parameters: {'lr': 0.00010522171446435206, 'num_epochs': 4}. Best is trial 24 with value: -0.12558939607567013.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 151.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 151.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 152.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 151.83it/s]\n",
      "[I 2025-06-19 07:55:28,179] Trial 25 finished with value: -0.12565319821946846 and parameters: {'lr': 0.000421846515706026, 'num_epochs': 4}. Best is trial 25 with value: -0.12565319821946846.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 179.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 151.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 177.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 178.08it/s]\n",
      "[I 2025-06-19 07:55:28,357] Trial 26 finished with value: -0.1262539635890916 and parameters: {'lr': 0.0031169708174515844, 'num_epochs': 4}. Best is trial 26 with value: -0.1262539635890916.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 154.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 175.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 175.46it/s]\n",
      "[I 2025-06-19 07:55:28,445] Trial 27 finished with value: -0.1265854620330484 and parameters: {'lr': 0.0031909645302631625, 'num_epochs': 3}. Best is trial 27 with value: -0.1265854620330484.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 176.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 105.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 109.79it/s]\n",
      "[I 2025-06-19 07:55:28,568] Trial 28 finished with value: -0.12688098623009506 and parameters: {'lr': 0.003285445672773412, 'num_epochs': 3}. Best is trial 28 with value: -0.12688098623009506.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 141.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 131.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 144.27it/s]\n",
      "[I 2025-06-19 07:55:28,673] Trial 29 finished with value: -0.12684689295479432 and parameters: {'lr': 0.00435135770514316, 'num_epochs': 3}. Best is trial 28 with value: -0.12688098623009506.\n"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.0443</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>1.2613</td>\n",
       "      <td>1.0678</td>\n",
       "      <td>1.1922</td>\n",
       "      <td>1.0657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>1.2626</td>\n",
       "      <td>1.0696</td>\n",
       "      <td>1.1944</td>\n",
       "      <td>1.0672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>1.2653</td>\n",
       "      <td>1.0727</td>\n",
       "      <td>1.1965</td>\n",
       "      <td>1.0690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>1.2672</td>\n",
       "      <td>1.0750</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>1.0725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>1.2689</td>\n",
       "      <td>1.0768</td>\n",
       "      <td>1.1997</td>\n",
       "      <td>1.0720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2821</td>\n",
       "      <td>1.0921</td>\n",
       "      <td>1.2127</td>\n",
       "      <td>1.0857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>1.3096</td>\n",
       "      <td>1.1244</td>\n",
       "      <td>1.2361</td>\n",
       "      <td>1.1148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   \n",
       "1           0.1452 0.0443  0.1382   0.0032   0.0211     0.0221   \n",
       "2           0.1453 0.0118  0.1383   0.0242   0.0361     0.0302   \n",
       "3           0.1453 0.0018  0.1382   0.0133   0.0149     0.0147   \n",
       "4           0.1453 0.0008  0.1383   0.0104   0.0017     0.0026   \n",
       "5           0.1452 0.0513  0.1382   0.0087   0.0512     0.0532   \n",
       "10          0.1452 0.0229  0.1381   0.0065   0.0002     0.0000   \n",
       "20          0.1452 0.0098  0.1382   0.0032   0.0128     0.0125   \n",
       "\n",
       "    action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                0.3386        0.0000                0.5364         0.0000  \n",
       "1                1.2613        1.0678                1.1922         1.0657  \n",
       "2                1.2626        1.0696                1.1944         1.0672  \n",
       "3                1.2653        1.0727                1.1965         1.0690  \n",
       "4                1.2672        1.0750                1.2000         1.0725  \n",
       "5                1.2689        1.0768                1.1997         1.0720  \n",
       "10               1.2821        1.0921                1.2127         1.0857  \n",
       "20               1.3096        1.1244                1.2361         1.1148  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: -0.9690: 100%|██████████| 1/1 [00:00<00:00, 49.90it/s]\n",
      "Epoch [1/1], Loss: 103.7313: 100%|██████████| 1/1 [00:00<00:00, 48.08it/s]\n",
      "Epoch [1/1], Loss: -1.1012: 100%|██████████| 1/1 [00:00<00:00, 21.52it/s]\n",
      "Epoch [1/1], Loss: 103.8110: 100%|██████████| 1/1 [00:00<00:00, 21.84it/s]\n",
      "Epoch [1/1], Loss: -1.0617: 100%|██████████| 1/1 [00:00<00:00, 14.53it/s]\n",
      "Epoch [1/1], Loss: 103.8067: 100%|██████████| 1/1 [00:00<00:00, 15.78it/s]\n",
      "Epoch [1/1], Loss: -0.8680: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "Epoch [1/1], Loss: 103.9722: 100%|██████████| 1/1 [00:00<00:00, 10.33it/s]\n",
      "Epoch [1/1], Loss: -0.9874: 100%|██████████| 1/1 [00:00<00:00, 10.48it/s]\n",
      "Epoch [1/1], Loss: 103.7223: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
      "Epoch [1/1], Loss: -1.0314: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n",
      "Epoch [1/1], Loss: 103.8244: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
      "Epoch [1/1], Loss: -1.0509: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Epoch [1/1], Loss: 103.9615: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2599</td>\n",
       "      <td>1.0661</td>\n",
       "      <td>1.1906</td>\n",
       "      <td>1.0657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2601</td>\n",
       "      <td>1.0665</td>\n",
       "      <td>1.1911</td>\n",
       "      <td>1.0660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2606</td>\n",
       "      <td>1.0671</td>\n",
       "      <td>1.1914</td>\n",
       "      <td>1.0663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2610</td>\n",
       "      <td>1.0675</td>\n",
       "      <td>1.1921</td>\n",
       "      <td>1.0670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2613</td>\n",
       "      <td>1.0679</td>\n",
       "      <td>1.1921</td>\n",
       "      <td>1.0669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2638</td>\n",
       "      <td>1.0708</td>\n",
       "      <td>1.1945</td>\n",
       "      <td>1.0694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2685</td>\n",
       "      <td>1.0763</td>\n",
       "      <td>1.1984</td>\n",
       "      <td>1.0743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1           0.1453 0.0442  0.1382   0.0031   0.0210     0.0218   0.0000   \n",
       "2           0.1453 0.0123  0.1382   0.0241   0.0370     0.0309   0.0000   \n",
       "3           0.1453 0.0027  0.1382   0.0132   0.0154     0.0151   0.0000   \n",
       "4           0.1453 0.0002  0.1382   0.0102   0.0023     0.0031   0.0000   \n",
       "5           0.1453 0.0503  0.1382   0.0086   0.0502     0.0520   0.0000   \n",
       "10          0.1453 0.0201  0.1382   0.0065   0.0002     0.0000   0.0000   \n",
       "20          0.1453 0.0089  0.1382   0.0034   0.0115     0.0113   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1       0.0000       0.0000       0.0000         0.0000               1.2599   \n",
       "2       0.0000       0.0000       0.0000         0.0000               1.2601   \n",
       "3       0.0000       0.0000       0.0000         0.0000               1.2606   \n",
       "4       0.0000       0.0000       0.0000         0.0000               1.2610   \n",
       "5       0.0000       0.0000       0.0000         0.0000               1.2613   \n",
       "10      0.0000       0.0000       0.0000         0.0000               1.2638   \n",
       "20      0.0000       0.0000       0.0000         0.0000               1.2685   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.5364         0.0000  \n",
       "1         1.0661                1.1906         1.0657  \n",
       "2         1.0665                1.1911         1.0660  \n",
       "3         1.0671                1.1914         1.0663  \n",
       "4         1.0675                1.1921         1.0670  \n",
       "5         1.0679                1.1921         1.0669  \n",
       "10        1.0708                1.1945         1.0694  \n",
       "20        1.0763                1.1984         1.0743  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.0324: 100%|██████████| 10/10 [00:00<00:00, 51.08it/s]\n",
      "Epoch [10/10], Loss: 103.8724: 100%|██████████| 10/10 [00:00<00:00, 45.51it/s]\n",
      "Epoch [10/10], Loss: -1.2277: 100%|██████████| 10/10 [00:00<00:00, 24.38it/s]\n",
      "Epoch [10/10], Loss: 103.7001: 100%|██████████| 10/10 [00:00<00:00, 23.29it/s]\n",
      "Epoch [10/10], Loss: -1.2147: 100%|██████████| 10/10 [00:00<00:00, 16.48it/s]\n",
      "Epoch [10/10], Loss: 103.3702: 100%|██████████| 10/10 [00:00<00:00, 13.81it/s]\n",
      "Epoch [10/10], Loss: -1.0215: 100%|██████████| 10/10 [00:00<00:00, 11.39it/s]\n",
      "Epoch [10/10], Loss: 103.7327: 100%|██████████| 10/10 [00:00<00:00, 11.25it/s]\n",
      "Epoch [10/10], Loss: -1.2477: 100%|██████████| 10/10 [00:01<00:00,  9.45it/s]\n",
      "Epoch [10/10], Loss: 103.3463: 100%|██████████| 10/10 [00:01<00:00,  8.47it/s]\n",
      "Epoch [10/10], Loss: -1.5837: 100%|██████████| 10/10 [00:02<00:00,  4.87it/s]\n",
      "Epoch [10/10], Loss: 103.1211: 100%|██████████| 10/10 [00:02<00:00,  4.62it/s]\n",
      "Epoch [10/10], Loss: -5.8129: 100%|██████████| 10/10 [00:04<00:00,  2.27it/s]\n",
      "Epoch [10/10], Loss: 102.9916: 100%|██████████| 10/10 [00:04<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=10, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>1.0816</td>\n",
       "      <td>1.2032</td>\n",
       "      <td>1.0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2871</td>\n",
       "      <td>1.0995</td>\n",
       "      <td>1.2186</td>\n",
       "      <td>1.0893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3095</td>\n",
       "      <td>1.1251</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>1.1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3261</td>\n",
       "      <td>1.1443</td>\n",
       "      <td>1.2600</td>\n",
       "      <td>1.1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3455</td>\n",
       "      <td>1.1664</td>\n",
       "      <td>1.2692</td>\n",
       "      <td>1.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4985</td>\n",
       "      <td>1.3407</td>\n",
       "      <td>1.3654</td>\n",
       "      <td>1.2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0338</td>\n",
       "      <td>1.9246</td>\n",
       "      <td>1.6279</td>\n",
       "      <td>1.6433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1           0.1451 0.0462  0.1381   0.0028   0.0221     0.0240   0.0000   \n",
       "2           0.1453 0.0158  0.1383   0.0246   0.0427     0.0336   0.0000   \n",
       "3           0.1453 0.0045  0.1382   0.0137   0.0115     0.0118   0.0000   \n",
       "4           0.1453 0.0006  0.1383   0.0117   0.0018     0.0006   0.0000   \n",
       "5           0.1449 0.0336  0.1377   0.0085   0.0359     0.0393   0.0000   \n",
       "10          0.1448 0.0236  0.1370   0.0037   0.0011     0.0013   0.0000   \n",
       "20          0.1476 0.0106  0.1411   0.0031   0.0055     0.0050   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1       0.0000       0.0000       0.0000         0.0000               1.2730   \n",
       "2       0.0000       0.0000       0.0000         0.0000               1.2871   \n",
       "3       0.0000       0.0000       0.0000         0.0000               1.3095   \n",
       "4       0.0000       0.0000       0.0000         0.0000               1.3261   \n",
       "5       0.0000       0.0000       0.0000         0.0000               1.3455   \n",
       "10      0.0000       0.0000       0.0000         0.0000               1.4985   \n",
       "20      0.0000       0.0000       0.0000         0.0000               2.0338   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.5364         0.0000  \n",
       "1         1.0816                1.2032         1.0708  \n",
       "2         1.0995                1.2186         1.0893  \n",
       "3         1.1251                1.2390         1.1124  \n",
       "4         1.1443                1.2600         1.1368  \n",
       "5         1.1664                1.2692         1.1500  \n",
       "10        1.3407                1.3654         1.2810  \n",
       "20        1.9246                1.6279         1.6433  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.7198: 100%|██████████| 10/10 [00:00<00:00, 101.46it/s]\n",
      "Epoch [10/10], Loss: 98.9210: 100%|██████████| 10/10 [00:00<00:00, 104.40it/s]\n",
      "Epoch [10/10], Loss: -3.0079: 100%|██████████| 10/10 [00:00<00:00, 52.63it/s]\n",
      "Epoch [10/10], Loss: 91.4421: 100%|██████████| 10/10 [00:00<00:00, 50.28it/s]\n",
      "Epoch [10/10], Loss: -4.7455: 100%|██████████| 10/10 [00:00<00:00, 35.61it/s]\n",
      "Epoch [10/10], Loss: 89.6301: 100%|██████████| 10/10 [00:00<00:00, 32.14it/s]\n",
      "Epoch [10/10], Loss: -5.4052: 100%|██████████| 10/10 [00:00<00:00, 26.85it/s]\n",
      "Epoch [10/10], Loss: 85.7448: 100%|██████████| 10/10 [00:00<00:00, 25.09it/s]\n"
     ]
    }
   ],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4746</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>1.3637</td>\n",
       "      <td>1.2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.1998</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7363</td>\n",
       "      <td>1.6121</td>\n",
       "      <td>1.5940</td>\n",
       "      <td>1.5158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.1355</td>\n",
       "      <td>2.0325</td>\n",
       "      <td>1.8826</td>\n",
       "      <td>1.8401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.1391</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5199</td>\n",
       "      <td>2.4330</td>\n",
       "      <td>2.1781</td>\n",
       "      <td>2.1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1          0.1430 0.0497  0.1361   0.0009   0.0148     0.0294   0.0000   \n",
       "2          0.1449 0.1010  0.1397   0.0187   0.1998     0.1172   0.0000   \n",
       "3          0.1450 0.1332  0.1399   0.0072   0.1406     0.1160   0.0000   \n",
       "4          0.1486 0.0274  0.1391   0.0125   0.0855     0.1512   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1      0.0000       0.0000       0.0000         0.0000               1.4746   \n",
       "2      0.0000       0.0000       0.0000         0.0000               1.7363   \n",
       "3      0.0000       0.0000       0.0000         0.0000               2.1355   \n",
       "4      0.0000       0.0000       0.0000         0.0000               2.5199   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.5364         0.0000  \n",
       "1        1.3141                1.3637         1.2353  \n",
       "2        1.6121                1.5940         1.5158  \n",
       "3        2.0325                1.8826         1.8401  \n",
       "4        2.4330                2.1781         2.1835  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
