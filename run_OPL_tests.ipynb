{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            n_users,\n",
    "                                                            n_actions,\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, 5, simulation_data, np.arange(5) + train_size, our_x)\n",
    "\n",
    "            bpr_model = BPRModel(\n",
    "                                n_users,\n",
    "                                n_actions,\n",
    "                                emb_x.shape[1], \n",
    "                                initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                                initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob[train_data['x_idx']]\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob[val_data['x_idx']]\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "            \n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            Bloss = BPRLoss()\n",
    "\n",
    "            \n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=10)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            # fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_a, our_x = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.3 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [1, 2, 3, 4, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-26 04:59:24,663] A new study created in memory with name: no-name-b2d61af0-e90c-4080-ba9a-909a1130bce3\n",
      "[I 2025-06-26 04:59:24,822] Trial 0 finished with value: 0.03717445820466081 and parameters: {'lr': 0.0004262053797632808, 'num_epochs': 4}. Best is trial 0 with value: 0.03717445820466081.\n",
      "[I 2025-06-26 04:59:24,967] Trial 1 finished with value: 0.037239037025643906 and parameters: {'lr': 0.0001690754500255302, 'num_epochs': 3}. Best is trial 0 with value: 0.03717445820466081.\n",
      "[I 2025-06-26 04:59:25,140] Trial 2 finished with value: -0.04234912566742463 and parameters: {'lr': 0.05054988798731421, 'num_epochs': 10}. Best is trial 2 with value: -0.04234912566742463.\n",
      "[I 2025-06-26 04:59:25,301] Trial 3 finished with value: 0.030784777314379536 and parameters: {'lr': 0.01718855759267997, 'num_epochs': 5}. Best is trial 2 with value: -0.04234912566742463.\n",
      "[I 2025-06-26 04:59:25,462] Trial 4 finished with value: 0.027687151987235037 and parameters: {'lr': 0.021040283673205287, 'num_epochs': 5}. Best is trial 2 with value: -0.04234912566742463.\n",
      "[I 2025-06-26 04:59:25,615] Trial 5 finished with value: 0.03725458836338297 and parameters: {'lr': 9.757223393978171e-05, 'num_epochs': 3}. Best is trial 2 with value: -0.04234912566742463.\n",
      "[I 2025-06-26 04:59:25,771] Trial 6 finished with value: 0.036474993101362235 and parameters: {'lr': 0.003667354224341071, 'num_epochs': 4}. Best is trial 2 with value: -0.04234912566742463.\n",
      "[I 2025-06-26 04:59:25,956] Trial 7 finished with value: 0.03724436530811975 and parameters: {'lr': 4.2696368968126335e-05, 'num_epochs': 9}. Best is trial 2 with value: -0.04234912566742463.\n",
      "[I 2025-06-26 04:59:26,136] Trial 8 finished with value: 0.03722668728381959 and parameters: {'lr': 8.303311248936205e-05, 'num_epochs': 9}. Best is trial 2 with value: -0.04234912566742463.\n",
      "[I 2025-06-26 04:59:26,291] Trial 9 finished with value: 0.03725104522670125 and parameters: {'lr': 0.00012560244365653034, 'num_epochs': 3}. Best is trial 2 with value: -0.04234912566742463.\n",
      "[I 2025-06-26 04:59:26,730] A new study created in memory with name: no-name-1b0775a2-6914-4a98-abdb-9df44ba3ae18\n",
      "[I 2025-06-26 04:59:27,060] Trial 0 finished with value: 0.09074300751884169 and parameters: {'lr': 0.010731443814962462, 'num_epochs': 5}. Best is trial 0 with value: 0.09074300751884169.\n",
      "[I 2025-06-26 04:59:27,424] Trial 1 finished with value: 0.09114600839307252 and parameters: {'lr': 0.005677516906642608, 'num_epochs': 10}. Best is trial 0 with value: 0.09074300751884169.\n",
      "[I 2025-06-26 04:59:27,788] Trial 2 finished with value: 0.09375313662469664 and parameters: {'lr': 0.003320660315884614, 'num_epochs': 9}. Best is trial 0 with value: 0.09074300751884169.\n",
      "[I 2025-06-26 04:59:28,112] Trial 3 finished with value: 0.09617993426432378 and parameters: {'lr': 0.0009805406965587903, 'num_epochs': 6}. Best is trial 0 with value: 0.09074300751884169.\n",
      "[I 2025-06-26 04:59:28,426] Trial 4 finished with value: 0.001941336691554274 and parameters: {'lr': 0.08528847134378936, 'num_epochs': 5}. Best is trial 4 with value: 0.001941336691554274.\n",
      "[I 2025-06-26 04:59:28,711] Trial 5 finished with value: 0.09627777069851247 and parameters: {'lr': 0.003939317058427563, 'num_epochs': 1}. Best is trial 4 with value: 0.001941336691554274.\n",
      "[I 2025-06-26 04:59:29,062] Trial 6 finished with value: 0.09570467977304307 and parameters: {'lr': 0.0013904066946544569, 'num_epochs': 10}. Best is trial 4 with value: 0.001941336691554274.\n",
      "[I 2025-06-26 04:59:29,403] Trial 7 finished with value: -0.0029731728541683156 and parameters: {'lr': 0.05020431053059707, 'num_epochs': 7}. Best is trial 7 with value: -0.0029731728541683156.\n",
      "[I 2025-06-26 04:59:29,760] Trial 8 finished with value: 0.0965275038591028 and parameters: {'lr': 1.5960166114823485e-05, 'num_epochs': 10}. Best is trial 7 with value: -0.0029731728541683156.\n",
      "[I 2025-06-26 04:59:30,058] Trial 9 finished with value: 0.09622340130738286 and parameters: {'lr': 0.0026581300007515194, 'num_epochs': 2}. Best is trial 7 with value: -0.0029731728541683156.\n",
      "[I 2025-06-26 04:59:30,804] A new study created in memory with name: no-name-96156322-aad8-4bb9-a0d2-53ddad79ad86\n",
      "[I 2025-06-26 04:59:31,315] Trial 0 finished with value: 0.09484873062754512 and parameters: {'lr': 0.0005584160938968787, 'num_epochs': 7}. Best is trial 0 with value: 0.09484873062754512.\n",
      "[I 2025-06-26 04:59:31,761] Trial 1 finished with value: 0.09592998461737368 and parameters: {'lr': 7.119323817425254e-05, 'num_epochs': 2}. Best is trial 0 with value: 0.09484873062754512.\n",
      "[I 2025-06-26 04:59:32,255] Trial 2 finished with value: 0.07002532018234475 and parameters: {'lr': 0.011932211125178805, 'num_epochs': 6}. Best is trial 2 with value: 0.07002532018234475.\n",
      "[I 2025-06-26 04:59:32,812] Trial 3 finished with value: 0.07139322811172971 and parameters: {'lr': 0.007361065827938132, 'num_epochs': 8}. Best is trial 2 with value: 0.07002532018234475.\n",
      "[I 2025-06-26 04:59:33,314] Trial 4 finished with value: -0.004590523145427544 and parameters: {'lr': 0.04333006653962178, 'num_epochs': 6}. Best is trial 4 with value: -0.004590523145427544.\n",
      "[I 2025-06-26 04:59:33,825] Trial 5 finished with value: 0.06052196930301784 and parameters: {'lr': 0.014448517235798649, 'num_epochs': 6}. Best is trial 4 with value: -0.004590523145427544.\n",
      "[I 2025-06-26 04:59:34,353] Trial 6 finished with value: -0.023112464606110023 and parameters: {'lr': 0.03562363019123391, 'num_epochs': 9}. Best is trial 6 with value: -0.023112464606110023.\n",
      "[I 2025-06-26 04:59:34,877] Trial 7 finished with value: 0.04061534199446515 and parameters: {'lr': 0.016708912744287055, 'num_epochs': 8}. Best is trial 6 with value: -0.023112464606110023.\n",
      "[I 2025-06-26 04:59:35,425] Trial 8 finished with value: -0.021031713639329902 and parameters: {'lr': 0.09760363315752477, 'num_epochs': 10}. Best is trial 6 with value: -0.023112464606110023.\n",
      "[I 2025-06-26 04:59:35,913] Trial 9 finished with value: 0.07952311754072205 and parameters: {'lr': 0.008827980290566625, 'num_epochs': 5}. Best is trial 6 with value: -0.023112464606110023.\n",
      "[I 2025-06-26 04:59:37,001] A new study created in memory with name: no-name-b70a70df-c77d-4fd2-b815-7678ddd14090\n",
      "[I 2025-06-26 04:59:37,715] Trial 0 finished with value: -0.008215026110980628 and parameters: {'lr': 0.016697599712536686, 'num_epochs': 7}. Best is trial 0 with value: -0.008215026110980628.\n",
      "[I 2025-06-26 04:59:38,411] Trial 1 finished with value: 0.06226316194594518 and parameters: {'lr': 0.0013514815497382593, 'num_epochs': 4}. Best is trial 0 with value: -0.008215026110980628.\n",
      "[I 2025-06-26 04:59:39,228] Trial 2 finished with value: -0.07276397259409079 and parameters: {'lr': 0.029872696252105178, 'num_epochs': 10}. Best is trial 2 with value: -0.07276397259409079.\n",
      "[I 2025-06-26 04:59:39,912] Trial 3 finished with value: -0.010226849290061596 and parameters: {'lr': 0.023820945025995587, 'num_epochs': 5}. Best is trial 2 with value: -0.07276397259409079.\n",
      "[I 2025-06-26 04:59:40,529] Trial 4 finished with value: 0.06364628512905705 and parameters: {'lr': 0.00018872851087341002, 'num_epochs': 2}. Best is trial 2 with value: -0.07276397259409079.\n",
      "[I 2025-06-26 04:59:41,188] Trial 5 finished with value: 0.06367986287637757 and parameters: {'lr': 5.3812020016253573e-05, 'num_epochs': 5}. Best is trial 2 with value: -0.07276397259409079.\n",
      "[I 2025-06-26 04:59:41,899] Trial 6 finished with value: 0.058118597548472414 and parameters: {'lr': 0.0019460584765685972, 'num_epochs': 9}. Best is trial 2 with value: -0.07276397259409079.\n",
      "[I 2025-06-26 04:59:42,614] Trial 7 finished with value: 0.06360653852874114 and parameters: {'lr': 8.461736905905271e-05, 'num_epochs': 7}. Best is trial 2 with value: -0.07276397259409079.\n",
      "[I 2025-06-26 04:59:43,218] Trial 8 finished with value: 0.06365622557201737 and parameters: {'lr': 0.00035728280490013043, 'num_epochs': 1}. Best is trial 2 with value: -0.07276397259409079.\n",
      "[I 2025-06-26 04:59:43,913] Trial 9 finished with value: -0.059045151970314735 and parameters: {'lr': 0.03143791398910475, 'num_epochs': 8}. Best is trial 2 with value: -0.07276397259409079.\n",
      "[I 2025-06-26 04:59:45,379] A new study created in memory with name: no-name-370ec3cb-295b-407e-a284-bd7f9d7068a7\n",
      "[I 2025-06-26 04:59:46,128] Trial 0 finished with value: 0.09631175499060651 and parameters: {'lr': 0.00023674752655145233, 'num_epochs': 1}. Best is trial 0 with value: 0.09631175499060651.\n",
      "[I 2025-06-26 04:59:46,943] Trial 1 finished with value: 0.038526391121386366 and parameters: {'lr': 0.017425423607109032, 'num_epochs': 5}. Best is trial 1 with value: 0.038526391121386366.\n",
      "[I 2025-06-26 04:59:47,703] Trial 2 finished with value: 0.006217936929959533 and parameters: {'lr': 0.07051672105342302, 'num_epochs': 2}. Best is trial 2 with value: 0.006217936929959533.\n",
      "[I 2025-06-26 04:59:48,521] Trial 3 finished with value: 0.0948329680035523 and parameters: {'lr': 0.0020083620624556436, 'num_epochs': 5}. Best is trial 2 with value: 0.006217936929959533.\n",
      "[I 2025-06-26 04:59:49,292] Trial 4 finished with value: 0.0956652398581686 and parameters: {'lr': 0.0018121664645028514, 'num_epochs': 3}. Best is trial 2 with value: 0.006217936929959533.\n",
      "[I 2025-06-26 04:59:50,063] Trial 5 finished with value: 0.09633974874171776 and parameters: {'lr': 2.6462209220719967e-05, 'num_epochs': 2}. Best is trial 2 with value: 0.006217936929959533.\n",
      "[I 2025-06-26 04:59:50,964] Trial 6 finished with value: 0.09628109424470932 and parameters: {'lr': 7.356517471435574e-05, 'num_epochs': 9}. Best is trial 2 with value: 0.006217936929959533.\n",
      "[I 2025-06-26 04:59:51,800] Trial 7 finished with value: 0.09633091804453336 and parameters: {'lr': 2.802979418326702e-05, 'num_epochs': 6}. Best is trial 2 with value: 0.006217936929959533.\n",
      "[I 2025-06-26 04:59:52,660] Trial 8 finished with value: -0.03337178473327573 and parameters: {'lr': 0.034343391776306076, 'num_epochs': 6}. Best is trial 8 with value: -0.03337178473327573.\n",
      "[I 2025-06-26 04:59:53,509] Trial 9 finished with value: -0.04152669587487112 and parameters: {'lr': 0.051630266889985024, 'num_epochs': 6}. Best is trial 9 with value: -0.04152669587487112.\n",
      "[I 2025-06-26 04:59:55,905] A new study created in memory with name: no-name-02c6bdfb-8aa7-43ed-8672-775bf3628f98\n",
      "[I 2025-06-26 04:59:57,492] Trial 0 finished with value: 0.10210401735537133 and parameters: {'lr': 0.00011600023702232875, 'num_epochs': 1}. Best is trial 0 with value: 0.10210401735537133.\n",
      "[I 2025-06-26 04:59:59,181] Trial 1 finished with value: 0.0974793785941171 and parameters: {'lr': 0.0011585576899175482, 'num_epochs': 4}. Best is trial 1 with value: 0.0974793785941171.\n",
      "[I 2025-06-26 05:00:00,808] Trial 2 finished with value: 0.10141931234096382 and parameters: {'lr': 0.00043055040241531226, 'num_epochs': 2}. Best is trial 1 with value: 0.0974793785941171.\n",
      "[W 2025-06-26 05:00:02,751] Trial 3 failed with parameters: {'lr': 0.0762198490775178, 'num_epochs': 10} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-06-26 05:00:02,751] Trial 3 failed with value nan.\n",
      "[I 2025-06-26 05:00:04,545] Trial 4 finished with value: 0.10181374686138286 and parameters: {'lr': 6.598884983680796e-05, 'num_epochs': 7}. Best is trial 1 with value: 0.0974793785941171.\n",
      "[I 2025-06-26 05:00:06,168] Trial 5 finished with value: 0.09402191258281642 and parameters: {'lr': 0.0035856421306209325, 'num_epochs': 2}. Best is trial 5 with value: 0.09402191258281642.\n",
      "[I 2025-06-26 05:00:07,964] Trial 6 finished with value: -0.08848149353363782 and parameters: {'lr': 0.029256688157578408, 'num_epochs': 7}. Best is trial 6 with value: -0.08848149353363782.\n",
      "[I 2025-06-26 05:00:09,817] Trial 7 finished with value: 0.09730663082879226 and parameters: {'lr': 0.0006926963865061758, 'num_epochs': 7}. Best is trial 6 with value: -0.08848149353363782.\n",
      "[I 2025-06-26 05:00:11,389] Trial 8 finished with value: 0.10165129307302383 and parameters: {'lr': 0.0006024407729702174, 'num_epochs': 1}. Best is trial 6 with value: -0.08848149353363782.\n",
      "[I 2025-06-26 05:00:13,018] Trial 9 finished with value: 0.10216364398148242 and parameters: {'lr': 1.6184726180482523e-05, 'num_epochs': 3}. Best is trial 6 with value: -0.08848149353363782.\n",
      "[I 2025-06-26 05:00:18,054] A new study created in memory with name: no-name-9d9093f0-99ac-47f3-a6d5-1251996e51ec\n",
      "[I 2025-06-26 05:00:21,794] Trial 0 finished with value: -0.08332454047297383 and parameters: {'lr': 0.019541044020187134, 'num_epochs': 7}. Best is trial 0 with value: -0.08332454047297383.\n",
      "[I 2025-06-26 05:00:25,321] Trial 1 finished with value: -0.04279697166432933 and parameters: {'lr': 0.014355918641906345, 'num_epochs': 4}. Best is trial 0 with value: -0.08332454047297383.\n",
      "[I 2025-06-26 05:00:28,683] Trial 2 finished with value: 0.10842261063407578 and parameters: {'lr': 0.00025031898420053094, 'num_epochs': 2}. Best is trial 0 with value: -0.08332454047297383.\n",
      "[I 2025-06-26 05:00:31,963] Trial 3 finished with value: 0.10879229543622261 and parameters: {'lr': 1.2432496436794119e-05, 'num_epochs': 1}. Best is trial 0 with value: -0.08332454047297383.\n",
      "[I 2025-06-26 05:00:35,416] Trial 4 finished with value: -0.07388337426754998 and parameters: {'lr': 0.03540396622202652, 'num_epochs': 3}. Best is trial 0 with value: -0.08332454047297383.\n",
      "[I 2025-06-26 05:00:38,728] Trial 5 finished with value: 0.10689173112962874 and parameters: {'lr': 0.0022282190407147138, 'num_epochs': 1}. Best is trial 0 with value: -0.08332454047297383.\n",
      "[I 2025-06-26 05:00:42,459] Trial 6 finished with value: 0.07508924986407238 and parameters: {'lr': 0.0019367371527722783, 'num_epochs': 7}. Best is trial 0 with value: -0.08332454047297383.\n",
      "[I 2025-06-26 05:00:45,846] Trial 7 finished with value: -0.08946505369524527 and parameters: {'lr': 0.04843696436013344, 'num_epochs': 2}. Best is trial 7 with value: -0.08946505369524527.\n",
      "[W 2025-06-26 05:00:49,427] Trial 8 failed with parameters: {'lr': 0.09040869514720482, 'num_epochs': 5} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-06-26 05:00:49,428] Trial 8 failed with value nan.\n",
      "[I 2025-06-26 05:00:52,809] Trial 9 finished with value: 0.10375844839083846 and parameters: {'lr': 0.002143985405784387, 'num_epochs': 2}. Best is trial 7 with value: -0.08946505369524527.\n"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1904</td>\n",
       "      <td>0.1899</td>\n",
       "      <td>0.1524</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.5069</td>\n",
       "      <td>1.0378</td>\n",
       "      <td>1.1542</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>0.6694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>1.1592</td>\n",
       "      <td>1.2819</td>\n",
       "      <td>1.0435</td>\n",
       "      <td>0.8357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>1.3957</td>\n",
       "      <td>1.5383</td>\n",
       "      <td>1.2811</td>\n",
       "      <td>1.1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>1.5903</td>\n",
       "      <td>1.7414</td>\n",
       "      <td>1.5547</td>\n",
       "      <td>1.3738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.1317</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>1.8113</td>\n",
       "      <td>1.9436</td>\n",
       "      <td>1.9940</td>\n",
       "      <td>1.8124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>2.1439</td>\n",
       "      <td>2.2478</td>\n",
       "      <td>2.7655</td>\n",
       "      <td>2.5865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1889</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>2.1254</td>\n",
       "      <td>2.2207</td>\n",
       "      <td>2.8812</td>\n",
       "      <td>2.6593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   \n",
       "1           0.1904 0.1899  0.1524   0.0293   0.1332     0.5069   \n",
       "2           0.1913 0.1715  0.1413   0.0089   0.0080     0.1411   \n",
       "3           0.1934 0.1934  0.1476   0.0063   0.0059     0.2330   \n",
       "4           0.1916 0.1916  0.1409   0.0330   0.1543     0.4980   \n",
       "5           0.1923 0.1923  0.1428   0.0013   0.1317     0.3333   \n",
       "10          0.1890 0.0114  0.1402   0.0054   0.0385     0.0718   \n",
       "20          0.1889 0.0217  0.1330   0.0297   0.0198     0.0187   \n",
       "\n",
       "    action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                0.3386        0.0000                0.5364         0.0000  \n",
       "1                1.0378        1.1542                0.8883         0.6694  \n",
       "2                1.1592        1.2819                1.0435         0.8357  \n",
       "3                1.3957        1.5383                1.2811         1.1333  \n",
       "4                1.5903        1.7414                1.5547         1.3738  \n",
       "5                1.8113        1.9436                1.9940         1.8124  \n",
       "10               2.1439        2.2478                2.7655         2.5865  \n",
       "20               2.1254        2.2207                2.8812         2.6593  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-26 06:00:47,088] A new study created in memory with name: no-name-f919595e-a7c7-4192-82a0-88df61c0332c\n",
      "[I 2025-06-26 06:00:47,250] Trial 0 finished with value: 0.037067933837222816 and parameters: {'lr': 0.0005225653346301143, 'num_epochs': 6}. Best is trial 0 with value: 0.037067933837222816.\n",
      "[I 2025-06-26 06:00:47,387] Trial 1 finished with value: 0.037253001270894676 and parameters: {'lr': 0.00027374423899567256, 'num_epochs': 1}. Best is trial 0 with value: 0.037067933837222816.\n",
      "[I 2025-06-26 06:00:47,560] Trial 2 finished with value: -0.014201574651007048 and parameters: {'lr': 0.03807918977276577, 'num_epochs': 9}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:47,701] Trial 3 finished with value: 0.03726279086963771 and parameters: {'lr': 5.0536893389243686e-05, 'num_epochs': 2}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:47,859] Trial 4 finished with value: 0.037096458943192856 and parameters: {'lr': 0.000574430717237854, 'num_epochs': 5}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:47,998] Trial 5 finished with value: 0.03724411994920657 and parameters: {'lr': 0.0006331207919239826, 'num_epochs': 1}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:48,154] Trial 6 finished with value: 0.03724627132928093 and parameters: {'lr': 7.477988541885005e-05, 'num_epochs': 5}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:48,309] Trial 7 finished with value: 0.030308328315091976 and parameters: {'lr': 0.02463928556847755, 'num_epochs': 3}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:48,490] Trial 8 finished with value: 0.03651941588136645 and parameters: {'lr': 0.001346442331046115, 'num_epochs': 9}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:48,647] Trial 9 finished with value: 0.03671392596654394 and parameters: {'lr': 0.0017028311394250487, 'num_epochs': 5}. Best is trial 2 with value: -0.014201574651007048.\n"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.2204</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0874</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4769</td>\n",
       "      <td>1.3154</td>\n",
       "      <td>1.5475</td>\n",
       "      <td>1.3733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1          0.1448 0.2204  0.1381   0.0131   0.0874     0.1590   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1      0.0000       0.0000       0.0000         0.0000               1.4769   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.5364         0.0000  \n",
       "1        1.3154                1.5475         1.3733  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trainer_trial() got an unexpected keyword argument 'num_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df6 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: trainer_trial() got an unexpected keyword argument 'num_epochs'"
     ]
    }
   ],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=10, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>1.0816</td>\n",
       "      <td>1.2032</td>\n",
       "      <td>1.0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2871</td>\n",
       "      <td>1.0995</td>\n",
       "      <td>1.2186</td>\n",
       "      <td>1.0893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3095</td>\n",
       "      <td>1.1251</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>1.1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3261</td>\n",
       "      <td>1.1443</td>\n",
       "      <td>1.2600</td>\n",
       "      <td>1.1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3455</td>\n",
       "      <td>1.1664</td>\n",
       "      <td>1.2692</td>\n",
       "      <td>1.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4985</td>\n",
       "      <td>1.3407</td>\n",
       "      <td>1.3654</td>\n",
       "      <td>1.2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0338</td>\n",
       "      <td>1.9246</td>\n",
       "      <td>1.6279</td>\n",
       "      <td>1.6433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1           0.1451 0.0462  0.1381   0.0028   0.0221     0.0240   0.0000   \n",
       "2           0.1453 0.0158  0.1383   0.0246   0.0427     0.0336   0.0000   \n",
       "3           0.1453 0.0045  0.1382   0.0137   0.0115     0.0118   0.0000   \n",
       "4           0.1453 0.0006  0.1383   0.0117   0.0018     0.0006   0.0000   \n",
       "5           0.1449 0.0336  0.1377   0.0085   0.0359     0.0393   0.0000   \n",
       "10          0.1448 0.0236  0.1370   0.0037   0.0011     0.0013   0.0000   \n",
       "20          0.1476 0.0106  0.1411   0.0031   0.0055     0.0050   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1       0.0000       0.0000       0.0000         0.0000               1.2730   \n",
       "2       0.0000       0.0000       0.0000         0.0000               1.2871   \n",
       "3       0.0000       0.0000       0.0000         0.0000               1.3095   \n",
       "4       0.0000       0.0000       0.0000         0.0000               1.3261   \n",
       "5       0.0000       0.0000       0.0000         0.0000               1.3455   \n",
       "10      0.0000       0.0000       0.0000         0.0000               1.4985   \n",
       "20      0.0000       0.0000       0.0000         0.0000               2.0338   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.5364         0.0000  \n",
       "1         1.0816                1.2032         1.0708  \n",
       "2         1.0995                1.2186         1.0893  \n",
       "3         1.1251                1.2390         1.1124  \n",
       "4         1.1443                1.2600         1.1368  \n",
       "5         1.1664                1.2692         1.1500  \n",
       "10        1.3407                1.3654         1.2810  \n",
       "20        1.9246                1.6279         1.6433  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.7198: 100%|██████████| 10/10 [00:00<00:00, 101.46it/s]\n",
      "Epoch [10/10], Loss: 98.9210: 100%|██████████| 10/10 [00:00<00:00, 104.40it/s]\n",
      "Epoch [10/10], Loss: -3.0079: 100%|██████████| 10/10 [00:00<00:00, 52.63it/s]\n",
      "Epoch [10/10], Loss: 91.4421: 100%|██████████| 10/10 [00:00<00:00, 50.28it/s]\n",
      "Epoch [10/10], Loss: -4.7455: 100%|██████████| 10/10 [00:00<00:00, 35.61it/s]\n",
      "Epoch [10/10], Loss: 89.6301: 100%|██████████| 10/10 [00:00<00:00, 32.14it/s]\n",
      "Epoch [10/10], Loss: -5.4052: 100%|██████████| 10/10 [00:00<00:00, 26.85it/s]\n",
      "Epoch [10/10], Loss: 85.7448: 100%|██████████| 10/10 [00:00<00:00, 25.09it/s]\n"
     ]
    }
   ],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4746</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>1.3637</td>\n",
       "      <td>1.2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.1998</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7363</td>\n",
       "      <td>1.6121</td>\n",
       "      <td>1.5940</td>\n",
       "      <td>1.5158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.1355</td>\n",
       "      <td>2.0325</td>\n",
       "      <td>1.8826</td>\n",
       "      <td>1.8401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.1391</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5199</td>\n",
       "      <td>2.4330</td>\n",
       "      <td>2.1781</td>\n",
       "      <td>2.1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1          0.1430 0.0497  0.1361   0.0009   0.0148     0.0294   0.0000   \n",
       "2          0.1449 0.1010  0.1397   0.0187   0.1998     0.1172   0.0000   \n",
       "3          0.1450 0.1332  0.1399   0.0072   0.1406     0.1160   0.0000   \n",
       "4          0.1486 0.0274  0.1391   0.0125   0.0855     0.1512   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1      0.0000       0.0000       0.0000         0.0000               1.4746   \n",
       "2      0.0000       0.0000       0.0000         0.0000               1.7363   \n",
       "3      0.0000       0.0000       0.0000         0.0000               2.1355   \n",
       "4      0.0000       0.0000       0.0000         0.0000               2.5199   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.5364         0.0000  \n",
       "1        1.3141                1.3637         1.2353  \n",
       "2        1.6121                1.5940         1.5158  \n",
       "3        2.0325                1.8826         1.8401  \n",
       "4        2.4330                2.1781         2.1835  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
