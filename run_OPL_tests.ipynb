{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import gym\n",
    "import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  val_size=2000\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            dataset['env'],\n",
    "                                                            pi_0,\n",
    "                                                            train_size + val_size\n",
    "                                                            )\n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "\n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            # bpr_model = BPRModel(\n",
    "            #                     n_users,\n",
    "            #                     n_actions,\n",
    "            #                     emb_x.shape[1], \n",
    "            #                     initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "            #                     initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "            #                     )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "                reg_dm = 0.0\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            # Bloss = BPRLoss()\n",
    "\n",
    "            \n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=5)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            # fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "            reg_dm = 0.0\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 300,\n",
    "                    n_users = 300,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.2 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [200, 400, 600, 800, 1000, 2000, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 08:40:57,366] A new study created in memory with name: no-name-3a0ee296-9bb6-4e2d-962b-c29221488835\n",
      "[I 2025-07-12 08:40:58,232] Trial 0 finished with value: -0.03396647770976957 and parameters: {'lr': 0.01398264025148408, 'num_epochs': 2}. Best is trial 0 with value: -0.03396647770976957.\n",
      "[I 2025-07-12 08:40:59,150] Trial 1 finished with value: -0.035891722931927744 and parameters: {'lr': 0.012063129849174296, 'num_epochs': 7}. Best is trial 1 with value: -0.035891722931927744.\n",
      "[I 2025-07-12 08:40:59,938] Trial 2 finished with value: -0.033900828544134703 and parameters: {'lr': 0.018759139203160833, 'num_epochs': 1}. Best is trial 1 with value: -0.035891722931927744.\n",
      "[I 2025-07-12 08:41:00,768] Trial 3 finished with value: -0.034003455889572 and parameters: {'lr': 0.002702916526479687, 'num_epochs': 5}. Best is trial 1 with value: -0.035891722931927744.\n",
      "[I 2025-07-12 08:41:01,672] Trial 4 finished with value: -0.046036645415588655 and parameters: {'lr': 0.047047437049545796, 'num_epochs': 10}. Best is trial 4 with value: -0.046036645415588655.\n",
      "[I 2025-07-12 08:41:16,869] A new study created in memory with name: no-name-c1817365-d7e3-475c-b6be-a91f955429d5\n",
      "[I 2025-07-12 08:41:18,552] Trial 0 finished with value: -0.023468076998138555 and parameters: {'lr': 1.244873362798433e-05, 'num_epochs': 3}. Best is trial 0 with value: -0.023468076998138555.\n",
      "[I 2025-07-12 08:41:20,259] Trial 1 finished with value: -0.023832660749707725 and parameters: {'lr': 0.002062224427714753, 'num_epochs': 4}. Best is trial 1 with value: -0.023832660749707725.\n",
      "[I 2025-07-12 08:41:22,205] Trial 2 finished with value: -0.0540431292217934 and parameters: {'lr': 0.03919351445404722, 'num_epochs': 10}. Best is trial 2 with value: -0.0540431292217934.\n",
      "[I 2025-07-12 08:41:24,508] Trial 3 finished with value: -0.031336315564860995 and parameters: {'lr': 0.009245577912302457, 'num_epochs': 10}. Best is trial 2 with value: -0.0540431292217934.\n",
      "[I 2025-07-12 08:41:26,612] Trial 4 finished with value: -0.02486388838327288 and parameters: {'lr': 0.011783690920225843, 'num_epochs': 2}. Best is trial 2 with value: -0.0540431292217934.\n",
      "[I 2025-07-12 08:41:46,508] A new study created in memory with name: no-name-ab4a3849-14f7-4f1b-b99d-a4c8dd136fa8\n",
      "[I 2025-07-12 08:41:49,061] Trial 0 finished with value: -0.028455018762379065 and parameters: {'lr': 2.129083207291623e-05, 'num_epochs': 5}. Best is trial 0 with value: -0.028455018762379065.\n",
      "[I 2025-07-12 08:41:51,533] Trial 1 finished with value: -0.028460043128861127 and parameters: {'lr': 5.784747974976294e-05, 'num_epochs': 4}. Best is trial 1 with value: -0.028460043128861127.\n",
      "[I 2025-07-12 08:41:54,057] Trial 2 finished with value: -0.02845201574391412 and parameters: {'lr': 3.6260094794988484e-05, 'num_epochs': 1}. Best is trial 1 with value: -0.028460043128861127.\n",
      "[I 2025-07-12 08:41:56,747] Trial 3 finished with value: -0.02939231539673924 and parameters: {'lr': 0.0021928443813069845, 'num_epochs': 9}. Best is trial 3 with value: -0.02939231539673924.\n",
      "[I 2025-07-12 08:41:59,393] Trial 4 finished with value: -0.029184629123902447 and parameters: {'lr': 0.0020424234608703456, 'num_epochs': 8}. Best is trial 3 with value: -0.02939231539673924.\n",
      "[I 2025-07-12 08:42:18,568] A new study created in memory with name: no-name-51fbe99f-4b56-4323-8aa3-f00d1170a4d2\n",
      "[I 2025-07-12 08:42:22,357] Trial 0 finished with value: -0.039063939399723316 and parameters: {'lr': 0.01793785117199208, 'num_epochs': 10}. Best is trial 0 with value: -0.039063939399723316.\n",
      "[I 2025-07-12 08:42:26,151] Trial 1 finished with value: -0.02286349531446586 and parameters: {'lr': 0.000991571459026842, 'num_epochs': 10}. Best is trial 0 with value: -0.039063939399723316.\n",
      "[I 2025-07-12 08:42:29,761] Trial 2 finished with value: -0.02246130858200917 and parameters: {'lr': 0.0016380046454131858, 'num_epochs': 2}. Best is trial 0 with value: -0.039063939399723316.\n",
      "[I 2025-07-12 08:42:33,329] Trial 3 finished with value: -0.02230447582962946 and parameters: {'lr': 4.196467342768051e-05, 'num_epochs': 2}. Best is trial 0 with value: -0.039063939399723316.\n",
      "[I 2025-07-12 08:42:36,873] Trial 4 finished with value: -0.02230321665175513 and parameters: {'lr': 3.9823323769126284e-05, 'num_epochs': 2}. Best is trial 0 with value: -0.039063939399723316.\n",
      "[I 2025-07-12 08:42:58,693] A new study created in memory with name: no-name-a440d138-1c66-40c2-8d99-9c21d9c7cc9c\n",
      "[I 2025-07-12 08:43:03,676] Trial 0 finished with value: -0.019361688171596107 and parameters: {'lr': 0.0007167903116666362, 'num_epochs': 10}. Best is trial 0 with value: -0.019361688171596107.\n",
      "[I 2025-07-12 08:43:08,467] Trial 1 finished with value: -0.04865436475515826 and parameters: {'lr': 0.032793498991004935, 'num_epochs': 6}. Best is trial 1 with value: -0.04865436475515826.\n",
      "[I 2025-07-12 08:43:12,955] Trial 2 finished with value: -0.019545010115324876 and parameters: {'lr': 0.007605896700480422, 'num_epochs': 1}. Best is trial 1 with value: -0.04865436475515826.\n",
      "[I 2025-07-12 08:43:18,504] Trial 3 finished with value: -0.04717676236561973 and parameters: {'lr': 0.07892924432293011, 'num_epochs': 9}. Best is trial 1 with value: -0.04865436475515826.\n",
      "[I 2025-07-12 08:43:23,955] Trial 4 finished with value: -0.021637370206109188 and parameters: {'lr': 0.006333878425287347, 'num_epochs': 5}. Best is trial 1 with value: -0.04865436475515826.\n",
      "[I 2025-07-12 08:43:53,579] A new study created in memory with name: no-name-4a93ca49-c744-4290-962b-386a22cfa657\n",
      "[I 2025-07-12 08:44:02,820] Trial 0 finished with value: -0.025817674478849625 and parameters: {'lr': 7.51851782230411e-05, 'num_epochs': 1}. Best is trial 0 with value: -0.025817674478849625.\n",
      "[I 2025-07-12 08:44:12,202] Trial 1 finished with value: -0.03239267925406038 and parameters: {'lr': 0.0031060399514452955, 'num_epochs': 8}. Best is trial 1 with value: -0.03239267925406038.\n",
      "[I 2025-07-12 08:44:20,982] Trial 2 finished with value: -0.036599197691530125 and parameters: {'lr': 0.013857708317177645, 'num_epochs': 3}. Best is trial 2 with value: -0.036599197691530125.\n",
      "[I 2025-07-12 08:44:30,152] Trial 3 finished with value: -0.025990064866609645 and parameters: {'lr': 0.0002146129636035798, 'num_epochs': 10}. Best is trial 2 with value: -0.036599197691530125.\n",
      "[I 2025-07-12 08:44:38,947] Trial 4 finished with value: -0.02660519701352301 and parameters: {'lr': 0.0014585411241632648, 'num_epochs': 4}. Best is trial 2 with value: -0.036599197691530125.\n",
      "[I 2025-07-12 08:45:22,930] A new study created in memory with name: no-name-f1f777fd-af5e-4e0a-8478-9394652163cf\n",
      "[I 2025-07-12 08:45:43,002] Trial 0 finished with value: -0.012761402579511416 and parameters: {'lr': 0.00013779584333251785, 'num_epochs': 9}. Best is trial 0 with value: -0.012761402579511416.\n",
      "[I 2025-07-12 08:46:01,319] Trial 1 finished with value: -0.012755955754277633 and parameters: {'lr': 9.54776203931226e-05, 'num_epochs': 3}. Best is trial 0 with value: -0.012761402579511416.\n",
      "[W 2025-07-12 08:46:20,138] Trial 2 failed with parameters: {'lr': 0.08757069653864945, 'num_epochs': 4} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-07-12 08:46:20,139] Trial 2 failed with value nan.\n",
      "[I 2025-07-12 08:46:38,672] Trial 3 finished with value: -0.012536734227708665 and parameters: {'lr': 0.0013039647069606846, 'num_epochs': 2}. Best is trial 0 with value: -0.012761402579511416.\n",
      "[I 2025-07-12 08:46:57,567] Trial 4 finished with value: -0.012760443364226347 and parameters: {'lr': 9.792907264219642e-05, 'num_epochs': 5}. Best is trial 0 with value: -0.012761402579511416.\n"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.2742</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.1010</td>\n",
       "      <td>1.3574</td>\n",
       "      <td>1.4125</td>\n",
       "      <td>0.5703</td>\n",
       "      <td>0.5167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>1.8604</td>\n",
       "      <td>1.9271</td>\n",
       "      <td>0.8334</td>\n",
       "      <td>0.8256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.2665</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.0529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>-0.0631</td>\n",
       "      <td>1.3552</td>\n",
       "      <td>1.4082</td>\n",
       "      <td>0.6658</td>\n",
       "      <td>0.6784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>-0.0046</td>\n",
       "      <td>-0.0987</td>\n",
       "      <td>1.7130</td>\n",
       "      <td>1.7587</td>\n",
       "      <td>0.9516</td>\n",
       "      <td>0.9624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.0814</td>\n",
       "      <td>0.6210</td>\n",
       "      <td>0.6265</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>0.2971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.2708</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.0106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0             0.0177 0.0272  0.0000   0.0164   0.0201     0.0282   \n",
       "200           0.0160 0.0000  0.0000   0.0178   0.0167    -0.1010   \n",
       "400           0.0139 0.0000  0.0000   0.0177  -0.0007     0.0038   \n",
       "600           0.0167 0.0269  0.0000   0.0155   0.0169     0.0212   \n",
       "800           0.0181 0.0000  0.0000   0.0095   0.0014    -0.0631   \n",
       "1000          0.0172 0.0000  0.0000   0.0108  -0.0046    -0.0987   \n",
       "2000          0.0153 0.0772  0.0000   0.0212   0.1033     0.0814   \n",
       "4000          0.0145 0.0188  0.0000   0.0168   0.0177     0.0198   \n",
       "\n",
       "      action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                  0.2742        0.0000                0.2684         0.0000  \n",
       "200                1.3574        1.4125                0.5703         0.5167  \n",
       "400                1.8604        1.9271                0.8334         0.8256  \n",
       "600                0.2665        0.1234                0.2750         0.0529  \n",
       "800                1.3552        1.4082                0.6658         0.6784  \n",
       "1000               1.7130        1.7587                0.9516         0.9624  \n",
       "2000               0.6210        0.6265                0.3805         0.2971  \n",
       "4000               0.2708        0.0172                0.2694         0.0106  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df5 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 117\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[1;32m    116\u001b[0m     policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(softmax(our_x \u001b[38;5;241m@\u001b[39m our_a\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     conv_results\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighberhoodmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_policy_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    118\u001b[0m     conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(calc_reward(dataset, policy), conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    119\u001b[0m     conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((emb_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)), np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((original_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))])\n",
      "File \u001b[0;32m/code/simulation_utils.py:208\u001b[0m, in \u001b[0;36meval_policy\u001b[0;34m(model, test_data, original_policy_prob, policy)\u001b[0m\n\u001b[1;32m    204\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# reward = test_data['q_x_a'][test_data['x_idx'], actions]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# res.append(reward.mean())\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pscore \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_policy_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    210\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(dm\u001b[38;5;241m.\u001b[39mestimate_policy_value(policy, scores))\n\u001b[1;32m    211\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(dr\u001b[38;5;241m.\u001b[39mestimate_policy_value(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], policy, scores, pscore\u001b[38;5;241m=\u001b[39mpscore))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 15 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.2204</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0874</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4769</td>\n",
       "      <td>1.3154</td>\n",
       "      <td>1.5475</td>\n",
       "      <td>1.3733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1          0.1448 0.2204  0.1381   0.0131   0.0874     0.1590   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1      0.0000       0.0000       0.0000         0.0000               1.4769   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.5364         0.0000  \n",
       "1        1.3154                1.5475         1.3733  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trainer_trial() got an unexpected keyword argument 'num_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df6 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: trainer_trial() got an unexpected keyword argument 'num_epochs'"
     ]
    }
   ],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>1.0816</td>\n",
       "      <td>1.2032</td>\n",
       "      <td>1.0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2871</td>\n",
       "      <td>1.0995</td>\n",
       "      <td>1.2186</td>\n",
       "      <td>1.0893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3095</td>\n",
       "      <td>1.1251</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>1.1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3261</td>\n",
       "      <td>1.1443</td>\n",
       "      <td>1.2600</td>\n",
       "      <td>1.1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3455</td>\n",
       "      <td>1.1664</td>\n",
       "      <td>1.2692</td>\n",
       "      <td>1.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4985</td>\n",
       "      <td>1.3407</td>\n",
       "      <td>1.3654</td>\n",
       "      <td>1.2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0338</td>\n",
       "      <td>1.9246</td>\n",
       "      <td>1.6279</td>\n",
       "      <td>1.6433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1           0.1451 0.0462  0.1381   0.0028   0.0221     0.0240   0.0000   \n",
       "2           0.1453 0.0158  0.1383   0.0246   0.0427     0.0336   0.0000   \n",
       "3           0.1453 0.0045  0.1382   0.0137   0.0115     0.0118   0.0000   \n",
       "4           0.1453 0.0006  0.1383   0.0117   0.0018     0.0006   0.0000   \n",
       "5           0.1449 0.0336  0.1377   0.0085   0.0359     0.0393   0.0000   \n",
       "10          0.1448 0.0236  0.1370   0.0037   0.0011     0.0013   0.0000   \n",
       "20          0.1476 0.0106  0.1411   0.0031   0.0055     0.0050   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1       0.0000       0.0000       0.0000         0.0000               1.2730   \n",
       "2       0.0000       0.0000       0.0000         0.0000               1.2871   \n",
       "3       0.0000       0.0000       0.0000         0.0000               1.3095   \n",
       "4       0.0000       0.0000       0.0000         0.0000               1.3261   \n",
       "5       0.0000       0.0000       0.0000         0.0000               1.3455   \n",
       "10      0.0000       0.0000       0.0000         0.0000               1.4985   \n",
       "20      0.0000       0.0000       0.0000         0.0000               2.0338   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.5364         0.0000  \n",
       "1         1.0816                1.2032         1.0708  \n",
       "2         1.0995                1.2186         1.0893  \n",
       "3         1.1251                1.2390         1.1124  \n",
       "4         1.1443                1.2600         1.1368  \n",
       "5         1.1664                1.2692         1.1500  \n",
       "10        1.3407                1.3654         1.2810  \n",
       "20        1.9246                1.6279         1.6433  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.7198: 100%|| 10/10 [00:00<00:00, 101.46it/s]\n",
      "Epoch [10/10], Loss: 98.9210: 100%|| 10/10 [00:00<00:00, 104.40it/s]\n",
      "Epoch [10/10], Loss: -3.0079: 100%|| 10/10 [00:00<00:00, 52.63it/s]\n",
      "Epoch [10/10], Loss: 91.4421: 100%|| 10/10 [00:00<00:00, 50.28it/s]\n",
      "Epoch [10/10], Loss: -4.7455: 100%|| 10/10 [00:00<00:00, 35.61it/s]\n",
      "Epoch [10/10], Loss: 89.6301: 100%|| 10/10 [00:00<00:00, 32.14it/s]\n",
      "Epoch [10/10], Loss: -5.4052: 100%|| 10/10 [00:00<00:00, 26.85it/s]\n",
      "Epoch [10/10], Loss: 85.7448: 100%|| 10/10 [00:00<00:00, 25.09it/s]\n"
     ]
    }
   ],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4746</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>1.3637</td>\n",
       "      <td>1.2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.1998</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7363</td>\n",
       "      <td>1.6121</td>\n",
       "      <td>1.5940</td>\n",
       "      <td>1.5158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.1355</td>\n",
       "      <td>2.0325</td>\n",
       "      <td>1.8826</td>\n",
       "      <td>1.8401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.1391</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5199</td>\n",
       "      <td>2.4330</td>\n",
       "      <td>2.1781</td>\n",
       "      <td>2.1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1          0.1430 0.0497  0.1361   0.0009   0.0148     0.0294   0.0000   \n",
       "2          0.1449 0.1010  0.1397   0.0187   0.1998     0.1172   0.0000   \n",
       "3          0.1450 0.1332  0.1399   0.0072   0.1406     0.1160   0.0000   \n",
       "4          0.1486 0.0274  0.1391   0.0125   0.0855     0.1512   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1      0.0000       0.0000       0.0000         0.0000               1.4746   \n",
       "2      0.0000       0.0000       0.0000         0.0000               1.7363   \n",
       "3      0.0000       0.0000       0.0000         0.0000               2.1355   \n",
       "4      0.0000       0.0000       0.0000         0.0000               2.5199   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.5364         0.0000  \n",
       "1        1.3141                1.3637         1.2353  \n",
       "2        1.6121                1.5940         1.5158  \n",
       "3        2.0325                1.8826         1.8401  \n",
       "4        2.4330                2.1781         2.1835  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
