{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import gym\n",
    "import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  val_size=2000\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-3, log=True)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            dataset['env'],\n",
    "                                                            pi_0,\n",
    "                                                            train_size + val_size\n",
    "                                                            )\n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "\n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            regression_model = RegressionModel(\n",
    "                                    n_actions=n_actions,\n",
    "                                    action_context=our_x,\n",
    "                                    base_model=LogisticRegression(random_state=12345)\n",
    "                                    )\n",
    "            \n",
    "            regression_model.fit(\n",
    "                                train_data['x'], \n",
    "                                train_data['a'],\n",
    "                                train_data['r'],\n",
    "                                original_policy_prob[train_data['x_idx'],\n",
    "                                train_data['a']].squeeze()\n",
    "                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "                \n",
    "            # Bloss = BPRLoss()\n",
    "            # bpr_model = BPRModel(\n",
    "            #                     n_users,\n",
    "            #                     n_actions,\n",
    "            #                     emb_x.shape[1], \n",
    "            #                     initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "            #                     initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "            #                     )\n",
    "\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials=10)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            # fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 4,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.2 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 3\n",
    "batch_size = 200\n",
    "num_neighbors = 101\n",
    "num_rounds_list = [30000, 60000, 80000, 90000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-26 21:06:34,155] A new study created in memory with name: no-name-84a6ac5c-d77f-4faa-8f6d-2955cde5cadb\n",
      "[I 2025-07-26 21:06:34,214] Trial 0 finished with value: 0.022273013326316357 and parameters: {'lr': 0.001290983955516664, 'num_epochs': 3}. Best is trial 0 with value: 0.022273013326316357.\n",
      "[I 2025-07-26 21:06:34,293] Trial 1 finished with value: 0.02514042769921196 and parameters: {'lr': 0.045469842648293346, 'num_epochs': 5}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:06:34,400] Trial 2 finished with value: 0.022163366101095094 and parameters: {'lr': 2.9236227752508726e-05, 'num_epochs': 9}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:06:34,462] Trial 3 finished with value: 0.021891265414385436 and parameters: {'lr': 0.00015372381897601518, 'num_epochs': 2}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:06:34,541] Trial 4 finished with value: 0.02210689621629705 and parameters: {'lr': 4.2466934782199485e-05, 'num_epochs': 5}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:06:34,656] Trial 5 finished with value: 0.022101972529661163 and parameters: {'lr': 0.0001089509206285136, 'num_epochs': 10}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:06:34,768] Trial 6 finished with value: 0.02314616974308388 and parameters: {'lr': 0.006579790250282824, 'num_epochs': 7}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:06:34,836] Trial 7 finished with value: 0.0218092954435837 and parameters: {'lr': 0.0001881618013540362, 'num_epochs': 2}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:06:34,919] Trial 8 finished with value: 0.02312199062202591 and parameters: {'lr': 0.010725617641919491, 'num_epochs': 4}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:06:34,995] Trial 9 finished with value: 0.021798008237561294 and parameters: {'lr': 8.565286340697322e-05, 'num_epochs': 3}. Best is trial 1 with value: 0.02514042769921196.\n",
      "[I 2025-07-26 21:08:13,839] A new study created in memory with name: no-name-41fed40e-9a3d-4dde-a786-7089f88b756e\n",
      "[I 2025-07-26 21:08:13,909] Trial 0 finished with value: 0.023405626493165414 and parameters: {'lr': 0.00965742150204404, 'num_epochs': 5}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:13,955] Trial 1 finished with value: 0.023129351477614815 and parameters: {'lr': 0.029086401944255273, 'num_epochs': 1}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:14,035] Trial 2 finished with value: 0.02317789876109544 and parameters: {'lr': 0.0008579234067594418, 'num_epochs': 6}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:14,163] Trial 3 finished with value: 0.023007167022540263 and parameters: {'lr': 3.146121326793967e-05, 'num_epochs': 10}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:14,230] Trial 4 finished with value: 0.023126460730335766 and parameters: {'lr': 0.0012451916912823583, 'num_epochs': 2}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:14,312] Trial 5 finished with value: 0.023115670229537102 and parameters: {'lr': 1.12822423444237e-05, 'num_epochs': 4}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:14,370] Trial 6 finished with value: 0.023154542487243433 and parameters: {'lr': 2.5650293841599168e-05, 'num_epochs': 1}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:14,451] Trial 7 finished with value: 0.023001290064762397 and parameters: {'lr': 9.020060944397277e-05, 'num_epochs': 5}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:14,554] Trial 8 finished with value: 0.02313335639697254 and parameters: {'lr': 0.009766884588127434, 'num_epochs': 8}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:08:14,608] Trial 9 finished with value: 0.02304384146171119 and parameters: {'lr': 2.018028673355708e-05, 'num_epochs': 1}. Best is trial 0 with value: 0.023405626493165414.\n",
      "[I 2025-07-26 21:09:54,713] A new study created in memory with name: no-name-f43e454f-e88f-4edc-99ef-5cc21188c766\n",
      "[I 2025-07-26 21:09:54,779] Trial 0 finished with value: 0.012362876832980343 and parameters: {'lr': 4.3468154734174913e-05, 'num_epochs': 4}. Best is trial 0 with value: 0.012362876832980343.\n",
      "[I 2025-07-26 21:09:54,885] Trial 1 finished with value: 0.012594831799422643 and parameters: {'lr': 0.0016578002989933078, 'num_epochs': 7}. Best is trial 1 with value: 0.012594831799422643.\n",
      "[I 2025-07-26 21:09:54,955] Trial 2 finished with value: 0.012878013277997168 and parameters: {'lr': 0.00011428718559927026, 'num_epochs': 3}. Best is trial 2 with value: 0.012878013277997168.\n",
      "[I 2025-07-26 21:09:55,048] Trial 3 finished with value: 0.013611688837039095 and parameters: {'lr': 0.008202155459090464, 'num_epochs': 7}. Best is trial 3 with value: 0.013611688837039095.\n",
      "[I 2025-07-26 21:09:55,136] Trial 4 finished with value: 0.012667037454983435 and parameters: {'lr': 0.0002793364477199508, 'num_epochs': 6}. Best is trial 3 with value: 0.013611688837039095.\n",
      "[I 2025-07-26 21:09:55,196] Trial 5 finished with value: 0.012713071694023352 and parameters: {'lr': 0.0008262956722547905, 'num_epochs': 2}. Best is trial 3 with value: 0.013611688837039095.\n",
      "[I 2025-07-26 21:09:55,264] Trial 6 finished with value: 0.012463815122664028 and parameters: {'lr': 0.012197241431518227, 'num_epochs': 3}. Best is trial 3 with value: 0.013611688837039095.\n",
      "[I 2025-07-26 21:09:55,378] Trial 7 finished with value: 0.012563437382703557 and parameters: {'lr': 0.006638605530576781, 'num_epochs': 10}. Best is trial 3 with value: 0.013611688837039095.\n",
      "[I 2025-07-26 21:09:55,453] Trial 8 finished with value: 0.012765057927535114 and parameters: {'lr': 0.00024512496740239744, 'num_epochs': 4}. Best is trial 3 with value: 0.013611688837039095.\n",
      "[I 2025-07-26 21:09:55,540] Trial 9 finished with value: 0.013005196300589193 and parameters: {'lr': 0.0379435284107688, 'num_epochs': 6}. Best is trial 3 with value: 0.013611688837039095.\n",
      "[I 2025-07-26 21:11:34,118] A new study created in memory with name: no-name-d842b9c6-d67f-45ae-a027-d964759fb54e\n",
      "[I 2025-07-26 21:11:34,231] Trial 0 finished with value: 0.011604083042803612 and parameters: {'lr': 0.00014263022459735334, 'num_epochs': 4}. Best is trial 0 with value: 0.011604083042803612.\n",
      "[I 2025-07-26 21:11:34,380] Trial 1 finished with value: 0.01161497324913608 and parameters: {'lr': 0.026195878609139904, 'num_epochs': 6}. Best is trial 1 with value: 0.01161497324913608.\n",
      "[I 2025-07-26 21:11:34,591] Trial 2 finished with value: 0.011359283416727716 and parameters: {'lr': 0.014411834649143333, 'num_epochs': 10}. Best is trial 1 with value: 0.01161497324913608.\n",
      "[I 2025-07-26 21:11:34,773] Trial 3 finished with value: 0.011651852098095127 and parameters: {'lr': 2.628525324549817e-05, 'num_epochs': 8}. Best is trial 3 with value: 0.011651852098095127.\n",
      "[I 2025-07-26 21:11:34,962] Trial 4 finished with value: 0.01176303458356952 and parameters: {'lr': 0.003975036884076132, 'num_epochs': 9}. Best is trial 4 with value: 0.01176303458356952.\n",
      "[I 2025-07-26 21:11:35,089] Trial 5 finished with value: 0.011762178849473443 and parameters: {'lr': 6.124214529363607e-05, 'num_epochs': 4}. Best is trial 4 with value: 0.01176303458356952.\n",
      "[I 2025-07-26 21:11:35,189] Trial 6 finished with value: 0.011892764303142084 and parameters: {'lr': 0.01145998993970084, 'num_epochs': 2}. Best is trial 6 with value: 0.011892764303142084.\n",
      "[I 2025-07-26 21:11:35,303] Trial 7 finished with value: 0.011815241786761686 and parameters: {'lr': 0.0002980019064808533, 'num_epochs': 3}. Best is trial 6 with value: 0.011892764303142084.\n",
      "[I 2025-07-26 21:11:35,432] Trial 8 finished with value: 0.011270087777604175 and parameters: {'lr': 0.024148072452113516, 'num_epochs': 4}. Best is trial 6 with value: 0.011892764303142084.\n",
      "[I 2025-07-26 21:11:35,575] Trial 9 finished with value: 0.011835389937582357 and parameters: {'lr': 0.000244266323575904, 'num_epochs': 4}. Best is trial 6 with value: 0.011892764303142084.\n",
      "[I 2025-07-26 21:13:14,176] A new study created in memory with name: no-name-bb9fcede-3168-4a6a-8501-416cea88e3dd\n",
      "[I 2025-07-26 21:13:14,268] Trial 0 finished with value: -0.010487750877589127 and parameters: {'lr': 1.4957443321708883e-05, 'num_epochs': 2}. Best is trial 0 with value: -0.010487750877589127.\n",
      "[I 2025-07-26 21:13:14,406] Trial 1 finished with value: -0.008840094195400303 and parameters: {'lr': 0.059832913302007545, 'num_epochs': 4}. Best is trial 1 with value: -0.008840094195400303.\n",
      "[I 2025-07-26 21:13:14,632] Trial 2 finished with value: -0.005067903625341876 and parameters: {'lr': 0.08372415179821904, 'num_epochs': 9}. Best is trial 2 with value: -0.005067903625341876.\n",
      "[I 2025-07-26 21:13:14,721] Trial 3 finished with value: -0.010402622726574542 and parameters: {'lr': 0.014956891559450932, 'num_epochs': 1}. Best is trial 2 with value: -0.005067903625341876.\n",
      "[I 2025-07-26 21:13:14,844] Trial 4 finished with value: -0.010668343353149342 and parameters: {'lr': 3.928043593794307e-05, 'num_epochs': 4}. Best is trial 2 with value: -0.005067903625341876.\n",
      "[I 2025-07-26 21:13:14,932] Trial 5 finished with value: -0.010368010287514919 and parameters: {'lr': 0.018536905708503377, 'num_epochs': 1}. Best is trial 2 with value: -0.005067903625341876.\n",
      "[I 2025-07-26 21:13:15,030] Trial 6 finished with value: -0.01022737028826434 and parameters: {'lr': 6.281245221954985e-05, 'num_epochs': 2}. Best is trial 2 with value: -0.005067903625341876.\n",
      "[I 2025-07-26 21:13:15,115] Trial 7 finished with value: -0.010388591055289496 and parameters: {'lr': 0.00951428863152773, 'num_epochs': 1}. Best is trial 2 with value: -0.005067903625341876.\n",
      "[I 2025-07-26 21:13:15,261] Trial 8 finished with value: -0.0104823125432546 and parameters: {'lr': 1.156575866175273e-05, 'num_epochs': 6}. Best is trial 2 with value: -0.005067903625341876.\n",
      "[I 2025-07-26 21:13:15,364] Trial 9 finished with value: -0.010481922820046969 and parameters: {'lr': 8.540878099378523e-05, 'num_epochs': 2}. Best is trial 2 with value: -0.005067903625341876.\n",
      "[I 2025-07-26 21:14:54,966] A new study created in memory with name: no-name-f209d453-277b-4185-958a-204a1504173d\n",
      "[I 2025-07-26 21:14:55,057] Trial 0 finished with value: -0.006288329280500393 and parameters: {'lr': 0.00048273581834581774, 'num_epochs': 2}. Best is trial 0 with value: -0.006288329280500393.\n",
      "[I 2025-07-26 21:14:55,152] Trial 1 finished with value: -0.006269795336744127 and parameters: {'lr': 0.0008936373072640611, 'num_epochs': 2}. Best is trial 1 with value: -0.006269795336744127.\n",
      "[I 2025-07-26 21:14:55,286] Trial 2 finished with value: -0.006261579666766036 and parameters: {'lr': 0.0006164382006925126, 'num_epochs': 5}. Best is trial 2 with value: -0.006261579666766036.\n",
      "[I 2025-07-26 21:14:55,400] Trial 3 finished with value: -0.006799872453251147 and parameters: {'lr': 0.027088947626478243, 'num_epochs': 3}. Best is trial 2 with value: -0.006261579666766036.\n",
      "[I 2025-07-26 21:14:55,536] Trial 4 finished with value: -0.006458270682380152 and parameters: {'lr': 0.005687788868600438, 'num_epochs': 5}. Best is trial 2 with value: -0.006261579666766036.\n",
      "[I 2025-07-26 21:14:55,743] Trial 5 finished with value: -0.006202071249980754 and parameters: {'lr': 0.0005707794781360477, 'num_epochs': 10}. Best is trial 5 with value: -0.006202071249980754.\n",
      "[I 2025-07-26 21:14:55,928] Trial 6 finished with value: -0.0063064519183215146 and parameters: {'lr': 0.00011249059081977943, 'num_epochs': 8}. Best is trial 5 with value: -0.006202071249980754.\n",
      "[I 2025-07-26 21:14:56,044] Trial 7 finished with value: -0.0063987633312500986 and parameters: {'lr': 0.000598573493127934, 'num_epochs': 3}. Best is trial 5 with value: -0.006202071249980754.\n",
      "[I 2025-07-26 21:14:56,143] Trial 8 finished with value: -0.006349856334061551 and parameters: {'lr': 0.0016535272954551839, 'num_epochs': 2}. Best is trial 5 with value: -0.006202071249980754.\n",
      "[I 2025-07-26 21:14:56,317] Trial 9 finished with value: -0.006371040097656576 and parameters: {'lr': 0.0007760124518639838, 'num_epochs': 8}. Best is trial 5 with value: -0.006202071249980754.\n",
      "[I 2025-07-26 21:16:35,488] A new study created in memory with name: no-name-c4ccb096-7390-4550-8273-073ddabada49\n",
      "[I 2025-07-26 21:16:35,675] Trial 0 finished with value: 0.007436454578493023 and parameters: {'lr': 0.0005469900215036322, 'num_epochs': 5}. Best is trial 0 with value: 0.007436454578493023.\n",
      "[I 2025-07-26 21:16:35,986] Trial 1 finished with value: 0.00756074514838765 and parameters: {'lr': 0.0011748779256440611, 'num_epochs': 9}. Best is trial 1 with value: 0.00756074514838765.\n",
      "[I 2025-07-26 21:16:36,255] Trial 2 finished with value: 0.006560801506958272 and parameters: {'lr': 0.010013689594311419, 'num_epochs': 10}. Best is trial 1 with value: 0.00756074514838765.\n",
      "[I 2025-07-26 21:16:36,470] Trial 3 finished with value: 0.007426893014140505 and parameters: {'lr': 0.0011827439719820938, 'num_epochs': 7}. Best is trial 1 with value: 0.00756074514838765.\n",
      "[I 2025-07-26 21:16:36,633] Trial 4 finished with value: 0.007428321962550919 and parameters: {'lr': 0.0001623068331204304, 'num_epochs': 4}. Best is trial 1 with value: 0.00756074514838765.\n",
      "[I 2025-07-26 21:16:36,744] Trial 5 finished with value: 0.008154960899328333 and parameters: {'lr': 0.050318904505876776, 'num_epochs': 1}. Best is trial 5 with value: 0.008154960899328333.\n",
      "[I 2025-07-26 21:16:37,011] Trial 6 finished with value: 0.007455018874800993 and parameters: {'lr': 1.3517730821072127e-05, 'num_epochs': 10}. Best is trial 5 with value: 0.008154960899328333.\n",
      "[I 2025-07-26 21:16:37,143] Trial 7 finished with value: 0.007537267796089174 and parameters: {'lr': 4.1567450158307274e-05, 'num_epochs': 2}. Best is trial 5 with value: 0.008154960899328333.\n",
      "[I 2025-07-26 21:16:37,303] Trial 8 finished with value: 0.007332187744575128 and parameters: {'lr': 0.00018061522234586488, 'num_epochs': 4}. Best is trial 5 with value: 0.008154960899328333.\n",
      "[I 2025-07-26 21:16:37,466] Trial 9 finished with value: 0.008041918775976032 and parameters: {'lr': 0.0022941192448985223, 'num_epochs': 4}. Best is trial 5 with value: 0.008154960899328333.\n",
      "[I 2025-07-26 21:18:16,678] A new study created in memory with name: no-name-d689920f-8cd4-4bdc-87f9-73a7060e61ea\n",
      "[I 2025-07-26 21:18:16,809] Trial 0 finished with value: 0.004699251129968248 and parameters: {'lr': 0.03703963224005444, 'num_epochs': 3}. Best is trial 0 with value: 0.004699251129968248.\n",
      "[I 2025-07-26 21:18:16,918] Trial 1 finished with value: 0.006147980840261913 and parameters: {'lr': 0.00017386609097437794, 'num_epochs': 1}. Best is trial 1 with value: 0.006147980840261913.\n",
      "[I 2025-07-26 21:18:17,056] Trial 2 finished with value: 0.00645874649037376 and parameters: {'lr': 0.009731376678817922, 'num_epochs': 3}. Best is trial 2 with value: 0.00645874649037376.\n",
      "[I 2025-07-26 21:18:17,308] Trial 3 finished with value: 0.006276542199198709 and parameters: {'lr': 0.00028360426814958686, 'num_epochs': 9}. Best is trial 2 with value: 0.00645874649037376.\n",
      "[I 2025-07-26 21:18:17,489] Trial 4 finished with value: 0.006415445195790589 and parameters: {'lr': 0.0012223288109580778, 'num_epochs': 5}. Best is trial 2 with value: 0.00645874649037376.\n",
      "[I 2025-07-26 21:18:17,600] Trial 5 finished with value: 0.006219064116856797 and parameters: {'lr': 0.00012528254101989297, 'num_epochs': 1}. Best is trial 2 with value: 0.00645874649037376.\n",
      "[I 2025-07-26 21:18:17,862] Trial 6 finished with value: 0.006188329696217621 and parameters: {'lr': 0.00120176330175045, 'num_epochs': 10}. Best is trial 2 with value: 0.00645874649037376.\n",
      "[I 2025-07-26 21:18:18,087] Trial 7 finished with value: 0.006598811643331398 and parameters: {'lr': 0.0037827584009052025, 'num_epochs': 7}. Best is trial 7 with value: 0.006598811643331398.\n",
      "[I 2025-07-26 21:18:18,306] Trial 8 finished with value: 0.00645908275266009 and parameters: {'lr': 0.01553919050140814, 'num_epochs': 7}. Best is trial 7 with value: 0.006598811643331398.\n",
      "[I 2025-07-26 21:18:18,562] Trial 9 finished with value: 0.0063234179700785344 and parameters: {'lr': 0.00029769177425724344, 'num_epochs': 9}. Best is trial 7 with value: 0.006598811643331398.\n",
      "[I 2025-07-26 21:19:57,602] A new study created in memory with name: no-name-f023a398-6f95-4504-8265-7f1a9b034269\n",
      "[I 2025-07-26 21:19:57,728] Trial 0 finished with value: 0.010795285751252428 and parameters: {'lr': 0.0002926021251768922, 'num_epochs': 2}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:58,007] Trial 1 finished with value: 0.01065796143633691 and parameters: {'lr': 0.0012605689576488128, 'num_epochs': 10}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:58,188] Trial 2 finished with value: 0.001138286317037298 and parameters: {'lr': 0.0606311254878531, 'num_epochs': 5}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:58,318] Trial 3 finished with value: 0.010496029194054277 and parameters: {'lr': 0.0005634738441928728, 'num_epochs': 2}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:58,582] Trial 4 finished with value: 0.010599197072936682 and parameters: {'lr': 0.0004033941987269452, 'num_epochs': 10}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:58,832] Trial 5 finished with value: 0.010673578117554566 and parameters: {'lr': 0.0003463183025154397, 'num_epochs': 9}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:59,012] Trial 6 finished with value: 0.010324773701148494 and parameters: {'lr': 0.00017915732891402055, 'num_epochs': 5}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:59,160] Trial 7 finished with value: 0.010381491279346256 and parameters: {'lr': 0.00015036682031775928, 'num_epochs': 3}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:59,376] Trial 8 finished with value: 0.010770815031393623 and parameters: {'lr': 0.006350134095351949, 'num_epochs': 7}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:19:59,491] Trial 9 finished with value: 0.010260638373862917 and parameters: {'lr': 5.9223152848564594e-05, 'num_epochs': 1}. Best is trial 0 with value: 0.010795285751252428.\n",
      "[I 2025-07-26 21:21:39,262] A new study created in memory with name: no-name-e9c55f94-c9b3-4276-9243-1f36acc84e14\n",
      "[I 2025-07-26 21:21:39,496] Trial 0 finished with value: 0.01841960999826359 and parameters: {'lr': 0.0038813812578314885, 'num_epochs': 8}. Best is trial 0 with value: 0.01841960999826359.\n",
      "[I 2025-07-26 21:21:39,614] Trial 1 finished with value: 0.018241963599835133 and parameters: {'lr': 0.004057540655101638, 'num_epochs': 1}. Best is trial 0 with value: 0.01841960999826359.\n",
      "[I 2025-07-26 21:21:39,787] Trial 2 finished with value: 0.015954580533163124 and parameters: {'lr': 0.032845337603466904, 'num_epochs': 4}. Best is trial 0 with value: 0.01841960999826359.\n",
      "[I 2025-07-26 21:21:39,967] Trial 3 finished with value: 0.018427562824679507 and parameters: {'lr': 1.9984188363207072e-05, 'num_epochs': 4}. Best is trial 3 with value: 0.018427562824679507.\n",
      "[I 2025-07-26 21:21:40,213] Trial 4 finished with value: 0.012143343027588047 and parameters: {'lr': 0.030598758988979, 'num_epochs': 7}. Best is trial 3 with value: 0.018427562824679507.\n",
      "[I 2025-07-26 21:21:40,471] Trial 5 finished with value: 0.01819612320863849 and parameters: {'lr': 0.004942132636127574, 'num_epochs': 7}. Best is trial 3 with value: 0.018427562824679507.\n",
      "[I 2025-07-26 21:21:40,751] Trial 6 finished with value: 0.01825966685675602 and parameters: {'lr': 6.405284785399226e-05, 'num_epochs': 8}. Best is trial 3 with value: 0.018427562824679507.\n",
      "[I 2025-07-26 21:21:40,989] Trial 7 finished with value: 0.01832668992161763 and parameters: {'lr': 0.003373432861027207, 'num_epochs': 6}. Best is trial 3 with value: 0.018427562824679507.\n",
      "[I 2025-07-26 21:21:41,150] Trial 8 finished with value: 0.018251625683369396 and parameters: {'lr': 0.00012718080633359038, 'num_epochs': 3}. Best is trial 3 with value: 0.018427562824679507.\n",
      "[I 2025-07-26 21:21:41,329] Trial 9 finished with value: 0.018166736272242148 and parameters: {'lr': 2.5914797854811166e-05, 'num_epochs': 4}. Best is trial 3 with value: 0.018427562824679507.\n",
      "[I 2025-07-26 21:23:21,211] A new study created in memory with name: no-name-91f8e818-fa39-4003-a605-0cf5d8c30203\n",
      "[I 2025-07-26 21:23:21,477] Trial 0 finished with value: 0.0038663744256419684 and parameters: {'lr': 0.0006328642221388627, 'num_epochs': 9}. Best is trial 0 with value: 0.0038663744256419684.\n",
      "[I 2025-07-26 21:23:21,761] Trial 1 finished with value: 0.003979575138712272 and parameters: {'lr': 9.019022510317006e-05, 'num_epochs': 9}. Best is trial 1 with value: 0.003979575138712272.\n",
      "[I 2025-07-26 21:23:21,883] Trial 2 finished with value: 0.0038471814507716542 and parameters: {'lr': 0.010059245522489496, 'num_epochs': 1}. Best is trial 1 with value: 0.003979575138712272.\n",
      "[I 2025-07-26 21:23:22,000] Trial 3 finished with value: 0.004099073254190312 and parameters: {'lr': 0.005065051557118606, 'num_epochs': 1}. Best is trial 3 with value: 0.004099073254190312.\n",
      "[I 2025-07-26 21:23:22,193] Trial 4 finished with value: 0.003886379942801667 and parameters: {'lr': 4.912952867916118e-05, 'num_epochs': 5}. Best is trial 3 with value: 0.004099073254190312.\n",
      "[I 2025-07-26 21:23:22,334] Trial 5 finished with value: 0.004071585845384218 and parameters: {'lr': 0.0009133284455138626, 'num_epochs': 2}. Best is trial 3 with value: 0.004099073254190312.\n",
      "[I 2025-07-26 21:23:22,470] Trial 6 finished with value: 0.003889311979516136 and parameters: {'lr': 0.0005964786871459002, 'num_epochs': 2}. Best is trial 3 with value: 0.004099073254190312.\n",
      "[I 2025-07-26 21:23:22,690] Trial 7 finished with value: 0.003829763026847852 and parameters: {'lr': 0.000375538468695594, 'num_epochs': 6}. Best is trial 3 with value: 0.004099073254190312.\n",
      "[I 2025-07-26 21:23:23,061] Trial 8 finished with value: 0.0038896543261240915 and parameters: {'lr': 0.004647408106320201, 'num_epochs': 10}. Best is trial 3 with value: 0.004099073254190312.\n",
      "[I 2025-07-26 21:23:23,220] Trial 9 finished with value: 0.0039088938258063 and parameters: {'lr': 2.103976797577842e-05, 'num_epochs': 2}. Best is trial 3 with value: 0.004099073254190312.\n",
      "[I 2025-07-26 21:25:02,851] A new study created in memory with name: no-name-d90b0d97-26d5-4327-bbd0-553fd99c7ac8\n",
      "[I 2025-07-26 21:25:03,082] Trial 0 finished with value: 0.005864626861815204 and parameters: {'lr': 0.002194060098224045, 'num_epochs': 7}. Best is trial 0 with value: 0.005864626861815204.\n",
      "[I 2025-07-26 21:25:03,329] Trial 1 finished with value: 0.005352612728412294 and parameters: {'lr': 0.0065491026764392096, 'num_epochs': 7}. Best is trial 0 with value: 0.005864626861815204.\n",
      "[I 2025-07-26 21:25:03,492] Trial 2 finished with value: 0.005860258215565791 and parameters: {'lr': 1.373331909887915e-05, 'num_epochs': 3}. Best is trial 0 with value: 0.005864626861815204.\n",
      "[I 2025-07-26 21:25:03,738] Trial 3 finished with value: 0.005767999434837715 and parameters: {'lr': 0.00041867334699912734, 'num_epochs': 7}. Best is trial 0 with value: 0.005864626861815204.\n",
      "[I 2025-07-26 21:25:03,867] Trial 4 finished with value: 0.005790145983951457 and parameters: {'lr': 0.0053504263479821935, 'num_epochs': 1}. Best is trial 0 with value: 0.005864626861815204.\n",
      "[I 2025-07-26 21:25:04,064] Trial 5 finished with value: 0.005951991707244514 and parameters: {'lr': 0.0006933397822100328, 'num_epochs': 5}. Best is trial 5 with value: 0.005951991707244514.\n",
      "[I 2025-07-26 21:25:04,290] Trial 6 finished with value: 0.005797613995473878 and parameters: {'lr': 0.00152800477841359, 'num_epochs': 6}. Best is trial 5 with value: 0.005951991707244514.\n",
      "[I 2025-07-26 21:25:04,455] Trial 7 finished with value: 0.005740302307237566 and parameters: {'lr': 0.006209365955120991, 'num_epochs': 3}. Best is trial 5 with value: 0.005951991707244514.\n",
      "[I 2025-07-26 21:25:04,616] Trial 8 finished with value: 0.0057763144937847755 and parameters: {'lr': 0.0017829438894520356, 'num_epochs': 3}. Best is trial 5 with value: 0.005951991707244514.\n",
      "[I 2025-07-26 21:25:04,758] Trial 9 finished with value: 0.005911747985136902 and parameters: {'lr': 0.0004841791367198637, 'num_epochs': 2}. Best is trial 5 with value: 0.005951991707244514.\n"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3528</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.2739</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0.4127</td>\n",
       "      <td>0.1669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.6133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.2202</td>\n",
       "      <td>0.0593</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>0.0589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.2062</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.3530</td>\n",
       "      <td>0.0071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0            0.0137 0.0238  0.0263   0.0310   0.0278     0.0240   \n",
       "300          0.0138 0.0147  0.0220   0.0252   0.0211     0.0160   \n",
       "600          0.0136 0.0095  0.0127   0.0135   0.0129     0.0094   \n",
       "800          0.0141 0.0182  0.0153   0.0165   0.0173     0.0183   \n",
       "900          0.0140 0.0142  0.0115   0.0118   0.0126     0.0135   \n",
       "\n",
       "     action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                 0.2073        0.0000                0.3528         0.0000  \n",
       "300               0.2739        0.1660                0.4127         0.1669  \n",
       "600               0.6788        0.5586                0.8382         0.6133  \n",
       "800               0.2202        0.0593                0.3542         0.0589  \n",
       "900               0.2062        0.0070                0.3530         0.0071  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3528</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2739</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0.4127</td>\n",
       "      <td>0.1669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5586</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.6133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2202</td>\n",
       "      <td>0.0593</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>0.0589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2062</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.3530</td>\n",
       "      <td>0.0071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0            0.0137 0.0238  0.0263   0.0310   0.0278     0.0240   0.0000   \n",
       "300          0.0138 0.0147  0.0220   0.0252   0.0211     0.0160   0.0000   \n",
       "600          0.0136 0.0095  0.0127   0.0135   0.0129     0.0094   0.0000   \n",
       "800          0.0141 0.0182  0.0153   0.0165   0.0173     0.0183   0.0000   \n",
       "900          0.0140 0.0142  0.0115   0.0118   0.0126     0.0135   0.0000   \n",
       "\n",
       "     reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0        0.0000       0.0000       0.0000         0.0000               0.2073   \n",
       "300      0.0000       0.0000       0.0000         0.0000               0.2739   \n",
       "600      0.0000       0.0000       0.0000         0.0000               0.6788   \n",
       "800      0.0000       0.0000       0.0000         0.0000               0.2202   \n",
       "900      0.0000       0.0000       0.0000         0.0000               0.2062   \n",
       "\n",
       "     action_delta  context_diff_to_real  context_delta  \n",
       "0          0.0000                0.3528         0.0000  \n",
       "300        0.1660                0.4127         0.1669  \n",
       "600        0.5586                0.8382         0.6133  \n",
       "800        0.0593                0.3542         0.0589  \n",
       "900        0.0070                0.3530         0.0071  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`pscore` must be 1D array, but got 0D array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df5 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size, val_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m val_data \u001b[38;5;241m=\u001b[39m get_train_data(n_actions, val_size, simulation_data, np\u001b[38;5;241m.\u001b[39marange(val_size) \u001b[38;5;241m+\u001b[39m train_size, our_x)\n\u001b[1;32m     72\u001b[0m regression_model \u001b[38;5;241m=\u001b[39m RegressionModel(\n\u001b[1;32m     73\u001b[0m                         n_actions\u001b[38;5;241m=\u001b[39mn_actions,\n\u001b[1;32m     74\u001b[0m                         action_context\u001b[38;5;241m=\u001b[39mour_x,\n\u001b[1;32m     75\u001b[0m                         base_model\u001b[38;5;241m=\u001b[39mLogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m)\n\u001b[1;32m     76\u001b[0m                         )\n\u001b[0;32m---> 78\u001b[0m \u001b[43mregression_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moriginal_policy_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m neighberhoodmodel \u001b[38;5;241m=\u001b[39m NeighborhoodModel(\n\u001b[1;32m     87\u001b[0m                                         train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_idx\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     88\u001b[0m                                         train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m                                         num_neighbors\u001b[38;5;241m=\u001b[39mnum_neighbors\n\u001b[1;32m     93\u001b[0m                                     )\n\u001b[1;32m     96\u001b[0m model \u001b[38;5;241m=\u001b[39m CFModel(\n\u001b[1;32m     97\u001b[0m                 n_users, \n\u001b[1;32m     98\u001b[0m                 n_actions, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 initial_actions_embeddings\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(our_a, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    102\u001b[0m                 )\n",
      "File \u001b[0;32m/code/models.py:324\u001b[0m, in \u001b[0;36mRegressionModel.fit\u001b[0;34m(self, context, action, reward, pscore, position, action_dist)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    290\u001b[0m     context: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m     action_dist: Optional[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the regression model on given logged bandit data.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m \n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m     \u001b[43mcheck_bandit_feedback_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     n \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen_list \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/code/saito_helpers.py:506\u001b[0m, in \u001b[0;36mcheck_bandit_feedback_inputs\u001b[0;34m(context, action, reward, expected_reward, position, pscore, action_context)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`action` elements must be non-negative integers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pscore \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    508\u001b[0m         context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m action\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m reward\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m pscore\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    509\u001b[0m     ):\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, but found it False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         )\n",
      "File \u001b[0;32m/code/saito_helpers.py:410\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, name, expected_dim)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(array)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m expected_dim:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `pscore` must be 1D array, but got 0D array"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
