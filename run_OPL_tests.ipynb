{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# import gym\n",
    "# import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")  # TF32 = big speedup on Ada\n",
    "\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "# from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "    num_runs,\n",
    "    num_neighbors,\n",
    "    num_rounds_list,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    val_size=2000,\n",
    "    n_trials=10,    \n",
    "    prev_best_params=None\n",
    "):\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # ---- Device & CUDA fast-paths ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_float32_matmul_precision(\"high\")  # TF32 on Ada\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "\n",
    "    ### NEW: indices once\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    def T(x):\n",
    "        return torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Add a dictionary to store the best hyperparameters for each training size\n",
    "    best_hyperparams_by_size = {}\n",
    "    best_reward = -float('inf')\n",
    "    overall_best_params = {}\n",
    "\n",
    "    # ---- Optuna objective uses train/val loaders from outer scope ----\n",
    "    def objective(trial):\n",
    "        # Current parameters\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "        trial_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "        trial_num_neighbors = trial.suggest_int(\"num_neighbors\", 3, 15)\n",
    "        lr_decay = trial.suggest_float(\"lr_decay\", 0.8, 1.0)\n",
    "\n",
    "        # Create neighborhood model and scores:\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "            train_data['x_idx'], train_data['a'],\n",
    "            our_a, our_x, train_data['r'],\n",
    "            num_neighbors=trial_num_neighbors\n",
    "        )\n",
    "\n",
    "        ### NEW: build trial_scores_all ONCE and keep on device\n",
    "        trial_scores_all = torch.as_tensor(\n",
    "            trial_neigh_model.predict(all_user_indices),   # shape [n_users, n_actions]\n",
    "            device=device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        trial_model = CFModel(\n",
    "            n_users, n_actions, emb_dim,\n",
    "            initial_user_embeddings=T(our_x),\n",
    "            initial_actions_embeddings=T(our_a)\n",
    "        ).to(device)\n",
    "\n",
    "        assert (not torch.cuda.is_available()) or next(trial_model.parameters()).is_cuda\n",
    "\n",
    "        # Create a DataLoader with trial batch size\n",
    "        final_train_loader = DataLoader(\n",
    "            cf_dataset, batch_size=trial_batch_size, shuffle=True,  # Use the trial's suggested batch size\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Modified train function call with learning rate decay\n",
    "        current_lr = lr\n",
    "        for epoch in range(epochs):\n",
    "            # Apply learning rate decay for each epoch after the first\n",
    "            if epoch > 0:\n",
    "                current_lr *= lr_decay\n",
    "                \n",
    "            train(\n",
    "                trial_model, final_train_loader, trial_neigh_model, trial_scores_all,\n",
    "                criterion=SNDRPolicyLoss(), num_epochs=1, lr=current_lr, device=str(device)\n",
    "            )\n",
    "        \n",
    "        #######################\n",
    "        #######################\n",
    "        # Switching to optimize on analytical rewards instead. \n",
    "        #######################\n",
    "        #######################\n",
    "        \n",
    "        # val =  validation_loop(trial_model, val_loader, trial_neigh_model, trial_scores_all, device=device)\n",
    "        # if not torch.isfinite(torch.tensor(val)):\n",
    "        #    raise optuna.TrialPruned()  # or return a sentinel very-low score\n",
    "        # return val\n",
    "\n",
    "        # Extract embeddings from the model\n",
    "        trial_x_t, trial_a_t = trial_model.get_params()\n",
    "        trial_x = trial_x_t.detach().cpu().numpy()\n",
    "        trial_a = trial_a_t.detach().cpu().numpy()\n",
    "\n",
    "        # Calculate policy and its reward\n",
    "        trial_policy = np.expand_dims(softmax(trial_x @ trial_a.T, axis=1), -1)\n",
    "        trial_policy_reward = calc_reward(dataset, trial_policy)\n",
    "    \n",
    "        # Return the policy reward (Optuna maximizes this value)\n",
    "        return trial_policy_reward\n",
    "\n",
    "    first = True\n",
    "\n",
    "    # Store previous best hyperparams to use when starting a new training size\n",
    "    last_best_params = None\n",
    "    if prev_best_params is not None:\n",
    "        last_best_params = prev_best_params\n",
    "\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                dataset, pi_0, train_size + val_size,\n",
    "                random_state=(run + 1) * train_size\n",
    "            )\n",
    "\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data   = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            regression_model = RegressionModel(\n",
    "                n_actions=n_actions, action_context=our_x,\n",
    "                base_model=LogisticRegression(random_state=12345)\n",
    "            )\n",
    "            regression_model.fit(\n",
    "                train_data['x'], train_data['a'], train_data['r'],\n",
    "                original_policy_prob[train_data['x_idx'], train_data['a']].squeeze()\n",
    "            )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=num_neighbors\n",
    "            )\n",
    "\n",
    "            ### NEW: build scores_all ONCE per NeighborhoodModel and keep it on device\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),   # [n_users, n_actions]\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "\n",
    "            # DataLoaders: feed the GPU\n",
    "            num_workers = 4 if torch.cuda.is_available() else 0\n",
    "            cf_dataset = CustomCFDataset(\n",
    "                train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "            )\n",
    "            train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            val_dataset = CustomCFDataset(\n",
    "                val_data['x_idx'], val_data['a'], val_data['r'], original_policy_prob\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset, batch_size=val_size, shuffle=False,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "\n",
    "            # ---- Hyperparam search ----\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "            # If we have previous best parameters, enqueue them first\n",
    "            if last_best_params is not None:\n",
    "                # Enqueue a trial with the previous best parameters\n",
    "                study.enqueue_trial(last_best_params)\n",
    "            study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            best_reward_for_size = study.best_value\n",
    "\n",
    "             # Update last_best_params for the next training size\n",
    "            last_best_params = best_params\n",
    "\n",
    "            # Store the best parameters for this training size\n",
    "            best_hyperparams_by_size[train_size] = {\n",
    "                \"params\": best_params,\n",
    "                \"reward\": best_reward_for_size\n",
    "            }\n",
    "\n",
    "            # Update overall best if this is better\n",
    "            if best_reward_for_size > best_reward:\n",
    "                best_reward = best_reward_for_size\n",
    "                overall_best_params = best_params.copy()\n",
    "                overall_best_params[\"train_size\"] = train_size\n",
    "\n",
    "            # ---- Retrain with best params and evaluate ----\n",
    "                # Use best hyperparameters from Optuna\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=best_params['num_neighbors']\n",
    "            )\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "            model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "            assert (not torch.cuda.is_available()) or next(model.parameters()).is_cuda\n",
    "\n",
    "            current_lr = best_params['lr']\n",
    "            for epoch in range(best_params['num_epochs']):\n",
    "                if epoch > 0:\n",
    "                    current_lr *= best_params['lr_decay']\n",
    "                train(\n",
    "                    model, train_loader, neighberhoodmodel, scores_all,\n",
    "                    criterion=SNDRPolicyLoss(),\n",
    "                    num_epochs=1, lr=current_lr,\n",
    "                    device=str(device)\n",
    "                )\n",
    "\n",
    "            # Evaluate using the same metric as Optuna\n",
    "            our_x_t, our_a_t = model.get_params()\n",
    "            our_a, our_x = our_a_t.detach().cpu().numpy(), our_x_t.detach().cpu().numpy()\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "            policy_reward = calc_reward(dataset, policy)\n",
    "            \n",
    "            eval_metrics = eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy)\n",
    "            action_diff_to_real = np.sqrt(np.mean((emb_a - our_a) ** 2))\n",
    "            action_delta = np.sqrt(np.mean((original_a - our_a) ** 2))\n",
    "            context_diff_to_real = np.sqrt(np.mean((emb_x - our_x) ** 2))\n",
    "            context_delta = np.sqrt(np.mean((original_x - our_x) ** 2))\n",
    "\n",
    "            row = np.concatenate([\n",
    "                np.atleast_1d(policy_reward),         # shape (1,)\n",
    "                np.atleast_1d(eval_metrics),          # shape (4,)\n",
    "                np.atleast_1d(action_diff_to_real),   # shape (1,)\n",
    "                np.atleast_1d(action_delta),          # shape (1,)\n",
    "                np.atleast_1d(context_diff_to_real),  # shape (1,)\n",
    "                np.atleast_1d(context_delta)          # shape (1,)\n",
    "            ])\n",
    "            conv_results.append(row)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "\n",
    "\n",
    "     # Return both the results DataFrame and the best hyperparameters\n",
    "    return pd.DataFrame.from_dict(results, orient='index'), best_hyperparams_by_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Item CTR: 0.12972795060603162\n",
      "Optimal greedy CTR: 0.19999707792821972\n",
      "Optimal Stochastic CTR: 0.19982996880994605\n",
      "Our Initial CTR: 0.1646085673501415\n"
     ]
    }
   ],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.2\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emb_a', 'our_a', 'original_a', 'emb_x', 'our_x', 'original_x', 'q_x_a', 'n_actions', 'n_users', 'emb_dim', 'user_prior'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 200\n",
    "num_neighbors = 6\n",
    "n_trials_for_optuna = 10\n",
    "num_rounds_list = [500, 1000] #[1000, 5000, 10000]\n",
    "\n",
    "# Manually define your best parameters\n",
    "best_params_to_use = {\n",
    "    \"lr\": 0.002,  # Learning rate\n",
    "    \"num_epochs\": 5,  # Number of training epochs\n",
    "    \"batch_size\": 256,  # Batch size for training\n",
    "    \"num_neighbors\": 8,  # Number of neighbors for neighborhood model\n",
    "    \"lr_decay\": 0.9  # Learning rate decay factor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of num_rounds_list: [100, 200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:23,283] A new study created in memory with name: no-name-66ae223e-00d9-4cc1-9d66-83abf314ef5f\n",
      "Best trial: 0. Best value: 0.164634:  20%|██        | 1/5 [00:01<00:05,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:24,728] Trial 0 finished with value: 0.16463418656448886 and parameters: {'lr': 0.002, 'num_epochs': 5, 'batch_size': 256, 'num_neighbors': 8, 'lr_decay': 0.9}. Best is trial 0 with value: 0.16463418656448886.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.164634:  40%|████      | 2/5 [00:02<00:03,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:25,452] Trial 1 finished with value: 0.16462522476151945 and parameters: {'lr': 0.0010579637402296635, 'num_epochs': 10, 'batch_size': 512, 'num_neighbors': 15, 'lr_decay': 0.8417683349171524}. Best is trial 0 with value: 0.16463418656448886.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.164634:  60%|██████    | 3/5 [00:02<00:01,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:26,113] Trial 2 finished with value: 0.1646254968256796 and parameters: {'lr': 0.0030713592141626105, 'num_epochs': 2, 'batch_size': 512, 'num_neighbors': 8, 'lr_decay': 0.8617102602345228}. Best is trial 0 with value: 0.16463418656448886.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.164658:  80%|████████  | 4/5 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:26,852] Trial 3 finished with value: 0.16465818848051156 and parameters: {'lr': 0.003450707521429363, 'num_epochs': 5, 'batch_size': 128, 'num_neighbors': 11, 'lr_decay': 0.9135035142045131}. Best is trial 3 with value: 0.16465818848051156.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.164658: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:27,616] Trial 4 finished with value: 0.16461207479014642 and parameters: {'lr': 0.00023529072140567985, 'num_epochs': 10, 'batch_size': 128, 'num_neighbors': 12, 'lr_decay': 0.8550301914939706}. Best is trial 3 with value: 0.16465818848051156.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:29,266] A new study created in memory with name: no-name-27248116-a272-476b-b256-5fbf7489aaca\n",
      "Best trial: 0. Best value: 0.164772:  20%|██        | 1/5 [00:00<00:02,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:29,938] Trial 0 finished with value: 0.16477230290600256 and parameters: {'lr': 0.003450707521429363, 'num_epochs': 5, 'batch_size': 128, 'num_neighbors': 11, 'lr_decay': 0.9135035142045131}. Best is trial 0 with value: 0.16477230290600256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.164772:  40%|████      | 2/5 [00:01<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:30,633] Trial 1 finished with value: 0.1646587837999768 and parameters: {'lr': 0.00018068112236885013, 'num_epochs': 2, 'batch_size': 256, 'num_neighbors': 10, 'lr_decay': 0.9544124122107603}. Best is trial 0 with value: 0.16477230290600256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.164796:  60%|██████    | 3/5 [00:02<00:01,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:31,313] Trial 2 finished with value: 0.1647963852926942 and parameters: {'lr': 0.005739931640961455, 'num_epochs': 3, 'batch_size': 128, 'num_neighbors': 10, 'lr_decay': 0.9361591152183173}. Best is trial 2 with value: 0.1647963852926942.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.164796:  80%|████████  | 4/5 [00:02<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:32,060] Trial 3 finished with value: 0.16467463671581084 and parameters: {'lr': 0.0008190814368376619, 'num_epochs': 10, 'batch_size': 512, 'num_neighbors': 12, 'lr_decay': 0.9912516109540179}. Best is trial 2 with value: 0.1647963852926942.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.164796: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:36:32,807] Trial 4 finished with value: 0.1647277009867415 and parameters: {'lr': 0.0010896212124819576, 'num_epochs': 10, 'batch_size': 128, 'num_neighbors': 7, 'lr_decay': 0.9461405852844451}. Best is trial 2 with value: 0.1647963852926942.\n",
      "\n",
      "=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\n",
      "\n",
      "Training Size: 100\n",
      "Best Reward: 0.164658\n",
      "Parameters:\n",
      "  lr: 0.003450707521429363\n",
      "  num_epochs: 5\n",
      "  batch_size: 128\n",
      "  num_neighbors: 11\n",
      "  lr_decay: 0.9135035142045131\n",
      "\n",
      "Training Size: 200\n",
      "Best Reward: 0.164796\n",
      "Parameters:\n",
      "  lr: 0.005739931640961455\n",
      "  num_epochs: 3\n",
      "  batch_size: 128\n",
      "  num_neighbors: 10\n",
      "  lr_decay: 0.9361591152183173\n",
      "===========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16460857</td>\n",
       "      <td>0.18881798</td>\n",
       "      <td>0.18429266</td>\n",
       "      <td>0.20380138</td>\n",
       "      <td>0.19798777</td>\n",
       "      <td>0.18097098</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.16465819</td>\n",
       "      <td>0.18285870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20126963</td>\n",
       "      <td>0.18990308</td>\n",
       "      <td>0.14726400</td>\n",
       "      <td>0.75679233</td>\n",
       "      <td>0.01449535</td>\n",
       "      <td>0.87628370</td>\n",
       "      <td>0.00589102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.16469507</td>\n",
       "      <td>0.16895652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17412083</td>\n",
       "      <td>0.17305334</td>\n",
       "      <td>0.16857739</td>\n",
       "      <td>0.75735020</td>\n",
       "      <td>0.02629954</td>\n",
       "      <td>0.87621012</td>\n",
       "      <td>0.01034721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0        0.16460857 0.18881798 0.18429266 0.20380138 0.19798777 0.18097098   \n",
       "100      0.16465819 0.18285870        NaN 0.20126963 0.18990308 0.14726400   \n",
       "200      0.16469507 0.16895652        NaN 0.17412083 0.17305334 0.16857739   \n",
       "\n",
       "     action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0             0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "100           0.75679233    0.01449535            0.87628370     0.00589102  \n",
       "200           0.75735020    0.02629954            0.87621012     0.01034721  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Value of num_rounds_list:\", num_rounds_list)\n",
    "\n",
    "# Run the optimization\n",
    "df4, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=35000, n_trials=n_trials_for_optuna,prev_best_params=best_params_to_use)\n",
    "\n",
    "# Print best hyperparameters for each training size\n",
    "print(\"\\n=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\")\n",
    "for train_size, params in best_hyperparams_by_size.items():\n",
    "    print(f\"\\nTraining Size: {train_size}\")\n",
    "    print(f\"Best Reward: {params['reward']:.6f}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, value in params['params'].items():\n",
    "        print(f\"  {param_name}: {value}\")\n",
    "print(\"===========================\\n\")\n",
    "\n",
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16460857</td>\n",
       "      <td>0.18881798</td>\n",
       "      <td>0.18429266</td>\n",
       "      <td>0.20380138</td>\n",
       "      <td>0.19798777</td>\n",
       "      <td>0.18097098</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.16465819</td>\n",
       "      <td>0.18285870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20126963</td>\n",
       "      <td>0.18990308</td>\n",
       "      <td>0.14726400</td>\n",
       "      <td>0.75679233</td>\n",
       "      <td>0.01449535</td>\n",
       "      <td>0.87628370</td>\n",
       "      <td>0.00589102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.16469507</td>\n",
       "      <td>0.16895652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17412083</td>\n",
       "      <td>0.17305334</td>\n",
       "      <td>0.16857739</td>\n",
       "      <td>0.75735020</td>\n",
       "      <td>0.02629954</td>\n",
       "      <td>0.87621012</td>\n",
       "      <td>0.01034721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0        0.16460857 0.18881798 0.18429266 0.20380138 0.19798777 0.18097098   \n",
       "100      0.16465819 0.18285870        NaN 0.20126963 0.18990308 0.14726400   \n",
       "200      0.16469507 0.16895652        NaN 0.17412083 0.17305334 0.16857739   \n",
       "\n",
       "     action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0             0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "100           0.75679233    0.01449535            0.87628370     0.00589102  \n",
       "200           0.75735020    0.02629954            0.87621012     0.01034721  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16460857</td>\n",
       "      <td>0.18881798</td>\n",
       "      <td>0.18429266</td>\n",
       "      <td>0.20380138</td>\n",
       "      <td>0.19798777</td>\n",
       "      <td>0.18097098</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.16465819</td>\n",
       "      <td>0.18285870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20126963</td>\n",
       "      <td>0.18990308</td>\n",
       "      <td>0.14726400</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.75679233</td>\n",
       "      <td>0.01449535</td>\n",
       "      <td>0.87628370</td>\n",
       "      <td>0.00589102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.16469507</td>\n",
       "      <td>0.16895652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17412083</td>\n",
       "      <td>0.17305334</td>\n",
       "      <td>0.16857739</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.75735020</td>\n",
       "      <td>0.02629954</td>\n",
       "      <td>0.87621012</td>\n",
       "      <td>0.01034721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0        0.16460857 0.18881798 0.18429266 0.20380138 0.19798777 0.18097098   \n",
       "100      0.16465819 0.18285870        NaN 0.20126963 0.18990308 0.14726400   \n",
       "200      0.16469507 0.16895652        NaN 0.17412083 0.17305334 0.16857739   \n",
       "\n",
       "       ipw_var  reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  \\\n",
       "0   0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "100 0.00000000         NaN   0.00000000   0.00000000     0.00000000   \n",
       "200 0.00000000         NaN   0.00000000   0.00000000     0.00000000   \n",
       "\n",
       "     action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0             0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "100           0.75679233    0.01449535            0.87628370     0.00589102  \n",
       "200           0.75735020    0.02629954            0.87621012     0.01034721  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OBPEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
