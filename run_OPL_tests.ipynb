{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# import gym\n",
    "# import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")  # TF32 = big speedup on Ada\n",
    "\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "# from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "    num_runs,\n",
    "    num_neighbors,\n",
    "    num_rounds_list,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    val_size=2000\n",
    "):\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # ---- Device & CUDA fast-paths ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_float32_matmul_precision(\"high\")  # TF32 on Ada\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "\n",
    "    ### NEW: indices once\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    def T(x):\n",
    "        return torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    # ---- Optuna objective uses train/val loaders from outer scope ----\n",
    "    def objective(trial):\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "            train_data['x_idx'], train_data['a'],\n",
    "            our_a, our_x, train_data['r'],\n",
    "            num_neighbors=num_neighbors\n",
    "        )\n",
    "\n",
    "        ### NEW: build trial_scores_all ONCE and keep on device\n",
    "        trial_scores_all = torch.as_tensor(\n",
    "            trial_neigh_model.predict(all_user_indices),   # shape [n_users, n_actions]\n",
    "            device=device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        trial_model = CFModel(\n",
    "            n_users, n_actions, emb_dim,\n",
    "            initial_user_embeddings=T(our_x),\n",
    "            initial_actions_embeddings=T(our_a)\n",
    "        ).to(device)\n",
    "\n",
    "        assert (not torch.cuda.is_available()) or next(trial_model.parameters()).is_cuda\n",
    "\n",
    "        ### CHANGED: pass trial_scores_all\n",
    "        train(\n",
    "            trial_model, train_loader, trial_neigh_model, trial_scores_all,\n",
    "            criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device\n",
    "        )\n",
    "        val =  validation_loop(trial_model, val_loader, trial_neigh_model, trial_scores_all, device=device)\n",
    "        if not torch.isfinite(torch.tensor(val)):\n",
    "            raise optuna.TrialPruned()  # or return a sentinel very-low score\n",
    "        return val\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                dataset, pi_0, train_size + val_size,\n",
    "                random_state=(run + 1) * train_size\n",
    "            )\n",
    "\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data   = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            regression_model = RegressionModel(\n",
    "                n_actions=n_actions, action_context=our_x,\n",
    "                base_model=LogisticRegression(random_state=12345)\n",
    "            )\n",
    "            regression_model.fit(\n",
    "                train_data['x'], train_data['a'], train_data['r'],\n",
    "                original_policy_prob[train_data['x_idx'], train_data['a']].squeeze()\n",
    "            )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=num_neighbors\n",
    "            )\n",
    "\n",
    "            ### NEW: build scores_all ONCE per NeighborhoodModel and keep it on device\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),   # [n_users, n_actions]\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "\n",
    "            # DataLoaders: feed the GPU\n",
    "            num_workers = 4 if torch.cuda.is_available() else 0\n",
    "            cf_dataset = CustomCFDataset(\n",
    "                train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "            )\n",
    "            train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            val_dataset = CustomCFDataset(\n",
    "                val_data['x_idx'], val_data['a'], val_data['r'], original_policy_prob\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset, batch_size=val_size, shuffle=False,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "\n",
    "            # ---- Hyperparam search ----\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            # ---- Retrain with best params ----\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=num_neighbors\n",
    "            )\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "            assert (not torch.cuda.is_available()) or next(model.parameters()).is_cuda\n",
    "\n",
    "            ### CHANGED: pass scores_all\n",
    "            train(\n",
    "                model, train_loader, neighberhoodmodel, scores_all,\n",
    "                criterion=SNDRPolicyLoss(),\n",
    "                num_epochs=best_params['num_epochs'], lr=best_params['lr'],\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # Pull updated params back to CPU numpy\n",
    "            our_x_t, our_a_t = model.get_params()\n",
    "            our_a, our_x = our_a_t.detach().cpu().numpy(), our_x_t.detach().cpu().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "\n",
    "            # reset the working embeddings for next run\n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Item CTR: 0.12972795060603162\n",
      "Optimal greedy CTR: 0.19999707792821972\n",
      "Optimal Stochastic CTR: 0.19982996880994605\n",
      "Our Initial CTR: 0.1646085673501415\n"
     ]
    }
   ],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.2\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emb_a', 'our_a', 'original_a', 'emb_x', 'our_x', 'original_x', 'q_x_a', 'n_actions', 'n_users', 'emb_dim', 'user_prior'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 200\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [100, 1000, 5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of num_rounds_list: <class 'list'>\n",
      "Value of num_rounds_list: [100, 1000, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:45,691] A new study created in memory with name: no-name-5fea4096-9355-4104-93ba-289be5ad7198\n",
      "Best trial: 0. Best value: 0.0685052:  10%|█         | 1/10 [00:02<00:23,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:48,307] Trial 0 finished with value: 0.06850524139895192 and parameters: {'lr': 0.0002654607201042784, 'num_epochs': 10}. Best is trial 0 with value: 0.06850524139895192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.0685183:  20%|██        | 2/10 [00:04<00:15,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:49,729] Trial 1 finished with value: 0.06851827368466862 and parameters: {'lr': 0.0005420404332403144, 'num_epochs': 4}. Best is trial 1 with value: 0.06851827368466862.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.0685183:  30%|███       | 3/10 [00:05<00:11,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:50,958] Trial 2 finished with value: 0.0675588422692373 and parameters: {'lr': 0.003184443088718641, 'num_epochs': 9}. Best is trial 1 with value: 0.06851827368466862.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.0685284:  40%|████      | 4/10 [00:06<00:09,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:52,321] Trial 3 finished with value: 0.06852836237972722 and parameters: {'lr': 0.000888732254103444, 'num_epochs': 2}. Best is trial 3 with value: 0.06852836237972722.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.0685454:  50%|█████     | 5/10 [00:07<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:53,560] Trial 4 finished with value: 0.06854542320330326 and parameters: {'lr': 0.0005849257243417382, 'num_epochs': 2}. Best is trial 4 with value: 0.06854542320330326.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.0685605:  60%|██████    | 6/10 [00:09<00:05,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:54,932] Trial 5 finished with value: 0.0685605079641652 and parameters: {'lr': 0.000126102617822091, 'num_epochs': 5}. Best is trial 5 with value: 0.0685605079641652.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.0685605:  70%|███████   | 7/10 [00:10<00:04,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:56,281] Trial 6 finished with value: 0.06850528291590169 and parameters: {'lr': 0.0006596203914983771, 'num_epochs': 4}. Best is trial 5 with value: 0.0685605079641652.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.0685605:  80%|████████  | 8/10 [00:11<00:02,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:57,617] Trial 7 finished with value: 0.06852502622592527 and parameters: {'lr': 0.0002753664103592038, 'num_epochs': 7}. Best is trial 5 with value: 0.0685605079641652.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.0685605:  90%|█████████ | 9/10 [00:13<00:01,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:08:59,044] Trial 8 finished with value: 0.0684886361248661 and parameters: {'lr': 0.0003615900593896426, 'num_epochs': 9}. Best is trial 5 with value: 0.0685605079641652.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.0685605: 100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:00,481] Trial 9 finished with value: 0.06676483224715063 and parameters: {'lr': 0.006094987159532914, 'num_epochs': 8}. Best is trial 5 with value: 0.0685605079641652.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:04,523] A new study created in memory with name: no-name-57086a7d-2440-4fb7-9c29-8e29a0de6184\n",
      "Best trial: 0. Best value: 0.116651:  10%|█         | 1/10 [00:05<00:46,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:09,646] Trial 0 finished with value: 0.11665085776872958 and parameters: {'lr': 0.00513900233081249, 'num_epochs': 4}. Best is trial 0 with value: 0.11665085776872958.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.116651:  20%|██        | 2/10 [00:09<00:38,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:14,158] Trial 1 finished with value: 0.10800976228539946 and parameters: {'lr': 0.007635075438001359, 'num_epochs': 6}. Best is trial 0 with value: 0.11665085776872958.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12313:  30%|███       | 3/10 [00:13<00:31,  4.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:18,409] Trial 2 finished with value: 0.12312963445972909 and parameters: {'lr': 0.0008185832253474291, 'num_epochs': 1}. Best is trial 2 with value: 0.12312963445972909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12313:  40%|████      | 4/10 [00:18<00:26,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:22,830] Trial 3 finished with value: 0.12126200983449899 and parameters: {'lr': 0.005724982060111826, 'num_epochs': 2}. Best is trial 2 with value: 0.12312963445972909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12313:  50%|█████     | 5/10 [00:22<00:22,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:27,163] Trial 4 finished with value: 0.12159988930705976 and parameters: {'lr': 0.0011375273215777977, 'num_epochs': 5}. Best is trial 2 with value: 0.12312963445972909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12313:  60%|██████    | 6/10 [00:27<00:18,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:31,808] Trial 5 finished with value: 0.12086210896080418 and parameters: {'lr': 0.001520037961209047, 'num_epochs': 5}. Best is trial 2 with value: 0.12312963445972909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12313:  70%|███████   | 7/10 [00:31<00:13,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:36,184] Trial 6 finished with value: 0.12227316434276188 and parameters: {'lr': 0.0007357831293124629, 'num_epochs': 6}. Best is trial 2 with value: 0.12312963445972909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12313:  80%|████████  | 8/10 [00:36<00:08,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:40,762] Trial 7 finished with value: 0.12312763073704458 and parameters: {'lr': 0.00023464842935984716, 'num_epochs': 4}. Best is trial 2 with value: 0.12312963445972909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.123203:  90%|█████████ | 9/10 [00:40<00:04,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:45,518] Trial 8 finished with value: 0.12320337501028035 and parameters: {'lr': 0.00010724048664860697, 'num_epochs': 9}. Best is trial 8 with value: 0.12320337501028035.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.123203: 100%|██████████| 10/10 [00:45<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:09:50,389] Trial 9 finished with value: 0.1137421606380046 and parameters: {'lr': 0.00559515888844531, 'num_epochs': 5}. Best is trial 8 with value: 0.12320337501028035.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:10:15,332] A new study created in memory with name: no-name-b709120d-83e5-4b2f-bd48-8e2e2d800e24\n",
      "Best trial: 0. Best value: 0.00775151:  10%|█         | 1/10 [00:22<03:24, 22.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:10:38,006] Trial 0 finished with value: 0.007751511197912647 and parameters: {'lr': 0.005679380780728679, 'num_epochs': 7}. Best is trial 0 with value: 0.007751511197912647.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.121373:  20%|██        | 2/10 [00:44<02:58, 22.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:11:00,115] Trial 1 finished with value: 0.12137257775949364 and parameters: {'lr': 0.002041867196698378, 'num_epochs': 2}. Best is trial 1 with value: 0.12137257775949364.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.123305:  30%|███       | 3/10 [01:06<02:35, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:11:22,029] Trial 2 finished with value: 0.12330506899323798 and parameters: {'lr': 0.00018602033136039747, 'num_epochs': 6}. Best is trial 2 with value: 0.12330506899323798.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.123305:  40%|████      | 4/10 [01:28<02:11, 21.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:11:43,723] Trial 3 finished with value: 0.12320342414546585 and parameters: {'lr': 0.00029996775390245814, 'num_epochs': 5}. Best is trial 2 with value: 0.12330506899323798.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.123305:  50%|█████     | 5/10 [01:49<01:48, 21.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:12:04,941] Trial 4 finished with value: 0.02414150495193551 and parameters: {'lr': 0.007585252303499978, 'num_epochs': 5}. Best is trial 2 with value: 0.12330506899323798.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.123305:  60%|██████    | 6/10 [02:10<01:25, 21.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:12:25,939] Trial 5 finished with value: 0.12145414952331102 and parameters: {'lr': 0.0004983648379489524, 'num_epochs': 10}. Best is trial 2 with value: 0.12330506899323798.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.123305:  70%|███████   | 7/10 [02:31<01:04, 21.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:12:47,262] Trial 6 finished with value: 0.12237386056925714 and parameters: {'lr': 0.00034040296062410095, 'num_epochs': 10}. Best is trial 2 with value: 0.12330506899323798.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.123528:  80%|████████  | 8/10 [02:52<00:42, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:13:08,132] Trial 7 finished with value: 0.12352846483790271 and parameters: {'lr': 0.00011619672893171236, 'num_epochs': 4}. Best is trial 7 with value: 0.12352846483790271.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.123528:  90%|█████████ | 9/10 [03:13<00:21, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:13:28,903] Trial 8 finished with value: 0.12345730307331454 and parameters: {'lr': 0.0002112085770288479, 'num_epochs': 3}. Best is trial 7 with value: 0.12352846483790271.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.123528: 100%|██████████| 10/10 [03:34<00:00, 21.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:13:49,810] Trial 9 finished with value: 0.12240656282516243 and parameters: {'lr': 0.0005731278137496035, 'num_epochs': 5}. Best is trial 7 with value: 0.12352846483790271.\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of num_rounds_list:\", type(num_rounds_list))\n",
    "print(\"Value of num_rounds_list:\", num_rounds_list)\n",
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16460857</td>\n",
       "      <td>0.26683245</td>\n",
       "      <td>0.17712219</td>\n",
       "      <td>0.14945164</td>\n",
       "      <td>0.17085227</td>\n",
       "      <td>0.24155110</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.16461216</td>\n",
       "      <td>0.26690491</td>\n",
       "      <td>0.17714852</td>\n",
       "      <td>0.14944676</td>\n",
       "      <td>0.17066500</td>\n",
       "      <td>0.24160329</td>\n",
       "      <td>0.75690164</td>\n",
       "      <td>0.00062972</td>\n",
       "      <td>0.87627144</td>\n",
       "      <td>0.00025842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.16461757</td>\n",
       "      <td>0.21255844</td>\n",
       "      <td>0.17328173</td>\n",
       "      <td>0.18571238</td>\n",
       "      <td>0.18720475</td>\n",
       "      <td>0.19274166</td>\n",
       "      <td>0.75690242</td>\n",
       "      <td>0.00328996</td>\n",
       "      <td>0.87628954</td>\n",
       "      <td>0.00148740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.16462194</td>\n",
       "      <td>0.18971987</td>\n",
       "      <td>0.17098008</td>\n",
       "      <td>0.17483275</td>\n",
       "      <td>0.17580786</td>\n",
       "      <td>0.17950374</td>\n",
       "      <td>0.75693903</td>\n",
       "      <td>0.00446385</td>\n",
       "      <td>0.87628090</td>\n",
       "      <td>0.00206469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0         0.16460857 0.26683245 0.17712219 0.14945164 0.17085227 0.24155110   \n",
       "100       0.16461216 0.26690491 0.17714852 0.14944676 0.17066500 0.24160329   \n",
       "1000      0.16461757 0.21255844 0.17328173 0.18571238 0.18720475 0.19274166   \n",
       "5000      0.16462194 0.18971987 0.17098008 0.17483275 0.17580786 0.17950374   \n",
       "\n",
       "      action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0              0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "100            0.75690164    0.00062972            0.87627144     0.00025842  \n",
       "1000           0.75690242    0.00328996            0.87628954     0.00148740  \n",
       "5000           0.75693903    0.00446385            0.87628090     0.00206469  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16460857</td>\n",
       "      <td>0.26683245</td>\n",
       "      <td>0.17712219</td>\n",
       "      <td>0.14945164</td>\n",
       "      <td>0.17085227</td>\n",
       "      <td>0.24155110</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.16461216</td>\n",
       "      <td>0.26690491</td>\n",
       "      <td>0.17714852</td>\n",
       "      <td>0.14944676</td>\n",
       "      <td>0.17066500</td>\n",
       "      <td>0.24160329</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.75690164</td>\n",
       "      <td>0.00062972</td>\n",
       "      <td>0.87627144</td>\n",
       "      <td>0.00025842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.16461757</td>\n",
       "      <td>0.21255844</td>\n",
       "      <td>0.17328173</td>\n",
       "      <td>0.18571238</td>\n",
       "      <td>0.18720475</td>\n",
       "      <td>0.19274166</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.75690242</td>\n",
       "      <td>0.00328996</td>\n",
       "      <td>0.87628954</td>\n",
       "      <td>0.00148740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.16462194</td>\n",
       "      <td>0.18971987</td>\n",
       "      <td>0.17098008</td>\n",
       "      <td>0.17483275</td>\n",
       "      <td>0.17580786</td>\n",
       "      <td>0.17950374</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.75693903</td>\n",
       "      <td>0.00446385</td>\n",
       "      <td>0.87628090</td>\n",
       "      <td>0.00206469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0         0.16460857 0.26683245 0.17712219 0.14945164 0.17085227 0.24155110   \n",
       "100       0.16461216 0.26690491 0.17714852 0.14944676 0.17066500 0.24160329   \n",
       "1000      0.16461757 0.21255844 0.17328173 0.18571238 0.18720475 0.19274166   \n",
       "5000      0.16462194 0.18971987 0.17098008 0.17483275 0.17580786 0.17950374   \n",
       "\n",
       "        ipw_var  reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  \\\n",
       "0    0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "100  0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "1000 0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "5000 0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "\n",
       "      action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0              0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "100            0.75690164    0.00062972            0.87627144     0.00025842  \n",
       "1000           0.75690242    0.00328996            0.87628954     0.00148740  \n",
       "5000           0.75693903    0.00446385            0.87628090     0.00206469  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [3000, 6000, 8000, 9000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:14:21,463] A new study created in memory with name: no-name-27a813fd-e2fe-4f5f-a398-dd5391a365df\n",
      "Best trial: 0. Best value: 0.112966:  10%|█         | 1/10 [00:12<01:53, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:14:34,105] Trial 0 finished with value: 0.1129662714455142 and parameters: {'lr': 0.0001490673405748964, 'num_epochs': 6}. Best is trial 0 with value: 0.1129662714455142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.112966:  20%|██        | 2/10 [00:25<01:42, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:14:47,005] Trial 1 finished with value: 0.11291203939897486 and parameters: {'lr': 0.00047530784918354364, 'num_epochs': 2}. Best is trial 0 with value: 0.1129662714455142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.112966:  30%|███       | 3/10 [00:38<01:29, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:14:59,859] Trial 2 finished with value: 0.08084231503459138 and parameters: {'lr': 0.005860266095947463, 'num_epochs': 6}. Best is trial 0 with value: 0.1129662714455142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.112966:  40%|████      | 4/10 [00:51<01:18, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:15:13,198] Trial 3 finished with value: 0.11262247621505565 and parameters: {'lr': 0.00019170855650737234, 'num_epochs': 10}. Best is trial 0 with value: 0.1129662714455142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.113222:  50%|█████     | 5/10 [01:04<01:04, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:15:25,842] Trial 4 finished with value: 0.1132220538268914 and parameters: {'lr': 0.0001170405725131894, 'num_epochs': 3}. Best is trial 4 with value: 0.1132220538268914.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.113222:  60%|██████    | 6/10 [01:17<00:51, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:15:38,885] Trial 5 finished with value: 0.11252789515523706 and parameters: {'lr': 0.0011919024613311698, 'num_epochs': 4}. Best is trial 4 with value: 0.1132220538268914.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.113222:  70%|███████   | 7/10 [01:30<00:38, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:15:51,596] Trial 6 finished with value: 0.1122886454379291 and parameters: {'lr': 0.0009552556465462943, 'num_epochs': 3}. Best is trial 4 with value: 0.1132220538268914.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.113222:  80%|████████  | 8/10 [01:43<00:26, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:16:05,065] Trial 7 finished with value: 0.11129045678071586 and parameters: {'lr': 0.0007298335485719134, 'num_epochs': 9}. Best is trial 4 with value: 0.1132220538268914.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.113222:  90%|█████████ | 9/10 [01:55<00:12, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:16:17,319] Trial 8 finished with value: 0.11138284902559228 and parameters: {'lr': 0.003099440972963746, 'num_epochs': 2}. Best is trial 4 with value: 0.1132220538268914.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.113222: 100%|██████████| 10/10 [02:08<00:00, 12.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:16:29,671] Trial 9 finished with value: 0.09407600354922759 and parameters: {'lr': 0.002834349734231312, 'num_epochs': 9}. Best is trial 4 with value: 0.1132220538268914.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:17:06,620] A new study created in memory with name: no-name-4cb7eae0-e34e-4f89-8d89-719bc982160c\n",
      "Best trial: 0. Best value: 0.089756:  10%|█         | 1/10 [00:24<03:36, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:17:30,699] Trial 0 finished with value: 0.08975603002674896 and parameters: {'lr': 0.0018715393944455188, 'num_epochs': 9}. Best is trial 0 with value: 0.08975603002674896.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.126579:  20%|██        | 2/10 [00:49<03:18, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:17:56,034] Trial 1 finished with value: 0.12657863873824327 and parameters: {'lr': 0.00020780235940691676, 'num_epochs': 3}. Best is trial 1 with value: 0.12657863873824327.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12665:  30%|███       | 3/10 [01:14<02:54, 24.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:18:21,082] Trial 2 finished with value: 0.12664991145880033 and parameters: {'lr': 0.00016341077089658171, 'num_epochs': 3}. Best is trial 2 with value: 0.12664991145880033.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12665:  40%|████      | 4/10 [01:39<02:30, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:18:46,546] Trial 3 finished with value: 0.12430813136973809 and parameters: {'lr': 0.0007871192128402262, 'num_epochs': 5}. Best is trial 2 with value: 0.12664991145880033.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12665:  50%|█████     | 5/10 [02:05<02:05, 25.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:19:11,773] Trial 4 finished with value: 0.12599888625179373 and parameters: {'lr': 0.0007230231745635745, 'num_epochs': 2}. Best is trial 2 with value: 0.12664991145880033.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.12665:  60%|██████    | 6/10 [02:30<01:41, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 11:19:37,345] Trial 5 pruned. \n"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
