{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import gym\n",
    "import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  val_size=2000\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            dataset['env'],\n",
    "                                                            pi_0,\n",
    "                                                            train_size + val_size\n",
    "                                                            )\n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "\n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            # bpr_model = BPRModel(\n",
    "            #                     n_users,\n",
    "            #                     n_actions,\n",
    "            #                     emb_x.shape[1], \n",
    "            #                     initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "            #                     initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "            #                     )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "                reg_dm = 0.0\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            # Bloss = BPRLoss()\n",
    "\n",
    "            \n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=5)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            # fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "            reg_dm = 0.0\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 180,\n",
    "                    n_users = 180,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.2 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 200\n",
    "num_neighbors = 51\n",
    "num_rounds_list = [1000, 2000, 3000, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:38:48,647] A new study created in memory with name: no-name-d51a3529-5882-4495-a263-f200e32486e0\n",
      "[I 2025-07-12 11:38:50,412] Trial 0 finished with value: -0.001421813136377667 and parameters: {'lr': 0.001667000269167642, 'num_epochs': 4}. Best is trial 0 with value: -0.001421813136377667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053679542930237224 0.017338296539337973 0.005555555555555556 0.0182 88.56600189208984\n",
      "Estimated rewards variance: 0.01143674616268241\n",
      "Estimated rewards mean: 0.017393447009291976\n",
      "Estimated rewards dm: 0.01739326335618298\n",
      "Estimated rewards iw: 1.0000658706169503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:38:52,208] Trial 1 finished with value: -0.010262909056418707 and parameters: {'lr': 0.0415548211213662, 'num_epochs': 9}. Best is trial 1 with value: -0.010262909056418707.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053679542930237224 0.017338296539337973 0.005555555555555556 0.0182 88.56600189208984\n",
      "Estimated rewards variance: 0.017574149986079448\n",
      "Estimated rewards mean: 0.01864935314854631\n",
      "Estimated rewards dm: 0.018646335209822815\n",
      "Estimated rewards iw: 0.9902100905592409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:38:54,103] Trial 2 finished with value: -0.0013806729402571216 and parameters: {'lr': 2.868994421386672e-05, 'num_epochs': 7}. Best is trial 1 with value: -0.010262909056418707.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053679542930237224 0.017338296539337973 0.005555555555555556 0.0182 88.56600189208984\n",
      "Estimated rewards variance: 0.01141900638303754\n",
      "Estimated rewards mean: 0.01740540245616678\n",
      "Estimated rewards dm: 0.0174052297603873\n",
      "Estimated rewards iw: 0.9999923432666546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:38:55,864] Trial 3 finished with value: -0.001426719511954666 and parameters: {'lr': 0.008377871856152458, 'num_epochs': 2}. Best is trial 1 with value: -0.010262909056418707.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053679542930237224 0.017338296539337973 0.005555555555555556 0.0182 88.56600189208984\n",
      "Estimated rewards variance: 0.01144847303391115\n",
      "Estimated rewards mean: 0.017407833195578724\n",
      "Estimated rewards dm: 0.01740768975244712\n",
      "Estimated rewards iw: 0.9997847003922323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:38:57,529] Trial 4 finished with value: -0.0013804041610357039 and parameters: {'lr': 2.597879663273124e-05, 'num_epochs': 1}. Best is trial 1 with value: -0.010262909056418707.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053679542930237224 0.017338296539337973 0.005555555555555556 0.0182 88.56600189208984\n",
      "Estimated rewards variance: 0.011418953870294547\n",
      "Estimated rewards mean: 0.01740558484360278\n",
      "Estimated rewards dm: 0.01740541250205122\n",
      "Estimated rewards iw: 0.999997675360792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:39:15,360] A new study created in memory with name: no-name-8b6f4bf5-cdf4-4638-90ea-f4f74fafdb0b\n",
      "[I 2025-07-12 11:39:18,824] Trial 0 finished with value: 0.0007022101963990359 and parameters: {'lr': 0.0008267064161580157, 'num_epochs': 1}. Best is trial 0 with value: 0.0007022101963990359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05314686060029868 0.01352486851982936 0.005555555555555556 0.0138 88.19860076904297\n",
      "Estimated rewards variance: 0.007766610436416377\n",
      "Estimated rewards mean: 0.013479515361050255\n",
      "Estimated rewards dm: 0.01347945891993797\n",
      "Estimated rewards iw: 0.9997560490528876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:39:22,259] Trial 1 finished with value: 0.0006988624747999994 and parameters: {'lr': 0.0011847953277015395, 'num_epochs': 2}. Best is trial 1 with value: 0.0006988624747999994.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05314686060029869 0.013524868519829363 0.005555555555555556 0.0138 88.19860076904297\n",
      "Estimated rewards variance: 0.007769186268025502\n",
      "Estimated rewards mean: 0.013480405290713888\n",
      "Estimated rewards dm: 0.013480350056850415\n",
      "Estimated rewards iw: 0.9996601870161991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:39:25,876] Trial 2 finished with value: 0.0006743853592347179 and parameters: {'lr': 0.0024885158096756128, 'num_epochs': 7}. Best is trial 2 with value: 0.0006743853592347179.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05314686060029868 0.01352486851982936 0.005555555555555556 0.0138 88.19860076904297\n",
      "Estimated rewards variance: 0.007800681037254123\n",
      "Estimated rewards mean: 0.013507742062405972\n",
      "Estimated rewards dm: 0.013507685823920883\n",
      "Estimated rewards iw: 0.996096479896002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:39:29,304] Trial 3 finished with value: 0.0006990516463400796 and parameters: {'lr': 0.0003300561468802669, 'num_epochs': 2}. Best is trial 2 with value: 0.0006743853592347179.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05314686060029868 0.013524868519829363 0.005555555555555556 0.0138 88.19860076904297\n",
      "Estimated rewards variance: 0.007766758902631575\n",
      "Estimated rewards mean: 0.013476601061446984\n",
      "Estimated rewards dm: 0.013476547734123386\n",
      "Estimated rewards iw: 0.9999312903161328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:39:32,918] Trial 4 finished with value: 0.0006162381168083753 and parameters: {'lr': 0.006474796005172913, 'num_epochs': 5}. Best is trial 4 with value: 0.0006162381168083753.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05314686060029869 0.013524868519829363 0.005555555555555556 0.0138 88.19860076904297\n",
      "Estimated rewards variance: 0.007824928032703913\n",
      "Estimated rewards mean: 0.013489484970608945\n",
      "Estimated rewards dm: 0.013489374477999642\n",
      "Estimated rewards iw: 0.9916043721513467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:39:55,193] A new study created in memory with name: no-name-b59523c6-4928-4a31-b53a-9f63b2af9511\n",
      "[I 2025-07-12 11:40:00,622] Trial 0 finished with value: 0.0020380606936551797 and parameters: {'lr': 0.001926867315624549, 'num_epochs': 4}. Best is trial 0 with value: 0.0020380606936551797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05830298610458417 0.017930466230908347 0.005555555555555556 0.0108 88.99579620361328\n",
      "Estimated rewards variance: 0.009771880841086267\n",
      "Estimated rewards mean: 0.018114353506505365\n",
      "Estimated rewards dm: 0.018115780323191322\n",
      "Estimated rewards iw: 0.9990450248153724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:40:06,240] Trial 1 finished with value: 0.0019870202832318433 and parameters: {'lr': 6.451815191981529e-05, 'num_epochs': 7}. Best is trial 1 with value: 0.0019870202832318433.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05830298610458417 0.017930466230908347 0.005555555555555556 0.0108 88.99579620361328\n",
      "Estimated rewards variance: 0.00983196181881963\n",
      "Estimated rewards mean: 0.01816215582720323\n",
      "Estimated rewards dm: 0.018163582130751373\n",
      "Estimated rewards iw: 0.9999517341545877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:40:11,521] Trial 2 finished with value: 0.001984715944478306 and parameters: {'lr': 1.0417350246698749e-05, 'num_epochs': 1}. Best is trial 2 with value: 0.001984715944478306.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05830298610458415 0.01793046623090835 0.005555555555555556 0.0108 88.99579620361328\n",
      "Estimated rewards variance: 0.00983516228357327\n",
      "Estimated rewards mean: 0.018165116760238558\n",
      "Estimated rewards dm: 0.01816654293421198\n",
      "Estimated rewards iw: 0.9999990937949363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:40:16,794] Trial 3 finished with value: 0.0019318331259345276 and parameters: {'lr': 0.003041205182259255, 'num_epochs': 2}. Best is trial 3 with value: 0.0019318331259345276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05830298610458415 0.01793046623090835 0.005555555555555556 0.0108 88.99579620361328\n",
      "Estimated rewards variance: 0.00983090514135159\n",
      "Estimated rewards mean: 0.018105230267989103\n",
      "Estimated rewards dm: 0.018106666598292815\n",
      "Estimated rewards iw: 0.9987375298653602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:40:22,626] Trial 4 finished with value: 0.001841995537373433 and parameters: {'lr': 0.0014087109519266517, 'num_epochs': 10}. Best is trial 4 with value: 0.001841995537373433.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05830298610458416 0.01793046623090835 0.005555555555555556 0.0108 88.99579620361328\n",
      "Estimated rewards variance: 0.009848445966640081\n",
      "Estimated rewards mean: 0.018044250117223527\n",
      "Estimated rewards dm: 0.018045715906220214\n",
      "Estimated rewards iw: 0.9972312540219226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:40:49,495] A new study created in memory with name: no-name-00bbb04c-f83e-43cb-b1eb-92a8a2982864\n",
      "[I 2025-07-12 11:40:57,286] Trial 0 finished with value: 0.003049270090122928 and parameters: {'lr': 0.002124313970742067, 'num_epochs': 7}. Best is trial 0 with value: 0.003049270090122928.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05459684694381458 0.0146445249504824 0.005555555555555556 0.0128 88.21700286865234\n",
      "Estimated rewards variance: 0.007063084609510306\n",
      "Estimated rewards mean: 0.014669163761667227\n",
      "Estimated rewards dm: 0.014669546049722442\n",
      "Estimated rewards iw: 1.0007179370048387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:41:05,028] Trial 1 finished with value: 0.0029944618923268343 and parameters: {'lr': 0.0001128782197339371, 'num_epochs': 7}. Best is trial 1 with value: 0.0029944618923268343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05459684694381458 0.014644524950482398 0.005555555555555556 0.0128 88.21700286865234\n",
      "Estimated rewards variance: 0.00705083472134069\n",
      "Estimated rewards mean: 0.014594202556242202\n",
      "Estimated rewards dm: 0.01459457304850975\n",
      "Estimated rewards iw: 0.9999552599812157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:41:12,574] Trial 2 finished with value: -0.0046365972546273775 and parameters: {'lr': 0.022848220993349037, 'num_epochs': 4}. Best is trial 2 with value: -0.0046365972546273775.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05459684694381458 0.014644524950482398 0.005555555555555556 0.0128 88.21700286865234\n",
      "Estimated rewards variance: 0.013763071962569106\n",
      "Estimated rewards mean: 0.018005837551197976\n",
      "Estimated rewards dm: 0.018006286387830764\n",
      "Estimated rewards iw: 0.8841507938290964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:41:19,818] Trial 3 finished with value: 0.002765568847382504 and parameters: {'lr': 0.007605619228011804, 'num_epochs': 3}. Best is trial 2 with value: -0.0046365972546273775.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05459684694381459 0.014644524950482398 0.005555555555555556 0.0128 88.21700286865234\n",
      "Estimated rewards variance: 0.007018549986818782\n",
      "Estimated rewards mean: 0.01431219600592467\n",
      "Estimated rewards dm: 0.01431257559857445\n",
      "Estimated rewards iw: 0.9953142851872845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:41:27,735] Trial 4 finished with value: 0.002000388279942946 and parameters: {'lr': 0.0038960450928688544, 'num_epochs': 10}. Best is trial 2 with value: -0.0046365972546273775.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05459684694381458 0.014644524950482398 0.005555555555555556 0.0128 88.21700286865234\n",
      "Estimated rewards variance: 0.007925352794421442\n",
      "Estimated rewards mean: 0.01503884978395125\n",
      "Estimated rewards dm: 0.015039215456372648\n",
      "Estimated rewards iw: 0.982427525098866\n"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.2703</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>1.0946</td>\n",
       "      <td>1.0995</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>1.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>0.1329</td>\n",
       "      <td>0.2963</td>\n",
       "      <td>0.1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.2717</td>\n",
       "      <td>0.0662</td>\n",
       "      <td>0.2726</td>\n",
       "      <td>0.0563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.8088</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.7965</td>\n",
       "      <td>0.7827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0             0.0151 0.0200  0.0000   0.0181   0.0186     0.0196   \n",
       "1000          0.0151 0.0000  0.0000   0.0179   0.0137     0.0045   \n",
       "2000          0.0146 0.0153  0.0000   0.0137   0.0141     0.0147   \n",
       "3000          0.0146 0.0177  0.0000   0.0179   0.0174     0.0163   \n",
       "4000          0.0166 0.0293  0.0000   0.0162   0.0234     0.0281   \n",
       "\n",
       "      action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                  0.2703        0.0000                0.2661         0.0000  \n",
       "1000               1.0946        1.0995                0.9902         1.0044  \n",
       "2000               0.2892        0.1329                0.2963         0.1123  \n",
       "3000               0.2717        0.0662                0.2726         0.0563  \n",
       "4000               0.8088        0.7738                0.7965         0.7827  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`pscore` must be 1D array, but got 0D array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df5 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 118\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size, val_size)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[1;32m    117\u001b[0m     policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(softmax(our_x \u001b[38;5;241m@\u001b[39m our_a\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m     conv_results\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighberhoodmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_policy_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m     conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(calc_reward(dataset, policy), conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    120\u001b[0m     conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((emb_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)), np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((original_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))])\n",
      "File \u001b[0;32m/code/simulation_utils.py:214\u001b[0m, in \u001b[0;36meval_policy\u001b[0;34m(model, test_data, original_policy_prob, policy)\u001b[0m\n\u001b[1;32m    211\u001b[0m pscore \u001b[38;5;241m=\u001b[39m original_policy_prob[test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_idx\u001b[39m\u001b[38;5;124m'\u001b[39m], actions]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    213\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(dm\u001b[38;5;241m.\u001b[39mestimate_policy_value(policy, scores))\n\u001b[0;32m--> 214\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_policy_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpscore\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    215\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(ipw\u001b[38;5;241m.\u001b[39mestimate_policy_value(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], policy, pscore\u001b[38;5;241m=\u001b[39mpscore))\n\u001b[1;32m    216\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(sndr\u001b[38;5;241m.\u001b[39mestimate_policy_value(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], policy, scores, pscore\u001b[38;5;241m=\u001b[39mpscore))\n",
      "File \u001b[0;32m/code/estimators.py:1050\u001b[0m, in \u001b[0;36mDoublyRobust.estimate_policy_value\u001b[0;34m(self, reward, action, action_dist, estimated_rewards_by_reg_model, pscore, position, estimated_pscore, **kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     pscore_ \u001b[38;5;241m=\u001b[39m estimated_pscore\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m     \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m     pscore_ \u001b[38;5;241m=\u001b[39m pscore\n\u001b[1;32m   1052\u001b[0m check_ope_inputs(\n\u001b[1;32m   1053\u001b[0m     action_dist\u001b[38;5;241m=\u001b[39maction_dist,\n\u001b[1;32m   1054\u001b[0m     position\u001b[38;5;241m=\u001b[39mposition,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     estimated_rewards_by_reg_model\u001b[38;5;241m=\u001b[39mestimated_rewards_by_reg_model,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/code/saito_helpers.py:410\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, name, expected_dim)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(array)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m expected_dim:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `pscore` must be 1D array, but got 0D array"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
