{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# import gym\n",
    "# import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")  # TF32 = big speedup on Ada\n",
    "\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "# from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "    num_runs,\n",
    "    num_neighbors,\n",
    "    num_rounds_list,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    val_size=2000,\n",
    "    n_trials=10,    \n",
    "    prev_best_params=None\n",
    "):\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # ---- Device & CUDA fast-paths ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_float32_matmul_precision(\"high\")  # TF32 on Ada\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "\n",
    "    ### NEW: indices once\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    def T(x):\n",
    "        return torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Add a dictionary to store the best hyperparameters for each training size\n",
    "    best_hyperparams_by_size = {}\n",
    "    best_reward = -float('inf')\n",
    "    overall_best_params = {}\n",
    "\n",
    "    # ---- Optuna objective uses train/val loaders from outer scope ----\n",
    "    def objective(trial):\n",
    "        # Current parameters\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "        trial_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "        trial_num_neighbors = trial.suggest_int(\"num_neighbors\", 3, 15)\n",
    "        lr_decay = trial.suggest_float(\"lr_decay\", 0.8, 1.0)\n",
    "\n",
    "        # Create neighborhood model and scores:\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "            train_data['x_idx'], train_data['a'],\n",
    "            our_a, our_x, train_data['r'],\n",
    "            num_neighbors=trial_num_neighbors\n",
    "        )\n",
    "\n",
    "        ### NEW: build trial_scores_all ONCE and keep on device\n",
    "        trial_scores_all = torch.as_tensor(\n",
    "            trial_neigh_model.predict(all_user_indices),   # shape [n_users, n_actions]\n",
    "            device=device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        trial_model = CFModel(\n",
    "            n_users, n_actions, emb_dim,\n",
    "            initial_user_embeddings=T(our_x),\n",
    "            initial_actions_embeddings=T(our_a)\n",
    "        ).to(device)\n",
    "\n",
    "        assert (not torch.cuda.is_available()) or next(trial_model.parameters()).is_cuda\n",
    "\n",
    "        # Create a DataLoader with trial batch size\n",
    "        final_train_loader = DataLoader(\n",
    "            cf_dataset, batch_size=trial_batch_size, shuffle=True,  # Use the trial's suggested batch size\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Modified train function call with learning rate decay\n",
    "        current_lr = lr\n",
    "        for epoch in range(epochs):\n",
    "            # Apply learning rate decay for each epoch after the first\n",
    "            if epoch > 0:\n",
    "                current_lr *= lr_decay\n",
    "                \n",
    "            train(\n",
    "                trial_model, final_train_loader, trial_neigh_model, trial_scores_all,\n",
    "                criterion=SNDRPolicyLoss(), num_epochs=1, lr=current_lr, device=str(device)\n",
    "            )\n",
    "        \n",
    "        #######################\n",
    "        #######################\n",
    "        # Switching to optimize on analytical rewards instead. \n",
    "        #######################\n",
    "        #######################\n",
    "        \n",
    "        # val =  validation_loop(trial_model, val_loader, trial_neigh_model, trial_scores_all, device=device)\n",
    "        # if not torch.isfinite(torch.tensor(val)):\n",
    "        #    raise optuna.TrialPruned()  # or return a sentinel very-low score\n",
    "        # return val\n",
    "\n",
    "        # Extract embeddings from the model\n",
    "        trial_x_t, trial_a_t = trial_model.get_params()\n",
    "        trial_x = trial_x_t.detach().cpu().numpy()\n",
    "        trial_a = trial_a_t.detach().cpu().numpy()\n",
    "\n",
    "        # Calculate policy and its reward\n",
    "        trial_policy = np.expand_dims(softmax(trial_x @ trial_a.T, axis=1), -1)\n",
    "        trial_policy_reward = calc_reward(dataset, trial_policy)\n",
    "    \n",
    "        # Return the policy reward (Optuna maximizes this value)\n",
    "        return trial_policy_reward\n",
    "\n",
    "    first = True\n",
    "\n",
    "    # Store previous best hyperparams to use when starting a new training size\n",
    "    last_best_params = None\n",
    "    if prev_best_params is not None:\n",
    "        last_best_params = prev_best_params\n",
    "\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                dataset, pi_0, train_size + val_size,\n",
    "                random_state=(run + 1) * train_size\n",
    "            )\n",
    "\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data   = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            regression_model = RegressionModel(\n",
    "                n_actions=n_actions, action_context=our_x,\n",
    "                base_model=LogisticRegression(random_state=12345)\n",
    "            )\n",
    "            regression_model.fit(\n",
    "                train_data['x'], train_data['a'], train_data['r'],\n",
    "                original_policy_prob[train_data['x_idx'], train_data['a']].squeeze()\n",
    "            )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=num_neighbors\n",
    "            )\n",
    "\n",
    "            ### NEW: build scores_all ONCE per NeighborhoodModel and keep it on device\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),   # [n_users, n_actions]\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "\n",
    "            # DataLoaders: feed the GPU\n",
    "            num_workers = 4 if torch.cuda.is_available() else 0\n",
    "            cf_dataset = CustomCFDataset(\n",
    "                train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "            )\n",
    "            train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            val_dataset = CustomCFDataset(\n",
    "                val_data['x_idx'], val_data['a'], val_data['r'], original_policy_prob\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset, batch_size=val_size, shuffle=False,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "\n",
    "            # ---- Hyperparam search ----\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "            # If we have previous best parameters, enqueue them first\n",
    "            if last_best_params is not None:\n",
    "                # Enqueue a trial with the previous best parameters\n",
    "                study.enqueue_trial(last_best_params)\n",
    "\n",
    "\n",
    "            study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            best_reward_for_size = study.best_value\n",
    "\n",
    "             # Update last_best_params for the next training size\n",
    "            last_best_params = best_params\n",
    "\n",
    "            # Store the best parameters for this training size\n",
    "            best_hyperparams_by_size[train_size] = {\n",
    "                \"params\": best_params,\n",
    "                \"reward\": best_reward_for_size\n",
    "            }\n",
    "\n",
    "            # Update overall best if this is better\n",
    "            if best_reward_for_size > best_reward:\n",
    "                best_reward = best_reward_for_size\n",
    "                overall_best_params = best_params.copy()\n",
    "                overall_best_params[\"train_size\"] = train_size\n",
    "\n",
    "            # ---- Retrain with best params ----\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=num_neighbors\n",
    "            )\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "            assert (not torch.cuda.is_available()) or next(model.parameters()).is_cuda\n",
    "\n",
    "            # When retraining with best params, implement the learning rate decay:\n",
    "            current_lr = best_params['lr']\n",
    "            for epoch in range(best_params['num_epochs']):\n",
    "                if epoch > 0:\n",
    "                    current_lr *= best_params['lr_decay']\n",
    "                \n",
    "                train(\n",
    "                    model, train_loader, neighberhoodmodel, scores_all,\n",
    "                    criterion=SNDRPolicyLoss(),\n",
    "                    num_epochs=1, lr=current_lr,\n",
    "                    device=str(device)\n",
    "                )\n",
    "\n",
    "            # Pull updated params back to CPU numpy\n",
    "            our_x_t, our_a_t = model.get_params()\n",
    "            our_a, our_x = our_a_t.detach().cpu().numpy(), our_x_t.detach().cpu().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "\n",
    "            # reset the working embeddings for next run\n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "\n",
    "     # Return both the results DataFrame and the best hyperparameters\n",
    "    return pd.DataFrame.from_dict(results, orient='index'), best_hyperparams_by_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Item CTR: 0.12972795060603162\n",
      "Optimal greedy CTR: 0.19999707792821972\n",
      "Optimal Stochastic CTR: 0.19982996880994605\n",
      "Our Initial CTR: 0.1646085673501415\n"
     ]
    }
   ],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.2\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emb_a', 'our_a', 'original_a', 'emb_x', 'our_x', 'original_x', 'q_x_a', 'n_actions', 'n_users', 'emb_dim', 'user_prior'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 200\n",
    "num_neighbors = 6\n",
    "n_trials_for_optuna = 20\n",
    "num_rounds_list = [1000, 5000, 10000]\n",
    "\n",
    "# Manually define your best parameters\n",
    "best_params_to_use = {\n",
    "    \"lr\": 0.002,  # Learning rate\n",
    "    \"num_epochs\": 5,  # Number of training epochs\n",
    "    \"batch_size\": 256,  # Batch size for training\n",
    "    \"num_neighbors\": 8,  # Number of neighbors for neighborhood model\n",
    "    \"lr_decay\": 0.9  # Learning rate decay factor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of num_rounds_list: [1000, 5000, 10000]\n"
     ]
    }
   ],
   "source": [
    "print(\"Value of num_rounds_list:\", num_rounds_list)\n",
    "\n",
    "# Run the optimization\n",
    "df4, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=35000, n_trials=n_trials_for_optuna,prev_best_params=best_params_to_use)\n",
    "\n",
    "# Print best hyperparameters for each training size\n",
    "print(\"\\n=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\")\n",
    "for train_size, params in best_hyperparams_by_size.items():\n",
    "    print(f\"\\nTraining Size: {train_size}\")\n",
    "    print(f\"Best Reward: {params['reward']:.6f}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, value in params['params'].items():\n",
    "        print(f\"  {param_name}: {value}\")\n",
    "print(\"===========================\\n\")\n",
    "\n",
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
