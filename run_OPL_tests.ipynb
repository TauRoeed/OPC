{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# import gym\n",
    "# import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "# from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.8f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  val_size=2000\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    #device = torch.device('cpu')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            dataset,\n",
    "                                                            pi_0,\n",
    "                                                            train_size + val_size,\n",
    "                                                            random_state=(run + 1) * train_size\n",
    "                                                            )\n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "\n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            regression_model = RegressionModel(\n",
    "                                    n_actions=n_actions,\n",
    "                                    action_context=our_x,\n",
    "                                    base_model=LogisticRegression(random_state=12345)\n",
    "                                    )\n",
    "            \n",
    "            regression_model.fit(\n",
    "                                train_data['x'], \n",
    "                                train_data['a'],\n",
    "                                train_data['r'],\n",
    "                                original_policy_prob[train_data['x_idx'],\n",
    "                                train_data['a']].squeeze()\n",
    "                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=val_size, shuffle=True)\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "                \n",
    "            # Bloss = BPRLoss()\n",
    "            # bpr_model = BPRModel(\n",
    "            #                     n_users,\n",
    "            #                     n_actions,\n",
    "            #                     emb_x.shape[1], \n",
    "            #                     initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "            #                     initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "            #                     )\n",
    "\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            # fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Item CTR: 0.14805474537506452\n",
      "Optimal greedy CTR: 0.1993706263086395\n",
      "Optimal Stochastic CTR: 0.19237265135418052\n",
      "Our Initial CTR: 0.17421144896729363\n"
     ]
    }
   ],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 4,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.4, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.2\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emb_a', 'our_a', 'original_a', 'emb_x', 'our_x', 'original_x', 'q_x_a', 'n_actions', 'n_users', 'emb_dim', 'user_prior'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 200\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [100, 1000, 5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:15,543] A new study created in memory with name: no-name-72b1b479-85f2-43ca-9141-3ae767c95a6c\n",
      "Best trial: 0. Best value: 0.0597385:  10%|█         | 1/10 [00:03<00:29,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:18,862] Trial 0 finished with value: 0.059738499466196124 and parameters: {'lr': 0.00010895008936317886, 'num_epochs': 2}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385:  20%|██        | 2/10 [00:05<00:22,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:21,385] Trial 1 finished with value: 0.059737989427549376 and parameters: {'lr': 0.0002640820740793505, 'num_epochs': 1}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385:  30%|███       | 3/10 [00:08<00:18,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:23,876] Trial 2 finished with value: 0.05972951024368253 and parameters: {'lr': 0.0001288513640104121, 'num_epochs': 8}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385:  40%|████      | 4/10 [00:10<00:15,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:26,360] Trial 3 finished with value: 0.059681123958409577 and parameters: {'lr': 0.000671615764072603, 'num_epochs': 8}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385:  50%|█████     | 5/10 [00:13<00:13,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:29,065] Trial 4 finished with value: 0.05973626383405789 and parameters: {'lr': 0.00010507512555306303, 'num_epochs': 4}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385:  60%|██████    | 6/10 [00:16<00:10,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:31,669] Trial 5 finished with value: 0.059716355951880074 and parameters: {'lr': 0.0003165569769377956, 'num_epochs': 7}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385:  70%|███████   | 7/10 [00:18<00:07,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:34,178] Trial 6 finished with value: 0.05972137075709563 and parameters: {'lr': 0.0002521050277769762, 'num_epochs': 7}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385:  80%|████████  | 8/10 [00:21<00:05,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:36,952] Trial 7 finished with value: 0.05972496580699756 and parameters: {'lr': 0.00028816280150273637, 'num_epochs': 5}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385:  90%|█████████ | 9/10 [00:24<00:02,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:39,555] Trial 8 finished with value: 0.05972290383313898 and parameters: {'lr': 0.00016266660437986273, 'num_epochs': 10}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0597385: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:42,160] Trial 9 finished with value: 0.05973503993935411 and parameters: {'lr': 0.00026551446572454444, 'num_epochs': 2}. Best is trial 0 with value: 0.059738499466196124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:45,920] A new study created in memory with name: no-name-309970e3-f742-44d3-be34-d3fbac95d798\n",
      "Best trial: 0. Best value: 0.0796372:  10%|█         | 1/10 [00:05<00:46,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:51,065] Trial 0 finished with value: 0.07963718864633709 and parameters: {'lr': 0.00031669020900083325, 'num_epochs': 9}. Best is trial 0 with value: 0.07963718864633709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0796372:  20%|██        | 2/10 [00:10<00:40,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:56:56,072] Trial 1 finished with value: 0.0796174847990189 and parameters: {'lr': 0.0003325972397360591, 'num_epochs': 2}. Best is trial 0 with value: 0.07963718864633709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0796818:  30%|███       | 3/10 [00:15<00:35,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:57:01,148] Trial 2 finished with value: 0.07968181354760832 and parameters: {'lr': 0.0009163032794591783, 'num_epochs': 7}. Best is trial 2 with value: 0.07968181354760832.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0796818:  40%|████      | 4/10 [00:20<00:31,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:57:06,695] Trial 3 finished with value: 0.07961508847744811 and parameters: {'lr': 0.00029569804678457746, 'num_epochs': 3}. Best is trial 2 with value: 0.07968181354760832.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0796818:  50%|█████     | 5/10 [00:26<00:26,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:57:12,161] Trial 4 finished with value: 0.07961253057442715 and parameters: {'lr': 0.00015786229636644586, 'num_epochs': 6}. Best is trial 2 with value: 0.07968181354760832.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0796818:  60%|██████    | 6/10 [00:31<00:20,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:57:17,003] Trial 5 finished with value: 0.07961328912649371 and parameters: {'lr': 0.00047794159557080877, 'num_epochs': 1}. Best is trial 2 with value: 0.07968181354760832.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0796818:  70%|███████   | 7/10 [00:35<00:15,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:57:21,876] Trial 6 finished with value: 0.07961642489081437 and parameters: {'lr': 0.0009076401165972231, 'num_epochs': 1}. Best is trial 2 with value: 0.07968181354760832.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0796818:  80%|████████  | 8/10 [00:40<00:09,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:57:26,685] Trial 7 finished with value: 0.07960997618507543 and parameters: {'lr': 0.000128626698878024, 'num_epochs': 5}. Best is trial 2 with value: 0.07968181354760832.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0796818:  90%|█████████ | 9/10 [00:45<00:04,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:57:31,635] Trial 8 finished with value: 0.07965698670679999 and parameters: {'lr': 0.0005414921606618851, 'num_epochs': 5}. Best is trial 2 with value: 0.07968181354760832.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0796818: 100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:57:36,708] Trial 9 finished with value: 0.07963042723537915 and parameters: {'lr': 0.0003509915763111679, 'num_epochs': 7}. Best is trial 2 with value: 0.07968181354760832.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:58:00,050] A new study created in memory with name: no-name-285c23ff-aea3-45ba-8cee-a0e0d726373f\n",
      "Best trial: 0. Best value: 0.115142:  10%|█         | 1/10 [00:22<03:21, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:58:22,437] Trial 0 finished with value: 0.11514241973129649 and parameters: {'lr': 0.00021840401521487708, 'num_epochs': 9}. Best is trial 0 with value: 0.11514241973129649.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.115198:  20%|██        | 2/10 [00:44<02:57, 22.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:58:44,519] Trial 1 finished with value: 0.11519835250110828 and parameters: {'lr': 0.0008523001567980232, 'num_epochs': 7}. Best is trial 1 with value: 0.11519835250110828.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.115198:  30%|███       | 3/10 [01:06<02:34, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:59:06,438] Trial 2 finished with value: 0.11516182042145146 and parameters: {'lr': 0.0005261080568123465, 'num_epochs': 6}. Best is trial 1 with value: 0.11519835250110828.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.11523:  40%|████      | 4/10 [01:29<02:14, 22.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:59:29,231] Trial 3 finished with value: 0.11522962722463213 and parameters: {'lr': 0.0005515118413429057, 'num_epochs': 10}. Best is trial 3 with value: 0.11522962722463213.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.11523:  50%|█████     | 5/10 [01:51<01:51, 22.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 00:59:51,612] Trial 4 finished with value: 0.11511319983106702 and parameters: {'lr': 0.00017429541849356308, 'num_epochs': 9}. Best is trial 3 with value: 0.11522962722463213.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.11523:  60%|██████    | 6/10 [02:13<01:28, 22.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 01:00:13,420] Trial 5 finished with value: 0.11513460269641176 and parameters: {'lr': 0.0008037241817958571, 'num_epochs': 3}. Best is trial 3 with value: 0.11522962722463213.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.11523:  70%|███████   | 7/10 [02:34<01:05, 21.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 01:00:35,025] Trial 6 finished with value: 0.11511220541627182 and parameters: {'lr': 0.0002733015944364403, 'num_epochs': 3}. Best is trial 3 with value: 0.11522962722463213.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.11523:  80%|████████  | 8/10 [02:57<00:44, 22.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 01:00:57,433] Trial 7 finished with value: 0.11515818789949663 and parameters: {'lr': 0.0003414270092671607, 'num_epochs': 8}. Best is trial 3 with value: 0.11522962722463213.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.11523:  90%|█████████ | 9/10 [03:19<00:22, 22.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 01:01:19,924] Trial 8 finished with value: 0.11514077411491108 and parameters: {'lr': 0.00020208941733531894, 'num_epochs': 10}. Best is trial 3 with value: 0.11522962722463213.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.11523: 100%|██████████| 10/10 [03:41<00:00, 22.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 01:01:41,640] Trial 9 finished with value: 0.11508644752396731 and parameters: {'lr': 0.00010616194706309917, 'num_epochs': 5}. Best is trial 3 with value: 0.11522962722463213.\n"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17421145</td>\n",
       "      <td>0.21861102</td>\n",
       "      <td>0.18602298</td>\n",
       "      <td>0.17799735</td>\n",
       "      <td>0.18827766</td>\n",
       "      <td>0.21464792</td>\n",
       "      <td>0.55681423</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.47484491</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.17421366</td>\n",
       "      <td>0.21859848</td>\n",
       "      <td>0.18602487</td>\n",
       "      <td>0.17799862</td>\n",
       "      <td>0.18826666</td>\n",
       "      <td>0.21464062</td>\n",
       "      <td>0.55680630</td>\n",
       "      <td>0.00021777</td>\n",
       "      <td>0.47484230</td>\n",
       "      <td>0.00008877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.17434445</td>\n",
       "      <td>0.18574171</td>\n",
       "      <td>0.17524174</td>\n",
       "      <td>0.17857121</td>\n",
       "      <td>0.18071052</td>\n",
       "      <td>0.18645199</td>\n",
       "      <td>0.55774376</td>\n",
       "      <td>0.01835192</td>\n",
       "      <td>0.47454550</td>\n",
       "      <td>0.01017083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.17448628</td>\n",
       "      <td>0.18943531</td>\n",
       "      <td>0.18122277</td>\n",
       "      <td>0.18266715</td>\n",
       "      <td>0.18193198</td>\n",
       "      <td>0.17994498</td>\n",
       "      <td>0.55813659</td>\n",
       "      <td>0.03636377</td>\n",
       "      <td>0.47434188</td>\n",
       "      <td>0.02381384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0         0.17421145 0.21861102 0.18602298 0.17799735 0.18827766 0.21464792   \n",
       "100       0.17421366 0.21859848 0.18602487 0.17799862 0.18826666 0.21464062   \n",
       "1000      0.17434445 0.18574171 0.17524174 0.17857121 0.18071052 0.18645199   \n",
       "5000      0.17448628 0.18943531 0.18122277 0.18266715 0.18193198 0.17994498   \n",
       "\n",
       "      action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0              0.55681423    0.00000000            0.47484491     0.00000000  \n",
       "100            0.55680630    0.00021777            0.47484230     0.00008877  \n",
       "1000           0.55774376    0.01835192            0.47454550     0.01017083  \n",
       "5000           0.55813659    0.03636377            0.47434188     0.02381384  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.14805475</td>\n",
       "      <td>0.14740000</td>\n",
       "      <td>0.14741001</td>\n",
       "      <td>0.16662598</td>\n",
       "      <td>0.14740000</td>\n",
       "      <td>0.14740000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1.39203558</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1.18711227</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>0.14805615</td>\n",
       "      <td>0.14738587</td>\n",
       "      <td>0.14741001</td>\n",
       "      <td>0.16662598</td>\n",
       "      <td>0.14740833</td>\n",
       "      <td>0.14738587</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1.39245616</td>\n",
       "      <td>0.00427159</td>\n",
       "      <td>1.18787022</td>\n",
       "      <td>0.00409119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>0.14804445</td>\n",
       "      <td>0.14670924</td>\n",
       "      <td>0.14675499</td>\n",
       "      <td>0.33325195</td>\n",
       "      <td>0.14722087</td>\n",
       "      <td>0.14670924</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1.39336612</td>\n",
       "      <td>0.01355300</td>\n",
       "      <td>1.19154604</td>\n",
       "      <td>0.01465414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>0.14805309</td>\n",
       "      <td>0.14658780</td>\n",
       "      <td>0.14654124</td>\n",
       "      <td>0.16662598</td>\n",
       "      <td>0.14661765</td>\n",
       "      <td>0.14658780</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1.39277489</td>\n",
       "      <td>0.00881483</td>\n",
       "      <td>1.18945526</td>\n",
       "      <td>0.00949291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>0.14800507</td>\n",
       "      <td>0.14485716</td>\n",
       "      <td>0.14544778</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.14476698</td>\n",
       "      <td>0.14485716</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1.39511598</td>\n",
       "      <td>0.01295597</td>\n",
       "      <td>1.18752366</td>\n",
       "      <td>0.00677315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0          0.14805475 0.14740000 0.14741001 0.16662598 0.14740000 0.14740000   \n",
       "30000      0.14805615 0.14738587 0.14741001 0.16662598 0.14740833 0.14738587   \n",
       "60000      0.14804445 0.14670924 0.14675499 0.33325195 0.14722087 0.14670924   \n",
       "80000      0.14805309 0.14658780 0.14654124 0.16662598 0.14661765 0.14658780   \n",
       "90000      0.14800507 0.14485716 0.14544778 0.00000000 0.14476698 0.14485716   \n",
       "\n",
       "         ipw_var  reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  \\\n",
       "0     0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "30000 0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "60000 0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "80000 0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "90000 0.00000000  0.00000000   0.00000000   0.00000000     0.00000000   \n",
       "\n",
       "       action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0               1.39203558    0.00000000            1.18711227     0.00000000  \n",
       "30000           1.39245616    0.00427159            1.18787022     0.00409119  \n",
       "60000           1.39336612    0.01355300            1.19154604     0.01465414  \n",
       "80000           1.39277489    0.00881483            1.18945526     0.00949291  \n",
       "90000           1.39511598    0.01295597            1.18752366     0.00677315  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [3000, 6000, 8000, 9000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_trial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df5 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3500\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer_trial' is not defined"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
