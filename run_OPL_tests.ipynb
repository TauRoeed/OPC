{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# import gym\n",
    "# import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")  # TF32 = big speedup on Ada\n",
    "\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "# from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "    num_runs,\n",
    "    num_neighbors,\n",
    "    num_rounds_list,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    val_size=2000,\n",
    "    n_trials=10,    \n",
    "    prev_best_params=None\n",
    "):\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    def T(x):\n",
    "        return torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    best_hyperparams_by_size = {}\n",
    "    best_reward = -float('inf')\n",
    "    overall_best_params = {}\n",
    "\n",
    "    last_best_params = prev_best_params if prev_best_params is not None else None\n",
    "\n",
    "     # ---- Add baseline row for sample size = 0 ----\n",
    "    pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "    original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "    # Use a dummy simulation for baseline\n",
    "    simulation_data = create_simulation_data_from_pi(\n",
    "        dataset, pi_0, val_size, random_state=0\n",
    "    )\n",
    "\n",
    "    train_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size), our_x)\n",
    "    val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size), our_x)\n",
    "\n",
    "    regression_model = RegressionModel(\n",
    "        n_actions=n_actions, action_context=our_x,\n",
    "        base_model=LogisticRegression(random_state=12345)\n",
    "    )\n",
    "    regression_model.fit(train_data['x'], train_data['a'], train_data['r'])\n",
    "\n",
    "    neighberhoodmodel = NeighborhoodModel(\n",
    "        train_data['x_idx'], train_data['a'],\n",
    "        our_a, our_x, train_data['r'],\n",
    "        num_neighbors=num_neighbors\n",
    "    )\n",
    "    scores_all = torch.as_tensor(\n",
    "        neighberhoodmodel.predict(all_user_indices),\n",
    "        device=device, dtype=torch.float32\n",
    "    )\n",
    "    model = CFModel(\n",
    "        n_users, n_actions, emb_dim,\n",
    "        initial_user_embeddings=T(our_x),\n",
    "        initial_actions_embeddings=T(our_a)\n",
    "    ).to(device)\n",
    "\n",
    "    policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "    policy_reward = calc_reward(dataset, policy)\n",
    "    eval_metrics = eval_policy(neighberhoodmodel, val_data, original_policy_prob, policy)\n",
    "    action_diff_to_real = np.sqrt(np.mean((emb_a - our_a) ** 2))\n",
    "    action_delta = np.sqrt(np.mean((original_a - our_a) ** 2))\n",
    "    context_diff_to_real = np.sqrt(np.mean((emb_x - our_x) ** 2))\n",
    "    context_delta = np.sqrt(np.mean((original_x - our_x) ** 2))\n",
    "\n",
    "    row = np.concatenate([\n",
    "        np.atleast_1d(policy_reward),\n",
    "        np.atleast_1d(eval_metrics),\n",
    "        np.atleast_1d(action_diff_to_real),\n",
    "        np.atleast_1d(action_delta),\n",
    "        np.atleast_1d(context_diff_to_real),\n",
    "        np.atleast_1d(context_delta)\n",
    "    ])\n",
    "    reg_dm = dm.estimate_policy_value(policy[val_data['x_idx']], regression_model.predict(val_data['x']))\n",
    "    reg_results = np.array([reg_dm])\n",
    "    conv_results = np.array([row])\n",
    "    results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "\n",
    "    # ---- Main training size loop ----\n",
    "    for train_size in num_rounds_list:\n",
    "        # Generate initial data for Optuna search\n",
    "        pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "        original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "        simulation_data = create_simulation_data_from_pi(\n",
    "            dataset, pi_0, train_size + val_size,\n",
    "            random_state=train_size\n",
    "        )\n",
    "        idx = np.arange(train_size)\n",
    "        train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "        num_workers = 4 if torch.cuda.is_available() else 0\n",
    "        cf_dataset = CustomCFDataset(\n",
    "            train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "        )\n",
    "\n",
    "        # Define Optuna objective inside the loop so it can access train_data and cf_dataset\n",
    "        def objective(trial):\n",
    "            lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "            epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "            trial_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "            trial_num_neighbors = trial.suggest_int(\"num_neighbors\", 3, 15)\n",
    "            lr_decay = trial.suggest_float(\"lr_decay\", 0.8, 1.0)\n",
    "\n",
    "            trial_neigh_model = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=trial_num_neighbors\n",
    "            )\n",
    "            trial_scores_all = torch.as_tensor(\n",
    "                trial_neigh_model.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "            trial_model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "            assert (not torch.cuda.is_available()) or next(trial_model.parameters()).is_cuda\n",
    "\n",
    "            final_train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=trial_batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            current_lr = lr\n",
    "            for epoch in range(epochs):\n",
    "                if epoch > 0:\n",
    "                    current_lr *= lr_decay\n",
    "                train(\n",
    "                    trial_model, final_train_loader, trial_neigh_model, trial_scores_all,\n",
    "                    criterion=SNDRPolicyLoss(), num_epochs=1, lr=current_lr, device=str(device)\n",
    "                )\n",
    "\n",
    "            trial_x_t, trial_a_t = trial_model.get_params()\n",
    "            trial_x = trial_x_t.detach().cpu().numpy()\n",
    "            trial_a = trial_a_t.detach().cpu().numpy()\n",
    "            trial_policy = np.expand_dims(softmax(trial_x @ trial_a.T, axis=1), -1)\n",
    "            trial_policy_reward = calc_reward(dataset, trial_policy)\n",
    "            return trial_policy_reward\n",
    "\n",
    "        # ---- Hyperparam search ----\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        if last_best_params is not None:\n",
    "            study.enqueue_trial(last_best_params)\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_reward_for_size = study.best_value\n",
    "        best_hyperparams_by_size[train_size] = {\n",
    "            \"params\": best_params,\n",
    "            \"reward\": best_reward_for_size\n",
    "        }\n",
    "        last_best_params = best_params\n",
    "\n",
    "        # ---- Final evaluation loop ----\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                dataset, pi_0, train_size + val_size,\n",
    "                random_state=(run + 1) * train_size\n",
    "            )\n",
    "            idx = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            regression_model = RegressionModel(\n",
    "                n_actions=n_actions, action_context=our_x,\n",
    "                base_model=LogisticRegression(random_state=12345)\n",
    "            )\n",
    "            regression_model.fit(\n",
    "                train_data['x'], train_data['a'], train_data['r'],\n",
    "                original_policy_prob[train_data['x_idx'], train_data['a']].squeeze()\n",
    "            )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=best_params['num_neighbors']\n",
    "            )\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "            model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "            assert (not torch.cuda.is_available()) or next(model.parameters()).is_cuda\n",
    "\n",
    "            cf_dataset = CustomCFDataset(\n",
    "                train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "            )\n",
    "            train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            val_dataset = CustomCFDataset(\n",
    "                val_data['x_idx'], val_data['a'], val_data['r'], original_policy_prob\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset, batch_size=val_size, shuffle=False,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            current_lr = best_params['lr']\n",
    "            for epoch in range(best_params['num_epochs']):\n",
    "                if epoch > 0:\n",
    "                    current_lr *= best_params['lr_decay']\n",
    "                train(\n",
    "                    model, train_loader, neighberhoodmodel, scores_all,\n",
    "                    criterion=SNDRPolicyLoss(),\n",
    "                    num_epochs=1, lr=current_lr,\n",
    "                    device=str(device)\n",
    "                )\n",
    "\n",
    "            our_x_t, our_a_t = model.get_params()\n",
    "            our_a, our_x = our_a_t.detach().cpu().numpy(), our_x_t.detach().cpu().numpy()\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "            policy_reward = calc_reward(dataset, policy)\n",
    "            eval_metrics = eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy)\n",
    "            action_diff_to_real = np.sqrt(np.mean((emb_a - our_a) ** 2))\n",
    "            action_delta = np.sqrt(np.mean((original_a - our_a) ** 2))\n",
    "            context_diff_to_real = np.sqrt(np.mean((emb_x - our_x) ** 2))\n",
    "            context_delta = np.sqrt(np.mean((original_x - our_x) ** 2))\n",
    "\n",
    "            row = np.concatenate([\n",
    "                np.atleast_1d(policy_reward),\n",
    "                np.atleast_1d(eval_metrics),\n",
    "                np.atleast_1d(action_diff_to_real),\n",
    "                np.atleast_1d(action_delta),\n",
    "                np.atleast_1d(context_diff_to_real),\n",
    "                np.atleast_1d(context_delta)\n",
    "            ])\n",
    "            conv_results.append(row)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index'), best_hyperparams_by_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Item CTR: 0.12972795060603162\n",
      "Optimal greedy CTR: 0.19999707792821972\n",
      "Optimal Stochastic CTR: 0.19982996880994605\n",
      "Our Initial CTR: 0.1646085673501415\n"
     ]
    }
   ],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.2\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emb_a', 'our_a', 'original_a', 'emb_x', 'our_x', 'original_x', 'q_x_a', 'n_actions', 'n_users', 'emb_dim', 'user_prior'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 2\n",
    "batch_size = 200\n",
    "num_neighbors = 6\n",
    "n_trials_for_optuna = 20\n",
    "num_rounds_list = [500, 1000] #, 5000, 10000]\n",
    "\n",
    "# Manually define your best parameters\n",
    "best_params_to_use = {\n",
    "    \"lr\": 0.002,  # Learning rate\n",
    "    \"num_epochs\": 5,  # Number of training epochs\n",
    "    \"batch_size\": 256,  # Batch size for training\n",
    "    \"num_neighbors\": 8,  # Number of neighbors for neighborhood model\n",
    "    \"lr_decay\": 0.9  # Learning rate decay factor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of num_rounds_list: [500, 1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:02:56,760] A new study created in memory with name: no-name-1da3e89c-e2c1-46d2-bd0f-c0bb83fb27ee\n",
      "Best trial: 0. Best value: 0.164715:   5%|▌         | 1/20 [00:03<00:58,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:02:59,829] Trial 0 finished with value: 0.1647152159909 and parameters: {'lr': 0.002, 'num_epochs': 5, 'batch_size': 256, 'num_neighbors': 8, 'lr_decay': 0.9}. Best is trial 0 with value: 0.1647152159909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.164715:  10%|█         | 2/20 [00:05<00:49,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:02,362] Trial 1 finished with value: 0.1646274224154382 and parameters: {'lr': 0.00020811128190312416, 'num_epochs': 8, 'batch_size': 256, 'num_neighbors': 13, 'lr_decay': 0.9627563310517935}. Best is trial 0 with value: 0.1647152159909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.164715:  15%|█▌        | 3/20 [00:08<00:45,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:04,882] Trial 2 finished with value: 0.1646327034931041 and parameters: {'lr': 0.00029610228969303504, 'num_epochs': 4, 'batch_size': 128, 'num_neighbors': 9, 'lr_decay': 0.8831152056434768}. Best is trial 0 with value: 0.1647152159909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.164715:  20%|██        | 4/20 [00:11<00:49,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:08,675] Trial 3 finished with value: 0.16464814359028 and parameters: {'lr': 0.0007364484774462089, 'num_epochs': 7, 'batch_size': 256, 'num_neighbors': 7, 'lr_decay': 0.8339882051168946}. Best is trial 0 with value: 0.1647152159909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  25%|██▌       | 5/20 [00:14<00:44,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:11,323] Trial 4 finished with value: 0.165308163919195 and parameters: {'lr': 0.0031687774077433523, 'num_epochs': 10, 'batch_size': 128, 'num_neighbors': 9, 'lr_decay': 0.9117642707313017}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  30%|███       | 6/20 [00:17<00:38,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:13,762] Trial 5 finished with value: 0.16461001229095254 and parameters: {'lr': 0.00012743651129106193, 'num_epochs': 1, 'batch_size': 256, 'num_neighbors': 4, 'lr_decay': 0.9725071263587736}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  35%|███▌      | 7/20 [00:19<00:35,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:16,364] Trial 6 finished with value: 0.16471065737444696 and parameters: {'lr': 0.0004096385404112414, 'num_epochs': 5, 'batch_size': 64, 'num_neighbors': 14, 'lr_decay': 0.9836749695792514}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  40%|████      | 8/20 [00:22<00:31,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:18,821] Trial 7 finished with value: 0.16461108213084957 and parameters: {'lr': 0.000250538546829754, 'num_epochs': 1, 'batch_size': 256, 'num_neighbors': 6, 'lr_decay': 0.8500475108143213}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  45%|████▌     | 9/20 [00:24<00:28,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:21,368] Trial 8 finished with value: 0.16495179063307494 and parameters: {'lr': 0.005088326254713456, 'num_epochs': 5, 'batch_size': 256, 'num_neighbors': 13, 'lr_decay': 0.9284784722078744}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  50%|█████     | 10/20 [00:27<00:26,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:23,961] Trial 9 finished with value: 0.1646373785783598 and parameters: {'lr': 0.00024026327992702996, 'num_epochs': 6, 'batch_size': 128, 'num_neighbors': 15, 'lr_decay': 0.9470860644865813}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  55%|█████▌    | 11/20 [00:29<00:23,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:26,573] Trial 10 finished with value: 0.1651151790586645 and parameters: {'lr': 0.00865131694973534, 'num_epochs': 10, 'batch_size': 512, 'num_neighbors': 11, 'lr_decay': 0.8658024592703468}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  60%|██████    | 12/20 [00:32<00:20,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:29,144] Trial 11 finished with value: 0.1652399428074737 and parameters: {'lr': 0.009851900546672098, 'num_epochs': 10, 'batch_size': 512, 'num_neighbors': 10, 'lr_decay': 0.8754700661724174}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.165308:  65%|██████▌   | 13/20 [00:35<00:18,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:31,760] Trial 12 finished with value: 0.1647663458001018 and parameters: {'lr': 0.002617456646444264, 'num_epochs': 10, 'batch_size': 512, 'num_neighbors': 11, 'lr_decay': 0.9266703022961029}. Best is trial 4 with value: 0.165308163919195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.166556:  70%|███████   | 14/20 [00:37<00:15,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:34,380] Trial 13 finished with value: 0.1665564714492538 and parameters: {'lr': 0.009681918372888688, 'num_epochs': 9, 'batch_size': 128, 'num_neighbors': 11, 'lr_decay': 0.8124880219948764}. Best is trial 13 with value: 0.1665564714492538.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.166556:  75%|███████▌  | 15/20 [00:40<00:12,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:36,946] Trial 14 finished with value: 0.1649904197483568 and parameters: {'lr': 0.0031026426662942685, 'num_epochs': 8, 'batch_size': 128, 'num_neighbors': 5, 'lr_decay': 0.8067089282094644}. Best is trial 13 with value: 0.1665564714492538.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.166556:  80%|████████  | 16/20 [00:43<00:11,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:40,741] Trial 15 finished with value: 0.16473918306170815 and parameters: {'lr': 0.0011880159870345914, 'num_epochs': 8, 'batch_size': 128, 'num_neighbors': 11, 'lr_decay': 0.8102490115202625}. Best is trial 13 with value: 0.1665564714492538.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.166556:  85%|████████▌ | 17/20 [00:46<00:08,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:43,348] Trial 16 finished with value: 0.1657259297041133 and parameters: {'lr': 0.004884096835657089, 'num_epochs': 9, 'batch_size': 128, 'num_neighbors': 3, 'lr_decay': 0.9098780431710485}. Best is trial 13 with value: 0.1665564714492538.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.166556:  90%|█████████ | 18/20 [00:49<00:05,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:46,024] Trial 17 finished with value: 0.16635804513835278 and parameters: {'lr': 0.005259336695703555, 'num_epochs': 9, 'batch_size': 64, 'num_neighbors': 3, 'lr_decay': 0.8347363521961699}. Best is trial 13 with value: 0.1665564714492538.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.166556:  95%|█████████▌| 19/20 [00:51<00:02,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:48,644] Trial 18 finished with value: 0.16623014300943686 and parameters: {'lr': 0.005485770914951094, 'num_epochs': 7, 'batch_size': 64, 'num_neighbors': 3, 'lr_decay': 0.8315209582655346}. Best is trial 13 with value: 0.1665564714492538.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.166556: 100%|██████████| 20/20 [00:54<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:51,222] Trial 19 finished with value: 0.16476246437476835 and parameters: {'lr': 0.001303473762292282, 'num_epochs': 3, 'batch_size': 64, 'num_neighbors': 7, 'lr_decay': 0.8284944671309039}. Best is trial 13 with value: 0.1665564714492538.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:55,335] A new study created in memory with name: no-name-136fcfba-f7b6-490e-a34b-0f8e7904b33d\n",
      "Best trial: 0. Best value: 0.169047:   5%|▌         | 1/20 [00:03<00:57,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:03:58,347] Trial 0 finished with value: 0.16904677940041388 and parameters: {'lr': 0.009681918372888688, 'num_epochs': 9, 'batch_size': 128, 'num_neighbors': 11, 'lr_decay': 0.8124880219948764}. Best is trial 0 with value: 0.16904677940041388.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.169047:  10%|█         | 2/20 [00:05<00:53,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:01,260] Trial 1 finished with value: 0.1676534878443323 and parameters: {'lr': 0.009773055353996451, 'num_epochs': 10, 'batch_size': 512, 'num_neighbors': 7, 'lr_decay': 0.9353732431489278}. Best is trial 0 with value: 0.16904677940041388.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.169047:  15%|█▌        | 3/20 [00:08<00:49,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:04,113] Trial 2 finished with value: 0.16717954127352563 and parameters: {'lr': 0.004351365553044151, 'num_epochs': 4, 'batch_size': 128, 'num_neighbors': 8, 'lr_decay': 0.9630207755040227}. Best is trial 0 with value: 0.16904677940041388.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.169047:  20%|██        | 4/20 [00:11<00:46,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:07,034] Trial 3 finished with value: 0.16690412333676105 and parameters: {'lr': 0.0013106535343548199, 'num_epochs': 8, 'batch_size': 256, 'num_neighbors': 10, 'lr_decay': 0.8322305737508512}. Best is trial 0 with value: 0.16904677940041388.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.169047:  25%|██▌       | 5/20 [00:15<00:50,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:11,152] Trial 4 finished with value: 0.16687322905444915 and parameters: {'lr': 0.0009669480244530776, 'num_epochs': 9, 'batch_size': 512, 'num_neighbors': 15, 'lr_decay': 0.8433822327164343}. Best is trial 0 with value: 0.16904677940041388.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.17054:  30%|███       | 6/20 [00:18<00:45,  3.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:14,228] Trial 5 finished with value: 0.17054006579236203 and parameters: {'lr': 0.0072870095422992685, 'num_epochs': 9, 'batch_size': 128, 'num_neighbors': 9, 'lr_decay': 0.9650591972062041}. Best is trial 5 with value: 0.17054006579236203.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  35%|███▌      | 7/20 [00:22<00:41,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:17,361] Trial 6 finished with value: 0.17144690158546425 and parameters: {'lr': 0.007213385884565424, 'num_epochs': 9, 'batch_size': 64, 'num_neighbors': 5, 'lr_decay': 0.8757335518577117}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  40%|████      | 8/20 [00:25<00:37,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:20,379] Trial 7 finished with value: 0.16688617940939598 and parameters: {'lr': 0.00013438881151394257, 'num_epochs': 9, 'batch_size': 128, 'num_neighbors': 15, 'lr_decay': 0.9698792007067164}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  45%|████▌     | 9/20 [00:27<00:33,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:23,207] Trial 8 finished with value: 0.16687713466009757 and parameters: {'lr': 0.0013827241709540767, 'num_epochs': 7, 'batch_size': 512, 'num_neighbors': 7, 'lr_decay': 0.8222591203643104}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  50%|█████     | 10/20 [00:30<00:29,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:26,013] Trial 9 finished with value: 0.16689054543713155 and parameters: {'lr': 0.00255056035290449, 'num_epochs': 6, 'batch_size': 512, 'num_neighbors': 8, 'lr_decay': 0.8435150940324251}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  55%|█████▌    | 11/20 [00:33<00:26,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:28,753] Trial 10 finished with value: 0.16688398815341146 and parameters: {'lr': 0.00033684774860401665, 'num_epochs': 1, 'batch_size': 64, 'num_neighbors': 3, 'lr_decay': 0.8881159495586852}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  60%|██████    | 12/20 [00:36<00:23,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:31,597] Trial 11 finished with value: 0.16747801062221057 and parameters: {'lr': 0.004357321801325476, 'num_epochs': 3, 'batch_size': 64, 'num_neighbors': 4, 'lr_decay': 0.8983465908981283}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  65%|██████▌   | 13/20 [00:39<00:20,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:34,651] Trial 12 finished with value: 0.16892621406211028 and parameters: {'lr': 0.004318475711603302, 'num_epochs': 7, 'batch_size': 64, 'num_neighbors': 5, 'lr_decay': 0.9322575580774413}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  70%|███████   | 14/20 [00:42<00:17,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:37,608] Trial 13 finished with value: 0.16829960348887513 and parameters: {'lr': 0.009998338171459962, 'num_epochs': 10, 'batch_size': 256, 'num_neighbors': 12, 'lr_decay': 0.8724327398080602}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  75%|███████▌  | 15/20 [00:45<00:14,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:40,494] Trial 14 finished with value: 0.16691242693163796 and parameters: {'lr': 0.00046924508013633314, 'num_epochs': 5, 'batch_size': 128, 'num_neighbors': 5, 'lr_decay': 0.987133488227082}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  80%|████████  | 16/20 [00:49<00:13,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:44,810] Trial 15 finished with value: 0.16754333069779678 and parameters: {'lr': 0.002493489030982789, 'num_epochs': 7, 'batch_size': 64, 'num_neighbors': 13, 'lr_decay': 0.9291465988503248}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  85%|████████▌ | 17/20 [00:52<00:09,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:47,829] Trial 16 finished with value: 0.16797458719844147 and parameters: {'lr': 0.0058813229741217395, 'num_epochs': 8, 'batch_size': 128, 'num_neighbors': 6, 'lr_decay': 0.8688608044705155}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  90%|█████████ | 18/20 [00:55<00:06,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:51,029] Trial 17 finished with value: 0.16788977993947404 and parameters: {'lr': 0.0022847416882829956, 'num_epochs': 10, 'batch_size': 64, 'num_neighbors': 9, 'lr_decay': 0.9206237369199889}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447:  95%|█████████▌| 19/20 [00:58<00:03,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:53,909] Trial 18 finished with value: 0.1668890853127018 and parameters: {'lr': 0.0006270253671676676, 'num_epochs': 5, 'batch_size': 256, 'num_neighbors': 10, 'lr_decay': 0.9569099411109969}. Best is trial 6 with value: 0.17144690158546425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.171447: 100%|██████████| 20/20 [01:01<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 00:04:56,874] Trial 19 finished with value: 0.16688721176002486 and parameters: {'lr': 0.00016823385111369444, 'num_epochs': 8, 'batch_size': 128, 'num_neighbors': 4, 'lr_decay': 0.9948149158035802}. Best is trial 6 with value: 0.17144690158546425.\n",
      "\n",
      "=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\n",
      "\n",
      "Training Size: 500\n",
      "Best Reward: 0.166556\n",
      "Parameters:\n",
      "  lr: 0.009681918372888688\n",
      "  num_epochs: 9\n",
      "  batch_size: 128\n",
      "  num_neighbors: 11\n",
      "  lr_decay: 0.8124880219948764\n",
      "\n",
      "Training Size: 1000\n",
      "Best Reward: 0.171447\n",
      "Parameters:\n",
      "  lr: 0.007213385884565424\n",
      "  num_epochs: 9\n",
      "  batch_size: 64\n",
      "  num_neighbors: 5\n",
      "  lr_decay: 0.8757335518577117\n",
      "===========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16460857</td>\n",
       "      <td>0.18648377</td>\n",
       "      <td>0.17616646</td>\n",
       "      <td>0.18441898</td>\n",
       "      <td>0.18191963</td>\n",
       "      <td>0.17284784</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.16634903</td>\n",
       "      <td>0.20744115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17381334</td>\n",
       "      <td>0.17509877</td>\n",
       "      <td>0.18550551</td>\n",
       "      <td>0.76442025</td>\n",
       "      <td>0.13951846</td>\n",
       "      <td>0.87814654</td>\n",
       "      <td>0.05867832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.16852269</td>\n",
       "      <td>0.20308214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.16935521</td>\n",
       "      <td>0.17195164</td>\n",
       "      <td>0.18671154</td>\n",
       "      <td>0.79439417</td>\n",
       "      <td>0.30102437</td>\n",
       "      <td>0.88687834</td>\n",
       "      <td>0.10292054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0         0.16460857 0.18648377 0.17616646 0.18441898 0.18191963 0.17284784   \n",
       "500       0.16634903 0.20744115        NaN 0.17381334 0.17509877 0.18550551   \n",
       "1000      0.16852269 0.20308214        NaN 0.16935521 0.17195164 0.18671154   \n",
       "\n",
       "      action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0              0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "500            0.76442025    0.13951846            0.87814654     0.05867832  \n",
       "1000           0.79439417    0.30102437            0.88687834     0.10292054  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Value of num_rounds_list:\", num_rounds_list)\n",
    "\n",
    "# Run the optimization\n",
    "df4, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=2000, n_trials=n_trials_for_optuna,prev_best_params=best_params_to_use)\n",
    "\n",
    "# Print best hyperparameters for each training size\n",
    "print(\"\\n=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\")\n",
    "for train_size, params in best_hyperparams_by_size.items():\n",
    "    print(f\"\\nTraining Size: {train_size}\")\n",
    "    print(f\"Best Reward: {params['reward']:.6f}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, value in params['params'].items():\n",
    "        print(f\"  {param_name}: {value}\")\n",
    "print(\"===========================\\n\")\n",
    "\n",
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OBPEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
