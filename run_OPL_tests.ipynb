{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import gym\n",
    "import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  val_size=2000\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            dataset['env'],\n",
    "                                                            pi_0,\n",
    "                                                            train_size + val_size\n",
    "                                                            )\n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "\n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            regression_model = RegressionModel(\n",
    "                                    n_actions=n_actions,\n",
    "                                    action_context=our_x,\n",
    "                                    base_model=LogisticRegression(random_state=12345)\n",
    "                                    )\n",
    "            \n",
    "            regression_model.fit(\n",
    "                                train_data['x'], \n",
    "                                train_data['a'],\n",
    "                                train_data['r'],\n",
    "                                original_policy_prob[train_data['x_idx'],\n",
    "                                train_data['a']].squeeze()\n",
    "                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "                \n",
    "            # Bloss = BPRLoss()\n",
    "            # bpr_model = BPRModel(\n",
    "            #                     n_users,\n",
    "            #                     n_actions,\n",
    "            #                     emb_x.shape[1], \n",
    "            #                     initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "            #                     initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "            #                     )\n",
    "\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=10)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            # fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 300,\n",
    "                    n_users = 300,\n",
    "                    emb_dim = 7,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.2 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 500\n",
    "num_neighbors = 51\n",
    "num_rounds_list = [10000, 20000, 30000, 40000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /tmp/ipykernel_131/2755471202.py\n",
      "Filename: /code/simulation_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   136    407.7 MiB    407.7 MiB           1   @profile    \n",
      "   137                                         def create_simulation_data_from_pi(env, policy, n):\n",
      "   138                                             \"\"\"\n",
      "   139                                             Samples one action per user from the given policy.\n",
      "   140                                             \n",
      "   141                                             Args:\n",
      "   142                                                 env: The RecoGym environment (from generate_dataset)\n",
      "   143                                                 policy: np.ndarray of shape (n_users, n_actions) – probability distributions over actions for each user\n",
      "   144                                                 \n",
      "   145                                             Returns:\n",
      "   146                                                 actions: np.ndarray of shape (n_users,) – sampled actions\n",
      "   147                                                 rewards: np.ndarray of shape (n_users,) – obtained rewards\n",
      "   148                                                 pscore: np.ndarray of shape (n_users,) – policy probability of the sampled action\n",
      "   149                                             \"\"\"\n",
      "   150    407.7 MiB      0.0 MiB           1       n_users, n_actions = policy.shape\n",
      "   151    407.7 MiB      0.0 MiB           1       users = np.random.randint(0, n_users, size=n)\n",
      "   152                                             \n",
      "   153    407.7 MiB      0.0 MiB           1       actions = np.zeros(n, dtype=int)\n",
      "   154    407.7 MiB      0.0 MiB           1       rewards = np.zeros(n, dtype=float)\n",
      "   155    407.7 MiB      0.0 MiB           1       pscore = np.zeros(n, dtype=float)\n",
      "   156                                         \n",
      "   157    407.7 MiB      0.0 MiB           1       agent = PolicyAgent(env.config, policy)\n",
      "   158    407.7 MiB      0.0 MiB           1       env.agent = agent\n",
      "   159    407.7 MiB      0.0 MiB           1       obs, reward, done, info = None, None, False, {}\n",
      "   160    407.7 MiB      0.0 MiB           1       env.reset(user_id=0)\n",
      "   161                                         \n",
      "   162    447.6 MiB      0.0 MiB        2001       for i, user_id in enumerate(users):\n",
      "   163                                                 \n",
      "   164    447.6 MiB      0.0 MiB        2000           env.reset(user_id=user_id)\n",
      "   165    447.6 MiB      0.0 MiB        2000           obs, reward, done = None, None, False\n",
      "   166                                         \n",
      "   167                                                 # keep stepping until you get an action\n",
      "   168    447.6 MiB      0.0 MiB        2000           action = None\n",
      "   169    447.6 MiB      0.0 MiB        6080           while not done and action is None:\n",
      "   170    447.6 MiB     39.9 MiB        4080               action, obs, reward, done, info = env.step_offline(obs, reward, done)\n",
      "   171    447.6 MiB      0.0 MiB        4080               if done and action is None:\n",
      "   172    447.6 MiB      0.0 MiB          80                   env.reset(user_id=user_id)\n",
      "   173    447.6 MiB      0.0 MiB          80                   obs, reward, done = None, None, False\n",
      "   174                                                         \n",
      "   175                                                 # Step the RecoGym env with this user-action pair\n",
      "   176    447.6 MiB      0.0 MiB        2000           actions[i] = action['a']\n",
      "   177    447.6 MiB      0.0 MiB        2000           rewards[i] = reward\n",
      "   178    447.6 MiB      0.0 MiB        2000           pscore[i] = action['ps']\n",
      "   179                                         \n",
      "   180    447.6 MiB      0.0 MiB           1       return dict(users=users, actions=actions, rewards=rewards, pscore=pscore, pi_0=policy)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    452.5 MiB    452.5 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    452.5 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    452.5 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    452.5 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    459.5 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    459.5 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    459.5 MiB      5.1 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    459.5 MiB      1.4 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    459.5 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    459.5 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    459.5 MiB      0.5 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    459.5 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    459.5 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    459.5 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    459.5 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    459.5 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    450.3 MiB    450.3 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    451.1 MiB      0.8 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    451.7 MiB      0.6 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    451.7 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    452.5 MiB      0.8 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    452.8 MiB      0.3 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    452.8 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    452.8 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n",
      "Filename: /code/simulation_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   136    458.3 MiB    458.3 MiB           1   @profile    \n",
      "   137                                         def create_simulation_data_from_pi(env, policy, n):\n",
      "   138                                             \"\"\"\n",
      "   139                                             Samples one action per user from the given policy.\n",
      "   140                                             \n",
      "   141                                             Args:\n",
      "   142                                                 env: The RecoGym environment (from generate_dataset)\n",
      "   143                                                 policy: np.ndarray of shape (n_users, n_actions) – probability distributions over actions for each user\n",
      "   144                                                 \n",
      "   145                                             Returns:\n",
      "   146                                                 actions: np.ndarray of shape (n_users,) – sampled actions\n",
      "   147                                                 rewards: np.ndarray of shape (n_users,) – obtained rewards\n",
      "   148                                                 pscore: np.ndarray of shape (n_users,) – policy probability of the sampled action\n",
      "   149                                             \"\"\"\n",
      "   150    458.3 MiB      0.0 MiB           1       n_users, n_actions = policy.shape\n",
      "   151    458.3 MiB      0.0 MiB           1       users = np.random.randint(0, n_users, size=n)\n",
      "   152                                             \n",
      "   153    458.3 MiB      0.0 MiB           1       actions = np.zeros(n, dtype=int)\n",
      "   154    458.3 MiB      0.0 MiB           1       rewards = np.zeros(n, dtype=float)\n",
      "   155    458.4 MiB      0.1 MiB           1       pscore = np.zeros(n, dtype=float)\n",
      "   156                                         \n",
      "   157    458.4 MiB      0.0 MiB           1       agent = PolicyAgent(env.config, policy)\n",
      "   158    458.4 MiB      0.0 MiB           1       env.agent = agent\n",
      "   159    458.4 MiB      0.0 MiB           1       obs, reward, done, info = None, None, False, {}\n",
      "   160    458.4 MiB      0.0 MiB           1       env.reset(user_id=0)\n",
      "   161                                         \n",
      "   162    464.9 MiB      0.0 MiB      300001       for i, user_id in enumerate(users):\n",
      "   163                                                 \n",
      "   164    464.9 MiB      0.0 MiB      300000           env.reset(user_id=user_id)\n",
      "   165    464.9 MiB      0.0 MiB      300000           obs, reward, done = None, None, False\n",
      "   166                                         \n",
      "   167                                                 # keep stepping until you get an action\n",
      "   168    464.9 MiB      0.0 MiB      300000           action = None\n",
      "   169    464.9 MiB      0.0 MiB      911846           while not done and action is None:\n",
      "   170    464.9 MiB      0.0 MiB      611846               action, obs, reward, done, info = env.step_offline(obs, reward, done)\n",
      "   171    464.9 MiB      0.0 MiB      611846               if done and action is None:\n",
      "   172    464.9 MiB      0.0 MiB       11846                   env.reset(user_id=user_id)\n",
      "   173    464.9 MiB      0.0 MiB       11846                   obs, reward, done = None, None, False\n",
      "   174                                                         \n",
      "   175                                                 # Step the RecoGym env with this user-action pair\n",
      "   176    464.9 MiB      2.8 MiB      300000           actions[i] = action['a']\n",
      "   177    464.9 MiB      1.9 MiB      300000           rewards[i] = reward\n",
      "   178    464.9 MiB      1.9 MiB      300000           pscore[i] = action['ps']\n",
      "   179                                         \n",
      "   180    464.9 MiB      0.0 MiB           1       return dict(users=users, actions=actions, rewards=rewards, pscore=pscore, pi_0=policy)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:58:36,628] A new study created in memory with name: no-name-98307af3-7474-4462-a379-97d6aef79d56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    460.6 MiB    460.6 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    460.6 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    460.6 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    460.6 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    465.4 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    465.4 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    465.4 MiB      3.2 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    465.4 MiB      1.2 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    465.4 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    465.4 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    465.4 MiB      0.4 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    465.4 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    465.4 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    465.4 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    465.4 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    465.4 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    460.6 MiB    460.6 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    460.6 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    460.6 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    460.6 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    460.6 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    458.7 MiB     -1.9 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    458.7 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    458.7 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:58:40,477] Trial 0 finished with value: -0.0009334901986337435 and parameters: {'lr': 0.07310256561283052, 'num_epochs': 5}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    481.8 MiB    481.8 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    481.8 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    481.8 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    481.8 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    491.0 MiB      0.1 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    481.9 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    486.4 MiB      4.5 MiB           1           policy = model(user_idx)\n",
      "   112    486.4 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    486.4 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    491.0 MiB      4.7 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    486.4 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    491.0 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    491.0 MiB    491.0 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    491.0 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    491.0 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    491.0 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    491.0 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    491.0 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    491.0 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    491.0 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    491.0 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    491.0 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    491.0 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    491.0 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    491.0 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    491.0 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    491.0 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    491.0 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    491.0 MiB    491.0 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    491.0 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    491.0 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    491.0 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    491.0 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    491.0 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    491.0 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    491.0 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:58:44,331] Trial 1 finished with value: 0.0032334163336903386 and parameters: {'lr': 0.0005167646042881456, 'num_epochs': 9}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    491.0 MiB    491.0 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    491.0 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    491.0 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    491.0 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    496.4 MiB      0.0 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    491.0 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    491.0 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    491.0 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    491.0 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    496.4 MiB      5.4 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    491.0 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    496.4 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    496.4 MiB    496.4 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    496.4 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    496.4 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    496.4 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    496.4 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    496.4 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    496.4 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    496.4 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    496.4 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    496.4 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    496.4 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    496.4 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    496.4 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    496.4 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    496.4 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    496.4 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    496.4 MiB    496.4 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    496.4 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    496.4 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    496.4 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    496.4 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    496.4 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    496.4 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    496.4 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:58:48,236] Trial 2 finished with value: 0.0032753629252436738 and parameters: {'lr': 0.0039219485436538986, 'num_epochs': 10}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    496.4 MiB    496.4 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    496.4 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    496.4 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    496.4 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    498.8 MiB      0.1 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    496.5 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    496.5 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    496.5 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    496.5 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    498.8 MiB      2.2 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    496.5 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    498.8 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:58:51,952] Trial 3 finished with value: 0.003364688971211522 and parameters: {'lr': 0.02040974789348627, 'num_epochs': 3}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    498.8 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    498.8 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    498.8 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    498.8 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    498.8 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    498.8 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    498.8 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    498.8 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    498.8 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    498.8 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    498.8 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    498.8 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    498.8 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    498.8 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    498.8 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    498.8 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    498.8 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    498.8 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    498.8 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    498.8 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    498.8 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n",
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    498.8 MiB    498.8 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    498.8 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    498.8 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    498.8 MiB      0.0 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    498.8 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    498.8 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    498.8 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    498.8 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    498.8 MiB      0.0 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    498.8 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    498.8 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:58:55,552] Trial 4 finished with value: 0.0032248838992258583 and parameters: {'lr': 0.0012278859072171403, 'num_epochs': 2}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    498.8 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    498.8 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    498.8 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    498.8 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    498.8 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    498.8 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    498.8 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    498.8 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    498.8 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    498.8 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    498.8 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    498.8 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    498.8 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    498.8 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    498.8 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    498.8 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    498.8 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    498.8 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    498.8 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    498.8 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    498.8 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n",
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    498.8 MiB    498.8 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    498.8 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    498.8 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    498.8 MiB      0.0 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    498.8 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    498.8 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    498.8 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    498.8 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    498.8 MiB      0.0 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    498.8 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    498.8 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:58:59,203] Trial 5 finished with value: 0.0032282004567382207 and parameters: {'lr': 0.06253885174007812, 'num_epochs': 3}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    498.8 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    498.8 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    498.8 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    498.8 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    498.8 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    498.8 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    498.8 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    498.8 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    498.8 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    498.8 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    498.8 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    498.8 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    498.8 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    498.8 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    498.8 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    498.8 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    498.8 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    498.8 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    498.8 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    498.8 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    498.8 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n",
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    498.8 MiB    498.8 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    498.8 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    498.8 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    498.8 MiB      0.0 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    498.8 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    498.8 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    498.8 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    498.8 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    498.8 MiB      0.0 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    498.8 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    498.8 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:59:02,834] Trial 6 finished with value: 0.003226208706936122 and parameters: {'lr': 0.00041673456213547764, 'num_epochs': 3}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    498.8 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    498.8 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    498.8 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    498.8 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    498.8 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    498.8 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    498.8 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    498.8 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    498.8 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    498.8 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    498.8 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    498.8 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    498.8 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    498.8 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    498.8 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    498.8 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    498.8 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    498.8 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    498.8 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    498.8 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    498.8 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n",
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    498.8 MiB    498.8 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    498.8 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    498.8 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    498.8 MiB      0.0 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    498.8 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    498.8 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    498.8 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    498.8 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    498.8 MiB      0.0 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    498.8 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    498.8 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    498.8 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    498.8 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    498.8 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    498.8 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    498.8 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    498.8 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    498.8 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    498.8 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    498.8 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    498.8 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    498.8 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    498.8 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    498.8 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    498.8 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    498.8 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    498.8 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    498.8 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    498.8 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    498.8 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    498.8 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    498.8 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:59:06,552] Trial 7 finished with value: 0.0021664443740100313 and parameters: {'lr': 0.059615197614511777, 'num_epochs': 5}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    498.8 MiB    498.8 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    498.8 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    498.8 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    498.8 MiB      0.0 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    498.8 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    498.8 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    498.8 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    498.8 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    498.8 MiB      0.0 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    498.8 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    498.8 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:59:10,221] Trial 8 finished with value: 0.002986294451445104 and parameters: {'lr': 0.035324165421166336, 'num_epochs': 4}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    498.8 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    498.8 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    498.8 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    498.8 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    498.8 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    498.8 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    498.8 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    498.8 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    498.8 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    498.8 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    498.8 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    498.8 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    498.8 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    498.8 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    498.8 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    498.8 MiB    498.8 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    498.8 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    498.8 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    498.8 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    498.8 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    498.8 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    498.8 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n",
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    498.8 MiB    498.8 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    498.8 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    498.8 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    498.8 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    501.2 MiB      0.0 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    498.8 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    498.8 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    498.8 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    498.8 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    501.2 MiB      2.4 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    498.8 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    501.2 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    501.2 MiB    501.2 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    501.2 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    501.2 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    501.2 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    501.2 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    501.2 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    501.2 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    501.2 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    501.2 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    501.2 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    501.2 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    501.2 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    501.2 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    501.2 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    501.2 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    501.2 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    501.2 MiB    501.2 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    501.2 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    501.2 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    501.2 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    501.2 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    501.2 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    501.2 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    501.2 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 06:59:13,970] Trial 9 finished with value: 0.003241692733924502 and parameters: {'lr': 0.003260091863572929, 'num_epochs': 4}. Best is trial 0 with value: -0.0009334901986337435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /code/training_utils.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    93    501.2 MiB    501.2 MiB           1   @profile\n",
      "    94                                         # 4. Define the training function\n",
      "    95                                         def validation_loop(model, val_loader, neighborhood_model, device='cpu'):\n",
      "    96                                         \n",
      "    97    501.2 MiB      0.0 MiB           1       model.to(device)\n",
      "    98                                         \n",
      "    99    501.2 MiB      0.0 MiB           1       model.eval() # Set the model to evaluation mode\n",
      "   100    501.2 MiB      0.0 MiB           1       estimated_rewards = 0.0\n",
      "   101                                         \n",
      "   102    501.2 MiB      0.0 MiB           2       for user_idx, action_idx, rewards, original_prob in val_loader:\n",
      "   103                                                 # Move data to GPU if available\n",
      "   104    501.2 MiB      0.0 MiB           1           if torch.cuda.is_available():\n",
      "   105                                                     user_idx = user_idx.to(device) \n",
      "   106                                                     action_idx = action_idx.to(device)\n",
      "   107                                                     rewards = rewards.to(device)\n",
      "   108                                                     original_prob = original_prob.to(device) \n",
      "   109                                                 \n",
      "   110                                                 # Forward pass\n",
      "   111    501.2 MiB      0.0 MiB           1           policy = model(user_idx)\n",
      "   112    501.2 MiB      0.0 MiB           1           pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
      "   113                                                 \n",
      "   114    501.2 MiB      0.0 MiB           1           scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device=device)\n",
      "   115                                                 \n",
      "   116    501.2 MiB      0.0 MiB           2           estimated_rewards += calc_estimated_policy_rewards(\n",
      "   117    501.2 MiB      0.0 MiB           1               pscore, scores, policy, rewards, action_idx.type(torch.long)\n",
      "   118                                                 )\n",
      "   119                                         \n",
      "   120    501.2 MiB      0.0 MiB           1       return estimated_rewards.mean().item()\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    65    501.2 MiB    501.2 MiB           1       @profile    \n",
      "    66                                             def convolve(self, test_actions, test_context):\n",
      "    67                                                 # Preallocate output\n",
      "    68    501.2 MiB      0.0 MiB           1           B = test_context.shape[0]\n",
      "    69    501.2 MiB      0.0 MiB           1           weighted_sum = np.zeros(B, dtype=np.float16)\n",
      "    70    501.2 MiB      0.0 MiB           1           sim_sum = np.zeros(B, dtype=np.float16)\n",
      "    71                                         \n",
      "    72    501.2 MiB      0.0 MiB         301           for chunk_start in range(0, B, self.chunksize):\n",
      "    73    501.2 MiB      0.0 MiB         300               chunk_end = min(chunk_start + self.chunksize, B)\n",
      "    74                                         \n",
      "    75    501.2 MiB      0.0 MiB         300               cosine_context = self.context_similarity[np.int32(test_context[chunk_start:chunk_end])][:, self.context].astype(np.float16)\n",
      "    76    501.2 MiB      0.0 MiB         300               cosine_actions = self.action_similarity[np.int32(test_actions[chunk_start:chunk_end])][:, self.actions].astype(np.float16)\n",
      "    77    501.2 MiB      0.0 MiB         300               tot_cosine = self.gamma * cosine_actions + (1 - self.gamma) * cosine_context\n",
      "    78                                         \n",
      "    79    501.2 MiB      0.0 MiB         300               del cosine_context, cosine_actions\n",
      "    80                                         \n",
      "    81    501.2 MiB      0.0 MiB         300               chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "    82                                         \n",
      "    83                                                     # Get similarity and rewards for each example in the chunk\n",
      "    84    501.2 MiB      0.0 MiB         300               similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, None], chunk_top_k_idx]\n",
      "    85    501.2 MiB      0.0 MiB         300               r_chunk = self.reward[chunk_top_k_idx]\n",
      "    86                                         \n",
      "    87    501.2 MiB      0.0 MiB         300               weighted_sum[chunk_start:chunk_end] = (similarity * r_chunk).sum(axis=1)\n",
      "    88    501.2 MiB      0.0 MiB         300               sim_sum[chunk_start:chunk_end] = similarity.sum(axis=1)\n",
      "    89                                         \n",
      "    90    501.2 MiB      0.0 MiB           1           return weighted_sum / (sim_sum + 1e-8)\n",
      "\n",
      "\n",
      "Filename: /code/models.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    92    501.2 MiB    501.2 MiB           1       @profile    \n",
      "    93                                             def context_convolve(self, test_context):\n",
      "    94    501.2 MiB      0.0 MiB           1           all_context = test_context.reshape(-1, 1) @ np.ones((1, self.action_similarity.shape[0]))\n",
      "    95    501.2 MiB      0.0 MiB           1           all_context = all_context.flatten()\n",
      "    96                                         \n",
      "    97    501.2 MiB      0.0 MiB           1           all_actions = np.arange(self.action_similarity.shape[0]).reshape(-1, 1) @ np.ones((1, test_context.shape[0]))\n",
      "    98    501.2 MiB      0.0 MiB           1           all_actions = all_actions.T.flatten()\n",
      "    99                                         \n",
      "   100    501.2 MiB      0.0 MiB           1           eta_all = self.convolve(all_actions, all_context)\n",
      "   101    501.2 MiB      0.0 MiB           1           eta_all = eta_all.reshape(test_context.shape[0], self.action_similarity.shape[0], 1)\n",
      "   102    501.2 MiB      0.0 MiB           1           return eta_all\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df4 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 187\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size, val_size)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\u001b[39;00m\n\u001b[1;32m    185\u001b[0m conv_results\u001b[38;5;241m.\u001b[39mappend(eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy))\n\u001b[0;32m--> 187\u001b[0m conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalc_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m, conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    188\u001b[0m conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((emb_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)), np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((original_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))])\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# temp.append(np.mean((emb_a-our_a)**2, axis=0))\u001b[39;00m\n",
      "File \u001b[0;32m/code/simulation_utils.py:61\u001b[0m, in \u001b[0;36mcalc_reward\u001b[0;34m(dataset, policy)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalc_reward\u001b[39m(dataset, policy):\n\u001b[0;32m---> 61\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_simulation_data_from_pi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sim[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/code/simulation_utils.py:170\u001b[0m, in \u001b[0;36mcreate_simulation_data_from_pi\u001b[0;34m(env, policy, n)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     action, obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep_offline(obs, reward, done)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m         env\u001b[38;5;241m.\u001b[39mreset(user_id\u001b[38;5;241m=\u001b[39muser_id)\n\u001b[1;32m    172\u001b[0m         obs, reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/recogym/envs/abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step_offline\u001b[0;34m(self, observation, reward, done)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    225\u001b[0m         action,\n\u001b[1;32m    226\u001b[0m         Observation(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action, observation, reward, done, info\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/recogym/envs/abstract.py:160\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action_id)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (action_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     sessions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_organic_sessions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    162\u001b[0m         Observation(\n\u001b[1;32m    163\u001b[0m             DefaultContext(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         info\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (action_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/recogym/envs/abstract.py:119\u001b[0m, in \u001b[0;36mAbstractEnv.generate_organic_sessions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     session\u001b[38;5;241m.\u001b[39mnext(\n\u001b[1;32m    114\u001b[0m         DefaultContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_user_id),\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproduct_view\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Update markov state.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/recogym/envs/reco_env_v1.py:87\u001b[0m, in \u001b[0;36mRecoEnv1.update_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     86\u001b[0m     old_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_transition\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_generator\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     89\u001b[0m     old_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time\n",
      "File \u001b[0;32mmtrand.pyx:922\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/numpy/core/numerictypes.py:416\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mReturns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[0;32m--> 416\u001b[0m     arg1 \u001b[38;5;241m=\u001b[39m \u001b[43mdtype\u001b[49m(arg1)\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[1;32m    418\u001b[0m     arg2 \u001b[38;5;241m=\u001b[39m dtype(arg2)\u001b[38;5;241m.\u001b[39mtype\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/numpy/core/numerictypes.py:416\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mReturns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[0;32m--> 416\u001b[0m     arg1 \u001b[38;5;241m=\u001b[39m \u001b[43mdtype\u001b[49m(arg1)\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[1;32m    418\u001b[0m     arg2 \u001b[38;5;241m=\u001b[39m dtype(arg2)\u001b[38;5;241m.\u001b[39mtype\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/memory_profiler.py:791\u001b[0m, in \u001b[0;36mLineProfiler.trace_memory_usage\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrace_memory_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame, event, arg):\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Callback for sys.settrace\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mf_code \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_map:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
