{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import gym\n",
    "import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = softmax(our_a @ our_x.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            dataset['env'],\n",
    "                                                            pi_0,\n",
    "                                                            train_size + 500\n",
    "                                                            )\n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "\n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, 500, simulation_data, np.arange(500) + train_size, our_x)\n",
    "\n",
    "            # bpr_model = BPRModel(\n",
    "            #                     n_users,\n",
    "            #                     n_actions,\n",
    "            #                     emb_x.shape[1], \n",
    "            #                     initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "            #                     initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "            #                     )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob[train_data['x_idx']]\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob[val_data['x_idx']]\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "            \n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "                reg_dm = 0.0\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            # Bloss = BPRLoss()\n",
    "\n",
    "            \n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=10)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            # fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_a, our_x = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            # reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "            reg_dm = 0.0\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.3 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [200, 400, 600, 800, 1000, 2000, 4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-10 06:45:32,122] A new study created in memory with name: no-name-a514921e-f2f0-403d-88f4-a75a5ef76398\n",
      "[I 2025-07-10 06:45:32,367] Trial 0 finished with value: -0.052747921026163345 and parameters: {'lr': 0.05802445000559664, 'num_epochs': 9}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:32,588] Trial 1 finished with value: -0.02691320859602407 and parameters: {'lr': 0.0015417202355803839, 'num_epochs': 7}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:32,824] Trial 2 finished with value: -0.049922192406169394 and parameters: {'lr': 0.04359832746188537, 'num_epochs': 10}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:33,036] Trial 3 finished with value: -0.026482111012396767 and parameters: {'lr': 0.00018352484549730537, 'num_epochs': 5}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:33,275] Trial 4 finished with value: -0.026488384816238116 and parameters: {'lr': 0.00015761870131719322, 'num_epochs': 8}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:33,475] Trial 5 finished with value: -0.026484945221167178 and parameters: {'lr': 0.00046902847726994115, 'num_epochs': 2}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:33,700] Trial 6 finished with value: -0.027146837476856006 and parameters: {'lr': 0.003929589921822787, 'num_epochs': 4}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:33,936] Trial 7 finished with value: -0.02713691003958818 and parameters: {'lr': 0.001780263655526498, 'num_epochs': 7}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:34,180] Trial 8 finished with value: -0.02643826995586515 and parameters: {'lr': 1.4943383700821256e-05, 'num_epochs': 8}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:34,415] Trial 9 finished with value: -0.026552012488203675 and parameters: {'lr': 0.0003604623220201291, 'num_epochs': 7}. Best is trial 0 with value: -0.052747921026163345.\n",
      "[I 2025-07-10 06:45:38,774] A new study created in memory with name: no-name-612c0fd9-12e5-407f-b9fd-a0869a5a9dca\n",
      "[I 2025-07-10 06:45:39,187] Trial 0 finished with value: -0.014362823386565391 and parameters: {'lr': 0.0007393448217408756, 'num_epochs': 3}. Best is trial 0 with value: -0.014362823386565391.\n",
      "[I 2025-07-10 06:45:39,669] Trial 1 finished with value: -0.014392556553721101 and parameters: {'lr': 0.00022265721361765662, 'num_epochs': 9}. Best is trial 1 with value: -0.014392556553721101.\n",
      "[I 2025-07-10 06:45:40,147] Trial 2 finished with value: -0.014376680932073114 and parameters: {'lr': 0.00025958648787276906, 'num_epochs': 9}. Best is trial 1 with value: -0.014392556553721101.\n",
      "[I 2025-07-10 06:45:40,565] Trial 3 finished with value: -0.014439301146811278 and parameters: {'lr': 0.00022092324610621165, 'num_epochs': 4}. Best is trial 3 with value: -0.014439301146811278.\n",
      "[I 2025-07-10 06:45:41,067] Trial 4 finished with value: -0.014460346622881221 and parameters: {'lr': 1.592430200654589e-05, 'num_epochs': 10}. Best is trial 4 with value: -0.014460346622881221.\n",
      "[I 2025-07-10 06:45:41,474] Trial 5 finished with value: -0.014225622823888569 and parameters: {'lr': 0.004063292178565956, 'num_epochs': 2}. Best is trial 4 with value: -0.014460346622881221.\n",
      "[I 2025-07-10 06:45:41,959] Trial 6 finished with value: -0.013331497334185875 and parameters: {'lr': 0.00429522983909109, 'num_epochs': 10}. Best is trial 4 with value: -0.014460346622881221.\n",
      "[I 2025-07-10 06:45:42,365] Trial 7 finished with value: -0.01435330367314518 and parameters: {'lr': 0.0010333815811818564, 'num_epochs': 2}. Best is trial 4 with value: -0.014460346622881221.\n",
      "[I 2025-07-10 06:45:42,760] Trial 8 finished with value: -0.01446521727896111 and parameters: {'lr': 1.1391476383959303e-05, 'num_epochs': 1}. Best is trial 8 with value: -0.01446521727896111.\n",
      "[I 2025-07-10 06:45:43,183] Trial 9 finished with value: -0.014313277266755771 and parameters: {'lr': 0.0014045469545238682, 'num_epochs': 4}. Best is trial 8 with value: -0.01446521727896111.\n",
      "[I 2025-07-10 06:45:48,162] A new study created in memory with name: no-name-ee68c675-0a8c-41e4-8ca9-c7fb63e4720f\n",
      "[I 2025-07-10 06:45:48,799] Trial 0 finished with value: -0.021466111799020436 and parameters: {'lr': 5.368332617324219e-05, 'num_epochs': 3}. Best is trial 0 with value: -0.021466111799020436.\n",
      "[I 2025-07-10 06:45:49,436] Trial 1 finished with value: -0.02144326623727265 and parameters: {'lr': 6.330770250044223e-05, 'num_epochs': 1}. Best is trial 0 with value: -0.021466111799020436.\n",
      "[I 2025-07-10 06:45:50,075] Trial 2 finished with value: -0.021578280176012238 and parameters: {'lr': 0.00021963909279791897, 'num_epochs': 3}. Best is trial 2 with value: -0.021578280176012238.\n",
      "[I 2025-07-10 06:45:50,767] Trial 3 finished with value: -0.05908502776750886 and parameters: {'lr': 0.09462537220125523, 'num_epochs': 7}. Best is trial 3 with value: -0.05908502776750886.\n",
      "[I 2025-07-10 06:45:51,493] Trial 4 finished with value: -0.02185103343357484 and parameters: {'lr': 0.00026008602492058673, 'num_epochs': 8}. Best is trial 3 with value: -0.05908502776750886.\n",
      "[I 2025-07-10 06:45:52,130] Trial 5 finished with value: -0.021451501810630113 and parameters: {'lr': 3.441498062744206e-05, 'num_epochs': 3}. Best is trial 3 with value: -0.05908502776750886.\n",
      "[I 2025-07-10 06:45:52,761] Trial 6 finished with value: -0.026830066302497638 and parameters: {'lr': 0.008850606317145205, 'num_epochs': 3}. Best is trial 3 with value: -0.05908502776750886.\n",
      "[I 2025-07-10 06:45:53,486] Trial 7 finished with value: -0.022746782452325358 and parameters: {'lr': 0.000726735019158297, 'num_epochs': 9}. Best is trial 3 with value: -0.05908502776750886.\n",
      "[I 2025-07-10 06:45:54,104] Trial 8 finished with value: -0.02151693121807975 and parameters: {'lr': 0.00043019842239630377, 'num_epochs': 1}. Best is trial 3 with value: -0.05908502776750886.\n",
      "[I 2025-07-10 06:45:54,752] Trial 9 finished with value: -0.02519946811968499 and parameters: {'lr': 0.005667384134908573, 'num_epochs': 3}. Best is trial 3 with value: -0.05908502776750886.\n",
      "[I 2025-07-10 06:46:00,262] A new study created in memory with name: no-name-6c6b21ed-b294-497a-a201-4f0cf902fe76\n",
      "[I 2025-07-10 06:46:01,164] Trial 0 finished with value: -0.019385237920363833 and parameters: {'lr': 0.002452773406863977, 'num_epochs': 5}. Best is trial 0 with value: -0.019385237920363833.\n",
      "[I 2025-07-10 06:46:02,089] Trial 1 finished with value: -0.04744596809613234 and parameters: {'lr': 0.044627112657894236, 'num_epochs': 5}. Best is trial 1 with value: -0.04744596809613234.\n",
      "[I 2025-07-10 06:46:02,997] Trial 2 finished with value: -0.01715287113412374 and parameters: {'lr': 1.1354292693386407e-05, 'num_epochs': 5}. Best is trial 1 with value: -0.04744596809613234.\n",
      "[I 2025-07-10 06:46:03,929] Trial 3 finished with value: -0.038134010854403357 and parameters: {'lr': 0.032738985340111194, 'num_epochs': 5}. Best is trial 1 with value: -0.04744596809613234.\n",
      "[I 2025-07-10 06:46:04,931] Trial 4 finished with value: -0.023014008546849984 and parameters: {'lr': 0.0030875065701225215, 'num_epochs': 9}. Best is trial 1 with value: -0.04744596809613234.\n",
      "[I 2025-07-10 06:46:05,809] Trial 5 finished with value: -0.03004829971192344 and parameters: {'lr': 0.05522059507699341, 'num_epochs': 1}. Best is trial 1 with value: -0.04744596809613234.\n",
      "[I 2025-07-10 06:46:06,754] Trial 6 finished with value: -0.017156059477682534 and parameters: {'lr': 1.0727075985654938e-05, 'num_epochs': 7}. Best is trial 1 with value: -0.04744596809613234.\n",
      "[I 2025-07-10 06:46:07,738] Trial 7 finished with value: -0.04976194174512842 and parameters: {'lr': 0.08359699680673231, 'num_epochs': 9}. Best is trial 7 with value: -0.04976194174512842.\n",
      "[I 2025-07-10 06:46:08,750] Trial 8 finished with value: -0.02326686322393668 and parameters: {'lr': 0.0027138356192612663, 'num_epochs': 10}. Best is trial 7 with value: -0.04976194174512842.\n",
      "[I 2025-07-10 06:46:09,584] Trial 9 finished with value: -0.017145123572333366 and parameters: {'lr': 1.129839787160298e-05, 'num_epochs': 1}. Best is trial 7 with value: -0.04976194174512842.\n",
      "[I 2025-07-10 06:46:15,662] A new study created in memory with name: no-name-3db67ac2-e8b0-4dd8-bd7e-41ec59b9b988\n",
      "[I 2025-07-10 06:46:16,891] Trial 0 finished with value: -0.023013584323330106 and parameters: {'lr': 0.009676886951572698, 'num_epochs': 8}. Best is trial 0 with value: -0.023013584323330106.\n",
      "[I 2025-07-10 06:46:18,166] Trial 1 finished with value: -0.012679298950485779 and parameters: {'lr': 0.0011669141166301116, 'num_epochs': 10}. Best is trial 0 with value: -0.023013584323330106.\n",
      "[I 2025-07-10 06:46:19,389] Trial 2 finished with value: -0.012725479456323728 and parameters: {'lr': 0.0015898057059064185, 'num_epochs': 7}. Best is trial 0 with value: -0.023013584323330106.\n",
      "[I 2025-07-10 06:46:20,634] Trial 3 finished with value: -0.02684938684505214 and parameters: {'lr': 0.013050131816330982, 'num_epochs': 8}. Best is trial 3 with value: -0.02684938684505214.\n",
      "[I 2025-07-10 06:46:21,792] Trial 4 finished with value: -0.01236362807833817 and parameters: {'lr': 3.0019892411928685e-05, 'num_epochs': 5}. Best is trial 3 with value: -0.02684938684505214.\n",
      "[I 2025-07-10 06:46:22,872] Trial 5 finished with value: -0.012425174086008657 and parameters: {'lr': 0.00493370122643738, 'num_epochs': 2}. Best is trial 3 with value: -0.02684938684505214.\n",
      "[I 2025-07-10 06:46:23,940] Trial 6 finished with value: -0.01236226568794202 and parameters: {'lr': 2.222487655832491e-05, 'num_epochs': 2}. Best is trial 3 with value: -0.02684938684505214.\n",
      "[I 2025-07-10 06:46:25,154] Trial 7 finished with value: -0.012361749362697653 and parameters: {'lr': 1.5839798271506946e-05, 'num_epochs': 7}. Best is trial 3 with value: -0.02684938684505214.\n",
      "[I 2025-07-10 06:46:26,471] Trial 8 finished with value: -0.013030158984112873 and parameters: {'lr': 0.0015905410289936616, 'num_epochs': 10}. Best is trial 3 with value: -0.02684938684505214.\n",
      "[I 2025-07-10 06:46:27,532] Trial 9 finished with value: -0.015351054006495349 and parameters: {'lr': 0.04699179114394867, 'num_epochs': 1}. Best is trial 3 with value: -0.02684938684505214.\n",
      "[I 2025-07-10 06:46:35,655] A new study created in memory with name: no-name-0a7cc6c1-be16-4c68-8b8b-83db44840b22\n",
      "[I 2025-07-10 06:46:38,192] Trial 0 finished with value: -0.013816576569170803 and parameters: {'lr': 0.0015002909619835998, 'num_epochs': 6}. Best is trial 0 with value: -0.013816576569170803.\n",
      "[I 2025-07-10 06:46:40,680] Trial 1 finished with value: -0.010666493949752578 and parameters: {'lr': 1.1641544940769215e-05, 'num_epochs': 6}. Best is trial 0 with value: -0.013816576569170803.\n",
      "[I 2025-07-10 06:46:43,373] Trial 2 finished with value: -0.047163180478784573 and parameters: {'lr': 0.040027855382087106, 'num_epochs': 9}. Best is trial 2 with value: -0.047163180478784573.\n",
      "[I 2025-07-10 06:46:46,072] Trial 3 finished with value: -0.03852531675443236 and parameters: {'lr': 0.008113158550826746, 'num_epochs': 9}. Best is trial 2 with value: -0.047163180478784573.\n",
      "[I 2025-07-10 06:46:48,594] Trial 4 finished with value: -0.011301843050521584 and parameters: {'lr': 0.000369766232410221, 'num_epochs': 6}. Best is trial 2 with value: -0.047163180478784573.\n",
      "[I 2025-07-10 06:46:51,340] Trial 5 finished with value: -0.011614005524740106 and parameters: {'lr': 0.00038815918550319837, 'num_epochs': 9}. Best is trial 2 with value: -0.047163180478784573.\n",
      "[I 2025-07-10 06:46:53,734] Trial 6 finished with value: -0.013354891745383526 and parameters: {'lr': 0.002018073098442342, 'num_epochs': 4}. Best is trial 2 with value: -0.047163180478784573.\n",
      "[I 2025-07-10 06:46:56,500] Trial 7 finished with value: -0.010829185333006612 and parameters: {'lr': 6.877674761587832e-05, 'num_epochs': 10}. Best is trial 2 with value: -0.047163180478784573.\n",
      "[I 2025-07-10 06:46:58,743] Trial 8 finished with value: -0.013584691838772254 and parameters: {'lr': 0.007170352605755713, 'num_epochs': 1}. Best is trial 2 with value: -0.047163180478784573.\n",
      "[I 2025-07-10 06:47:01,027] Trial 9 finished with value: -0.012549799631673143 and parameters: {'lr': 0.003923793254588984, 'num_epochs': 2}. Best is trial 2 with value: -0.047163180478784573.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`action_dist` must be a probability distribution",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df4 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 173\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size)\u001b[0m\n\u001b[1;32m    170\u001b[0m reg_results\u001b[38;5;241m.\u001b[39mappend(reg_dm)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m conv_results\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighberhoodmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_policy_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    175\u001b[0m conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(calc_reward(dataset, policy), conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    176\u001b[0m conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((emb_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)), np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean((original_a\u001b[38;5;241m-\u001b[39mour_a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))])\n",
      "File \u001b[0;32m/code/simulation_utils.py:211\u001b[0m, in \u001b[0;36meval_policy\u001b[0;34m(model, test_data, original_policy_prob, policy)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# reward = test_data['q_x_a'][test_data['x_idx'], actions]\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# res.append(reward.mean())\u001b[39;00m\n\u001b[1;32m    209\u001b[0m pscore \u001b[38;5;241m=\u001b[39m original_policy_prob[test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_idx\u001b[39m\u001b[38;5;124m'\u001b[39m], actions]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 211\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_policy_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    212\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(dr\u001b[38;5;241m.\u001b[39mestimate_policy_value(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], policy, scores, pscore\u001b[38;5;241m=\u001b[39mpscore))\n\u001b[1;32m    213\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(ipw\u001b[38;5;241m.\u001b[39mestimate_policy_value(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], policy, pscore\u001b[38;5;241m=\u001b[39mpscore))\n",
      "File \u001b[0;32m/code/estimators.py:771\u001b[0m, in \u001b[0;36mDirectMethod.estimate_policy_value\u001b[0;34m(self, action_dist, estimated_rewards_by_reg_model, position, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate the policy value of evaluation policy.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    764\u001b[0m \n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m check_array(\n\u001b[1;32m    767\u001b[0m     array\u001b[38;5;241m=\u001b[39mestimated_rewards_by_reg_model,\n\u001b[1;32m    768\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimated_rewards_by_reg_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    769\u001b[0m     expected_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    770\u001b[0m )\n\u001b[0;32m--> 771\u001b[0m \u001b[43mcheck_ope_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimated_rewards_by_reg_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimated_rewards_by_reg_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     position \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(action_dist\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m/code/saito_helpers.py:587\u001b[0m, in \u001b[0;36mcheck_ope_inputs\u001b[0;34m(action_dist, position, action, reward, pscore, estimated_rewards_by_reg_model, estimated_importance_weights)\u001b[0m\n\u001b[1;32m    585\u001b[0m check_array(array\u001b[38;5;241m=\u001b[39maction_dist, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(action_dist\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`action_dist` must be a probability distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# position\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: `action_dist` must be a probability distribution"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1904</td>\n",
       "      <td>0.1899</td>\n",
       "      <td>0.1524</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.5069</td>\n",
       "      <td>1.0378</td>\n",
       "      <td>1.1542</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>0.6694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>1.1592</td>\n",
       "      <td>1.2819</td>\n",
       "      <td>1.0435</td>\n",
       "      <td>0.8357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>1.3957</td>\n",
       "      <td>1.5383</td>\n",
       "      <td>1.2811</td>\n",
       "      <td>1.1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>1.5903</td>\n",
       "      <td>1.7414</td>\n",
       "      <td>1.5547</td>\n",
       "      <td>1.3738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.1317</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>1.8113</td>\n",
       "      <td>1.9436</td>\n",
       "      <td>1.9940</td>\n",
       "      <td>1.8124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>2.1439</td>\n",
       "      <td>2.2478</td>\n",
       "      <td>2.7655</td>\n",
       "      <td>2.5865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1889</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>2.1254</td>\n",
       "      <td>2.2207</td>\n",
       "      <td>2.8812</td>\n",
       "      <td>2.6593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   \n",
       "1           0.1904 0.1899  0.1524   0.0293   0.1332     0.5069   \n",
       "2           0.1913 0.1715  0.1413   0.0089   0.0080     0.1411   \n",
       "3           0.1934 0.1934  0.1476   0.0063   0.0059     0.2330   \n",
       "4           0.1916 0.1916  0.1409   0.0330   0.1543     0.4980   \n",
       "5           0.1923 0.1923  0.1428   0.0013   0.1317     0.3333   \n",
       "10          0.1890 0.0114  0.1402   0.0054   0.0385     0.0718   \n",
       "20          0.1889 0.0217  0.1330   0.0297   0.0198     0.0187   \n",
       "\n",
       "    action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                0.3386        0.0000                0.5364         0.0000  \n",
       "1                1.0378        1.1542                0.8883         0.6694  \n",
       "2                1.1592        1.2819                1.0435         0.8357  \n",
       "3                1.3957        1.5383                1.2811         1.1333  \n",
       "4                1.5903        1.7414                1.5547         1.3738  \n",
       "5                1.8113        1.9436                1.9940         1.8124  \n",
       "10               2.1439        2.2478                2.7655         2.5865  \n",
       "20               2.1254        2.2207                2.8812         2.6593  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_list = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-26 06:00:47,088] A new study created in memory with name: no-name-f919595e-a7c7-4192-82a0-88df61c0332c\n",
      "[I 2025-06-26 06:00:47,250] Trial 0 finished with value: 0.037067933837222816 and parameters: {'lr': 0.0005225653346301143, 'num_epochs': 6}. Best is trial 0 with value: 0.037067933837222816.\n",
      "[I 2025-06-26 06:00:47,387] Trial 1 finished with value: 0.037253001270894676 and parameters: {'lr': 0.00027374423899567256, 'num_epochs': 1}. Best is trial 0 with value: 0.037067933837222816.\n",
      "[I 2025-06-26 06:00:47,560] Trial 2 finished with value: -0.014201574651007048 and parameters: {'lr': 0.03807918977276577, 'num_epochs': 9}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:47,701] Trial 3 finished with value: 0.03726279086963771 and parameters: {'lr': 5.0536893389243686e-05, 'num_epochs': 2}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:47,859] Trial 4 finished with value: 0.037096458943192856 and parameters: {'lr': 0.000574430717237854, 'num_epochs': 5}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:47,998] Trial 5 finished with value: 0.03724411994920657 and parameters: {'lr': 0.0006331207919239826, 'num_epochs': 1}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:48,154] Trial 6 finished with value: 0.03724627132928093 and parameters: {'lr': 7.477988541885005e-05, 'num_epochs': 5}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:48,309] Trial 7 finished with value: 0.030308328315091976 and parameters: {'lr': 0.02463928556847755, 'num_epochs': 3}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:48,490] Trial 8 finished with value: 0.03651941588136645 and parameters: {'lr': 0.001346442331046115, 'num_epochs': 9}. Best is trial 2 with value: -0.014201574651007048.\n",
      "[I 2025-06-26 06:00:48,647] Trial 9 finished with value: 0.03671392596654394 and parameters: {'lr': 0.0017028311394250487, 'num_epochs': 5}. Best is trial 2 with value: -0.014201574651007048.\n"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.2204</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0874</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4769</td>\n",
       "      <td>1.3154</td>\n",
       "      <td>1.5475</td>\n",
       "      <td>1.3733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1          0.1448 0.2204  0.1381   0.0131   0.0874     0.1590   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1      0.0000       0.0000       0.0000         0.0000               1.4769   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.5364         0.0000  \n",
       "1        1.3154                1.5475         1.3733  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trainer_trial() got an unexpected keyword argument 'num_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df6 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: trainer_trial() got an unexpected keyword argument 'num_epochs'"
     ]
    }
   ],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>1.0816</td>\n",
       "      <td>1.2032</td>\n",
       "      <td>1.0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2871</td>\n",
       "      <td>1.0995</td>\n",
       "      <td>1.2186</td>\n",
       "      <td>1.0893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3095</td>\n",
       "      <td>1.1251</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>1.1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3261</td>\n",
       "      <td>1.1443</td>\n",
       "      <td>1.2600</td>\n",
       "      <td>1.1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3455</td>\n",
       "      <td>1.1664</td>\n",
       "      <td>1.2692</td>\n",
       "      <td>1.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4985</td>\n",
       "      <td>1.3407</td>\n",
       "      <td>1.3654</td>\n",
       "      <td>1.2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0338</td>\n",
       "      <td>1.9246</td>\n",
       "      <td>1.6279</td>\n",
       "      <td>1.6433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1           0.1451 0.0462  0.1381   0.0028   0.0221     0.0240   0.0000   \n",
       "2           0.1453 0.0158  0.1383   0.0246   0.0427     0.0336   0.0000   \n",
       "3           0.1453 0.0045  0.1382   0.0137   0.0115     0.0118   0.0000   \n",
       "4           0.1453 0.0006  0.1383   0.0117   0.0018     0.0006   0.0000   \n",
       "5           0.1449 0.0336  0.1377   0.0085   0.0359     0.0393   0.0000   \n",
       "10          0.1448 0.0236  0.1370   0.0037   0.0011     0.0013   0.0000   \n",
       "20          0.1476 0.0106  0.1411   0.0031   0.0055     0.0050   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1       0.0000       0.0000       0.0000         0.0000               1.2730   \n",
       "2       0.0000       0.0000       0.0000         0.0000               1.2871   \n",
       "3       0.0000       0.0000       0.0000         0.0000               1.3095   \n",
       "4       0.0000       0.0000       0.0000         0.0000               1.3261   \n",
       "5       0.0000       0.0000       0.0000         0.0000               1.3455   \n",
       "10      0.0000       0.0000       0.0000         0.0000               1.4985   \n",
       "20      0.0000       0.0000       0.0000         0.0000               2.0338   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.5364         0.0000  \n",
       "1         1.0816                1.2032         1.0708  \n",
       "2         1.0995                1.2186         1.0893  \n",
       "3         1.1251                1.2390         1.1124  \n",
       "4         1.1443                1.2600         1.1368  \n",
       "5         1.1664                1.2692         1.1500  \n",
       "10        1.3407                1.3654         1.2810  \n",
       "20        1.9246                1.6279         1.6433  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.7198: 100%|| 10/10 [00:00<00:00, 101.46it/s]\n",
      "Epoch [10/10], Loss: 98.9210: 100%|| 10/10 [00:00<00:00, 104.40it/s]\n",
      "Epoch [10/10], Loss: -3.0079: 100%|| 10/10 [00:00<00:00, 52.63it/s]\n",
      "Epoch [10/10], Loss: 91.4421: 100%|| 10/10 [00:00<00:00, 50.28it/s]\n",
      "Epoch [10/10], Loss: -4.7455: 100%|| 10/10 [00:00<00:00, 35.61it/s]\n",
      "Epoch [10/10], Loss: 89.6301: 100%|| 10/10 [00:00<00:00, 32.14it/s]\n",
      "Epoch [10/10], Loss: -5.4052: 100%|| 10/10 [00:00<00:00, 26.85it/s]\n",
      "Epoch [10/10], Loss: 85.7448: 100%|| 10/10 [00:00<00:00, 25.09it/s]\n"
     ]
    }
   ],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4746</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>1.3637</td>\n",
       "      <td>1.2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.1998</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7363</td>\n",
       "      <td>1.6121</td>\n",
       "      <td>1.5940</td>\n",
       "      <td>1.5158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.1355</td>\n",
       "      <td>2.0325</td>\n",
       "      <td>1.8826</td>\n",
       "      <td>1.8401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.1391</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5199</td>\n",
       "      <td>2.4330</td>\n",
       "      <td>2.1781</td>\n",
       "      <td>2.1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1          0.1430 0.0497  0.1361   0.0009   0.0148     0.0294   0.0000   \n",
       "2          0.1449 0.1010  0.1397   0.0187   0.1998     0.1172   0.0000   \n",
       "3          0.1450 0.1332  0.1399   0.0072   0.1406     0.1160   0.0000   \n",
       "4          0.1486 0.0274  0.1391   0.0125   0.0855     0.1512   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1      0.0000       0.0000       0.0000         0.0000               1.4746   \n",
       "2      0.0000       0.0000       0.0000         0.0000               1.7363   \n",
       "3      0.0000       0.0000       0.0000         0.0000               2.1355   \n",
       "4      0.0000       0.0000       0.0000         0.0000               2.5199   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.5364         0.0000  \n",
       "1        1.3141                1.3637         1.2353  \n",
       "2        1.6121                1.5940         1.5158  \n",
       "3        2.0325                1.8826         1.8401  \n",
       "4        2.4330                2.1781         2.1835  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
