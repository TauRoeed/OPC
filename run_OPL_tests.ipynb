{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "from abc import ABCMeta\n",
    "\n",
    "\n",
    "from obp.ope import (\n",
    "    RegressionModel,\n",
    "    DirectMethod as DM,\n",
    ")\n",
    "\n",
    "from my_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simluation_data_from_pi,\n",
    "    get_train_data,\n",
    "    CFModel,\n",
    "    CustomCFDataset,\n",
    "    NeighborhoodModel\n",
    ")\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(dataset, policy):\n",
    "    return np.array([np.sum(dataset['q_x_a'] * policy.squeeze(), axis=1).mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPWPolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(IPWPolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        # q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        # dm_grads = (scores * policy_prob.detach() * torch.log(policy_prob)).sum(dim=1)\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # reinforce trick step\n",
    "        # reinforce_grad = ((iw * (original_policy_rewards - q_hat_at_position) * log_pi) / iw.sum()) + dm_grads\n",
    "        reinforce_grad = iw * original_policy_rewards * log_pi\n",
    "        \n",
    "        return reinforce_grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNDRPolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(SNDRPolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        iw = iw.detach()\n",
    "        q_hat_at_position = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "        dm_reward = (scores * policy_prob.detach()).sum(dim=1)\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # reinforce trick step\n",
    "        r_hat = ((iw * (original_policy_rewards - q_hat_at_position)) / iw.sum()) + dm_reward\n",
    "        reinforce_grad = r_hat * log_pi\n",
    "        return reinforce_grad.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the training function\n",
    "def train(model, train_loader, neighborhood_model, num_epochs=1, lr=0.0001, device='cpu'):\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # here we can change the learning rate\n",
    "    criterion = SNDRPolicyLoss()\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    tq = tqdm(range(num_epochs))\n",
    "    for epoch in tq:\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for user_idx, action_idx, rewards, original_prob in train_loader:\n",
    "            # Move data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                user_idx = user_idx.to(device) \n",
    "                action_idx = action_idx.to(device)\n",
    "                rewards = rewards.to(device)\n",
    "                original_prob = original_prob.to(device) \n",
    "            \n",
    "            # Forward pass\n",
    "            policy = model(user_idx)\n",
    "            pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "            \n",
    "            scores = torch.tensor(neighborhood_model.predict(user_idx.cpu().numpy()), device='cpu')\n",
    "            \n",
    "            loss = criterion(\n",
    "                              pscore,\n",
    "                              scores,\n",
    "                              policy, \n",
    "                              rewards, \n",
    "                              action_idx.type(torch.long), \n",
    "                              )\n",
    "            \n",
    "            # Zero the gradients Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()                        \n",
    "            optimizer.step()\n",
    "            \n",
    "            # update neighborhood\n",
    "            # action_emb, context_emb = model.get_params()\n",
    "            \n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "            # Print statistics after each epoch\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            tq.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "        # neighborhood_model.update(action_emb.detach().numpy(), context_emb.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  num_epochs,\n",
    "                  lr=0.001\n",
    "                  ):\n",
    "    # Define device at the beginning\n",
    "    device = torch.device('cpu')  # Force CPU usage\n",
    "    \n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    \n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    first = True\n",
    "    zero = True\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simluation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            dataset[\"n_users\"],\n",
    "                                                            dataset[\"n_actions\"],\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            \n",
    "            regression_model = RegressionModel(\n",
    "                                                n_actions=n_actions,\n",
    "                                                action_context=our_x,\n",
    "                                                base_model=LogisticRegression(random_state=12345)\n",
    "                                                )\n",
    "            \n",
    "            regression_model.fit(train_data['x'], \n",
    "                        train_data['a'],\n",
    "                        train_data['r'],\n",
    "                        original_policy_prob[train_data['x_idx'],\n",
    "                        train_data['a']].squeeze()\n",
    "                        )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device='cpu'), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device='cpu')\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob[train_data['x_idx']]\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=False)\n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            train(model, train_loader, neighberhoodmodel, num_epochs=num_epochs, lr=lr, device='cpu')\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_a, our_x = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opl_results_dict(reg_results, conv_results):\n",
    "    reward = conv_results[:, 0]\n",
    "    return    dict(\n",
    "                policy_rewards=np.mean(reward),\n",
    "                ipw=np.mean(abs(conv_results[: ,3] - reward)),\n",
    "                reg_dm=np.mean(abs(reg_results - reward)),\n",
    "                conv_dm=np.mean(abs(conv_results[: ,1] - reward)),\n",
    "                conv_dr=np.mean(abs(conv_results[: ,2] - reward)),\n",
    "                conv_sndr=np.mean(abs(conv_results[: ,4] - reward)),\n",
    "\n",
    "                ipw_var=np.var(conv_results[: ,3]),\n",
    "                reg_dm_var=np.var(reg_results),\n",
    "                conv_dm_var=np.var(conv_results[: ,1]),\n",
    "                conv_dr_var=np.var(conv_results[: ,2]),\n",
    "                conv_sndr_var=np.var(conv_results[: ,4]),\n",
    "\n",
    "                                \n",
    "                # ipw_p_err=np.mean(abs(conv_results[: ,3] - reward) / reward) * 100,\n",
    "                # reg_dm_p_err=np.mean(abs(reg_results - reward) / reward) * 100,\n",
    "                # conv_dm_p_err=np.mean(abs(conv_results[: ,1] - reward) / reward) * 100,\n",
    "                # conv_dr_p_err=np.mean(abs(conv_results[: ,2] - reward) / reward) * 100,\n",
    "                # conv_sndr_p_err=np.mean(abs(conv_results[: ,4] - reward) / reward) * 100,\n",
    "                \n",
    "                action_diff_to_real=np.mean(conv_results[: ,5]),\n",
    "                action_delta=np.mean(conv_results[: ,6]),\n",
    "                context_diff_to_real=np.mean(conv_results[: ,7]),\n",
    "                context_delta=np.mean(conv_results[: ,8])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.3 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [1, 2, 3, 4, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.train() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df4 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 96\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size, num_epochs, lr)\u001b[0m\n\u001b[1;32m     93\u001b[0m     results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m get_opl_results_dict(reg_results, conv_results)\n\u001b[1;32m     94\u001b[0m     reg_results, conv_results \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 96\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighberhoodmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\u001b[39;00m\n\u001b[1;32m     99\u001b[0m our_a, our_x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_params()\n",
      "Cell \u001b[0;32mIn[74], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, neighborhood_model, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr) \u001b[38;5;66;03m# here we can change the learning rate\u001b[39;00m\n\u001b[1;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m SNDRPolicyLoss()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[1;32m      9\u001b[0m tq \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tq:\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.train() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=1, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=10, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OBPEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
