{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `calc_reward` Function\n",
    "Calculates the expected reward of a policy by computing the weighted average of true reward probabilities.\n",
    "\n",
    "### Parameters\n",
    "- `dataset` (dict): Contains dataset information including `q_x_a`, the true reward probabilities for each user-action pair\n",
    "- `policy` (numpy.ndarray): Policy probabilities with shape [n_users, n_actions, 1]\n",
    "\n",
    "### Returns\n",
    "- `numpy.ndarray`: A single-element array containing the expected policy reward\n",
    "\n",
    "### Mathematical Formulation\n",
    "Implements: $R_{gt} = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}{\\pi_{ij} \\cdot p_{ij}}$\n",
    "\n",
    "Where:\n",
    "- $\\pi_{ij}$ is the policy probability for user $i$ choosing action $j$\n",
    "- $p_{ij}$ is the true reward probability for user $i$ choosing action $j$ (stored in `q_x_a`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_opl_results_dict` Function\n",
    "\n",
    "This function processes evaluation results from various offline policy learning (OPL) estimators and computes summary statistics.\n",
    "\n",
    "### Parameters\n",
    "- **reg_results** (numpy.ndarray): Results from regression-based direct method estimator\n",
    "- **conv_results** (numpy.ndarray): Results from various estimators including true rewards and embeddings quality metrics\n",
    "\n",
    "### Returns\n",
    "- **dict**: A dictionary containing the following metrics:\n",
    "  - `policy_rewards`: Mean true reward of the learned policy\n",
    "  - Error metrics (absolute difference between estimator and true reward):\n",
    "    - `ipw`: Inverse Propensity Weighting estimator error\n",
    "    - `reg_dm`: Regression-based Direct Method estimator error\n",
    "    - `conv_dm`: Convolution-based Direct Method estimator error\n",
    "    - `conv_dr`: Convolution-based Doubly Robust estimator error\n",
    "    - `conv_sndr`: Convolution-based Self-Normalized Doubly Robust estimator error\n",
    "  - Variance metrics for each estimator:\n",
    "    - `ipw_var`, `reg_dm_var`, `conv_dm_var`, `conv_dr_var`, `conv_sndr_var`\n",
    "  - Embedding quality metrics:\n",
    "    - `action_diff_to_real`: RMSE between learned and real action embeddings\n",
    "    - `action_delta`: RMSE between learned and original action embeddings\n",
    "    - `context_diff_to_real`: RMSE between learned and real context embeddings\n",
    "    - `context_delta`: RMSE between learned and original context embeddings\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses the first column of `conv_results` as the ground truth reward\n",
    "- Contains commented-out code for percentage error calculations\n",
    "- Computes absolute errors rather than signed differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `IPWPolicyLoss` Class\n",
    "\n",
    "This class implements an Inverse Propensity Weighting (IPW) loss function for counterfactual policy learning from offline bandit data.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The loss implements the IPW estimator as a differentiable function:\n",
    "\n",
    "$$\\mathcal{L}_{IPW} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)} \\cdot r_i \\cdot \\log(\\pi_e(a_i|x_i))$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_e(a_i|x_i)$ is the probability of the new policy taking action $a_i$ for context $x_i$\n",
    "- $\\pi_0(a_i|x_i)$ is the propensity score (probability of the logging policy)\n",
    "- $r_i$ is the observed reward\n",
    "- $n$ is the batch size\n",
    "\n",
    "### Parameters\n",
    "- **log_eps** (float): Small constant added to prevent numerical instability in log calculations\n",
    "\n",
    "### Method: `forward`\n",
    "- **pscore** (Tensor): Propensity scores from original logging policy\n",
    "- **scores** (Tensor): Model-estimated reward predictions for each action (not being used)\n",
    "- **policy_prob** (Tensor): Probabilities from current policy being optimized\n",
    "- **original_policy_rewards** (Tensor): Observed rewards from logged data\n",
    "- **original_policy_actions** (Tensor): Actions that were taken in the logged data\n",
    "\n",
    "### Implementation Notes\n",
    "- Importance weights (`iw`) are detached from the computation graph\n",
    "- Uses the REINFORCE policy gradient method\n",
    "- The implementation includes commented-out code for more advanced variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SNDRPolicyLoss` Class\n",
    "\n",
    "This class implements a Self-Normalized Doubly Robust (SNDR) loss function for counterfactual policy learning from offline bandit data.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The loss combines IPW with direct method estimates for variance reduction:\n",
    "\n",
    "$$\\mathcal{L}_{SNDR} = \\frac{1}{n}\\sum_{i=1}^{n} \\left( \\frac{\\sum_{i=1}^{n}\\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)} \\cdot (r_i - \\hat{q}(x_i,a_i))}{\\sum_{i=1}^{n}\\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)}} + \\sum_{a}\\pi_e(a|x_i)\\hat{q}(x_i,a) \\right) \\cdot \\log(\\pi_e(a_i|x_i))$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_e(a_i|x_i)$ is the probability from the new policy\n",
    "- $\\pi_0(a_i|x_i)$ is the propensity score from the logging policy\n",
    "- $r_i$ is the observed reward\n",
    "- $\\hat{q}(x_i,a_i)$ is the estimated reward from a direct model\n",
    "- $n$ is the batch size\n",
    "\n",
    "### Parameters\n",
    "- **log_eps** (float): Small constant added to prevent numerical instability in log calculations\n",
    "\n",
    "### Method: `forward`\n",
    "- **pscore** (Tensor): Propensity scores from original logging policy\n",
    "- **scores** (Tensor): Model-estimated reward predictions for each action\n",
    "- **policy_prob** (Tensor): Probabilities from current policy being optimized\n",
    "- **original_policy_rewards** (Tensor): Observed rewards from logged data\n",
    "- **original_policy_actions** (Tensor): Actions that were taken in the logged data\n",
    "\n",
    "### Implementation Notes\n",
    "- Combines direct method rewards with importance-weighted corrections\n",
    "- Self-normalizes the importance weights by dividing by their sum\n",
    "- Generally provides lower variance estimates than pure IPW approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train` Function\n",
    "\n",
    "This function trains a policy model with Self-Normalized Doubly Robust (SNDR) loss for counterfactual policy learning.\n",
    "\n",
    "### Parameters\n",
    "- **model** (CFModel): The policy model to be trained, which maps users to action probabilities\n",
    "- **train_loader** (DataLoader): PyTorch data loader containing training data with user indices, actions, rewards, and logging policy probabilities\n",
    "- **neighborhood_model** (NeighborhoodModel): Model that provides reward estimates based on neighborhood information\n",
    "- **num_epochs** (int, default=1): Number of training epochs\n",
    "- **lr** (float, default=0.0001): Learning rate for the Adam optimizer\n",
    "- **device** (str or torch.device, default='cpu'): Device to run the training on\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes an Adam optimizer and SNDR loss criterion\n",
    "2. For each epoch:\n",
    "   - Iterates through batches from the data loader\n",
    "   - Moves data to specified device (CPU/GPU)\n",
    "   - Gets policy probabilities by running the model on user indices\n",
    "   - Computes propensity scores from the logging policy\n",
    "   - Gets reward predictions from neighborhood model\n",
    "   - Calculates loss using the SNDR criterion\n",
    "   - Performs backpropagation and optimization\n",
    "   - Tracks and displays running loss statistics\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses `tqdm` for progress visualization\n",
    "- Contains commented-out code for neighborhood model updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size\n",
    "                  ):\n",
    "    \n",
    "    # Define device at the beginning\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Optuna objective function\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "        epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "        trial_neigh_model = NeighborhoodModel(\n",
    "                                                train_data['x_idx'],\n",
    "                                                train_data['a'], \n",
    "                                                our_a,\n",
    "                                                our_x, \n",
    "                                                train_data['r'], \n",
    "                                                num_neighbors=num_neighbors\n",
    "                                            )\n",
    "        \n",
    "\n",
    "        trial_model = CFModel(\n",
    "                        n_users, \n",
    "                        n_actions, \n",
    "                        emb_dim, \n",
    "                        initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                        initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                        )\n",
    "        \n",
    "        # Training\n",
    "        train(trial_model, train_loader, trial_neigh_model, criterion=SNDRPolicyLoss(), num_epochs=epochs, lr=lr, device=device)\n",
    "        return validation_loop(trial_model, val_loader, trial_neigh_model)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "    \n",
    "    first = True\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            n_users,\n",
    "                                                            n_actions,\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, 5, simulation_data, np.arange(5) + train_size, our_x)\n",
    "\n",
    "            bpr_model = BPRModel(\n",
    "                                n_users,\n",
    "                                n_actions,\n",
    "                                emb_x.shape[1], \n",
    "                                initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                                initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "            \n",
    "            cf_dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob[train_data['x_idx']]\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(cf_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            val_dataset =  CustomCFDataset(\n",
    "                            val_data['x_idx'], \n",
    "                            val_data['a'], \n",
    "                            val_data['r'], \n",
    "                            original_policy_prob[val_data['x_idx']]\n",
    "                            )\n",
    "            \n",
    "            val_loader = DataLoader(val_dataset, batch_size=len(val_data['r']), shuffle=True)\n",
    "            \n",
    "            if first:\n",
    "                policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "                conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "                conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "                conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "                bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "                reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "                reg_results.append(reg_dm)\n",
    "                first = False\n",
    "                reg_results = np.array(reg_results)\n",
    "                conv_results = np.array(conv_results)\n",
    "                results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "                reg_results, conv_results = [], []\n",
    "            \n",
    "            Bloss = BPRLoss()\n",
    "\n",
    "            \n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=10)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x, device=device), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a, device=device)\n",
    "                            )\n",
    "\n",
    "            train(model, train_loader, neighberhoodmodel, criterion=SNDRPolicyLoss(), num_epochs=best_params['num_epochs'], lr=best_params['lr'], device=device)\n",
    "            fit_bpr(bpr_model, Bloss, train_loader, num_epochs=3, lr=0.001, device=device)\n",
    "            # neighborhood_model.update(model.get_params()[0].detach().numpy(), model.get_params()[1].detach().numpy())'\n",
    "\n",
    "            our_x, our_a = model.get_params()\n",
    "            our_a, our_x = our_a.detach().cpu().numpy(), our_x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            bpr_scores = bpr_model.calc_scores(torch.tensor(train_data['x_idx'], device=device, dtype=torch.long)).detach().cpu().numpy()\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], bpr_scores)\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_a-our_a)**2)), np.sqrt(np.mean((original_a-our_a)**2))])\n",
    "\n",
    "            # temp.append(np.mean((emb_a-our_a)**2, axis=0))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.sqrt(np.mean((emb_x-our_x)**2)), np.sqrt(np.mean((original_x-our_x)**2))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.3 # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "num_rounds_list = [3, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 20:17:29,753] A new study created in memory with name: no-name-19238248-cdbb-4944-833b-7be6499b8ed4\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\n",
      "[W 2025-06-25 20:17:30,163] Trial 0 failed with parameters: {'lr': 1.0136228521369278e-05, 'num_epochs': 4} because of the following error: TypeError(\"run_train_loop() missing 1 required positional argument: 'criterion'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_9/1660379736.py\", line 36, in objective\n",
      "    train(trial_model, train_loader, trial_neigh_model, num_epochs=epochs, lr=lr, device=device)\n",
      "  File \"/code/training_utils.py\", line 61, in train\n",
      "    user_idx = user_idx.to(device)\n",
      "TypeError: run_train_loop() missing 1 required positional argument: 'criterion'\n",
      "[W 2025-06-25 20:17:30,164] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "run_train_loop() missing 1 required positional argument: 'criterion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df4 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 137\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size)\u001b[0m\n\u001b[1;32m    133\u001b[0m Bloss \u001b[38;5;241m=\u001b[39m BPRLoss()\n\u001b[1;32m    136\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m    140\u001b[0m neighberhoodmodel \u001b[38;5;241m=\u001b[39m NeighborhoodModel(\n\u001b[1;32m    141\u001b[0m                                         train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_idx\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    142\u001b[0m                                         train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m                                         num_neighbors\u001b[38;5;241m=\u001b[39mnum_neighbors\n\u001b[1;32m    147\u001b[0m                                     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/optuna/study/study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/optuna/study/_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/optuna/study/_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/optuna/study/_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[20], line 36\u001b[0m, in \u001b[0;36mtrainer_trial.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m trial_model \u001b[38;5;241m=\u001b[39m CFModel(\n\u001b[1;32m     28\u001b[0m                 n_users, \n\u001b[1;32m     29\u001b[0m                 n_actions, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 initial_actions_embeddings\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(our_a, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     33\u001b[0m                 )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_neigh_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validation_loop(trial_model, val_loader, trial_neigh_model)\n",
      "File \u001b[0;32m/code/training_utils.py:61\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, neighborhood_model, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m tq \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tq:\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mrun_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighborhood_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: run_train_loop() missing 1 required positional argument: 'criterion'"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1910</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.4467</td>\n",
       "      <td>0.5062</td>\n",
       "      <td>0.6211</td>\n",
       "      <td>0.3294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1841</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3006</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.5356</td>\n",
       "      <td>0.0477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "0           0.1815 0.0208  0.1541   0.0011   0.0300     0.0327   \n",
       "3           0.1910 0.0715  0.1424   0.0008   0.0148     0.0543   \n",
       "20          0.1841 0.0008  0.1515   0.0002   0.0002     0.0001   \n",
       "\n",
       "    action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0                0.3386        0.0000                0.5364         0.0000  \n",
       "3                0.4467        0.5062                0.6211         0.3294  \n",
       "20               0.3006        0.0600                0.5356         0.0477  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.001$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: -0.9690: 100%|██████████| 1/1 [00:00<00:00, 49.90it/s]\n",
      "Epoch [1/1], Loss: 103.7313: 100%|██████████| 1/1 [00:00<00:00, 48.08it/s]\n",
      "Epoch [1/1], Loss: -1.1012: 100%|██████████| 1/1 [00:00<00:00, 21.52it/s]\n",
      "Epoch [1/1], Loss: 103.8110: 100%|██████████| 1/1 [00:00<00:00, 21.84it/s]\n",
      "Epoch [1/1], Loss: -1.0617: 100%|██████████| 1/1 [00:00<00:00, 14.53it/s]\n",
      "Epoch [1/1], Loss: 103.8067: 100%|██████████| 1/1 [00:00<00:00, 15.78it/s]\n",
      "Epoch [1/1], Loss: -0.8680: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "Epoch [1/1], Loss: 103.9722: 100%|██████████| 1/1 [00:00<00:00, 10.33it/s]\n",
      "Epoch [1/1], Loss: -0.9874: 100%|██████████| 1/1 [00:00<00:00, 10.48it/s]\n",
      "Epoch [1/1], Loss: 103.7223: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
      "Epoch [1/1], Loss: -1.0314: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n",
      "Epoch [1/1], Loss: 103.8244: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
      "Epoch [1/1], Loss: -1.0509: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Epoch [1/1], Loss: 103.9615: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2599</td>\n",
       "      <td>1.0661</td>\n",
       "      <td>1.1906</td>\n",
       "      <td>1.0657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2601</td>\n",
       "      <td>1.0665</td>\n",
       "      <td>1.1911</td>\n",
       "      <td>1.0660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2606</td>\n",
       "      <td>1.0671</td>\n",
       "      <td>1.1914</td>\n",
       "      <td>1.0663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2610</td>\n",
       "      <td>1.0675</td>\n",
       "      <td>1.1921</td>\n",
       "      <td>1.0670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2613</td>\n",
       "      <td>1.0679</td>\n",
       "      <td>1.1921</td>\n",
       "      <td>1.0669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2638</td>\n",
       "      <td>1.0708</td>\n",
       "      <td>1.1945</td>\n",
       "      <td>1.0694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2685</td>\n",
       "      <td>1.0763</td>\n",
       "      <td>1.1984</td>\n",
       "      <td>1.0743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1           0.1453 0.0442  0.1382   0.0031   0.0210     0.0218   0.0000   \n",
       "2           0.1453 0.0123  0.1382   0.0241   0.0370     0.0309   0.0000   \n",
       "3           0.1453 0.0027  0.1382   0.0132   0.0154     0.0151   0.0000   \n",
       "4           0.1453 0.0002  0.1382   0.0102   0.0023     0.0031   0.0000   \n",
       "5           0.1453 0.0503  0.1382   0.0086   0.0502     0.0520   0.0000   \n",
       "10          0.1453 0.0201  0.1382   0.0065   0.0002     0.0000   0.0000   \n",
       "20          0.1453 0.0089  0.1382   0.0034   0.0115     0.0113   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1       0.0000       0.0000       0.0000         0.0000               1.2599   \n",
       "2       0.0000       0.0000       0.0000         0.0000               1.2601   \n",
       "3       0.0000       0.0000       0.0000         0.0000               1.2606   \n",
       "4       0.0000       0.0000       0.0000         0.0000               1.2610   \n",
       "5       0.0000       0.0000       0.0000         0.0000               1.2613   \n",
       "10      0.0000       0.0000       0.0000         0.0000               1.2638   \n",
       "20      0.0000       0.0000       0.0000         0.0000               1.2685   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.5364         0.0000  \n",
       "1         1.0661                1.1906         1.0657  \n",
       "2         1.0665                1.1911         1.0660  \n",
       "3         1.0671                1.1914         1.0663  \n",
       "4         1.0675                1.1921         1.0670  \n",
       "5         1.0679                1.1921         1.0669  \n",
       "10        1.0708                1.1945         1.0694  \n",
       "20        1.0763                1.1984         1.0743  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.003$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.0324: 100%|██████████| 10/10 [00:00<00:00, 51.08it/s]\n",
      "Epoch [10/10], Loss: 103.8724: 100%|██████████| 10/10 [00:00<00:00, 45.51it/s]\n",
      "Epoch [10/10], Loss: -1.2277: 100%|██████████| 10/10 [00:00<00:00, 24.38it/s]\n",
      "Epoch [10/10], Loss: 103.7001: 100%|██████████| 10/10 [00:00<00:00, 23.29it/s]\n",
      "Epoch [10/10], Loss: -1.2147: 100%|██████████| 10/10 [00:00<00:00, 16.48it/s]\n",
      "Epoch [10/10], Loss: 103.3702: 100%|██████████| 10/10 [00:00<00:00, 13.81it/s]\n",
      "Epoch [10/10], Loss: -1.0215: 100%|██████████| 10/10 [00:00<00:00, 11.39it/s]\n",
      "Epoch [10/10], Loss: 103.7327: 100%|██████████| 10/10 [00:00<00:00, 11.25it/s]\n",
      "Epoch [10/10], Loss: -1.2477: 100%|██████████| 10/10 [00:01<00:00,  9.45it/s]\n",
      "Epoch [10/10], Loss: 103.3463: 100%|██████████| 10/10 [00:01<00:00,  8.47it/s]\n",
      "Epoch [10/10], Loss: -1.5837: 100%|██████████| 10/10 [00:02<00:00,  4.87it/s]\n",
      "Epoch [10/10], Loss: 103.1211: 100%|██████████| 10/10 [00:02<00:00,  4.62it/s]\n",
      "Epoch [10/10], Loss: -5.8129: 100%|██████████| 10/10 [00:04<00:00,  2.27it/s]\n",
      "Epoch [10/10], Loss: 102.9916: 100%|██████████| 10/10 [00:04<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, num_epochs=10, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>1.0816</td>\n",
       "      <td>1.2032</td>\n",
       "      <td>1.0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2871</td>\n",
       "      <td>1.0995</td>\n",
       "      <td>1.2186</td>\n",
       "      <td>1.0893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.1382</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3095</td>\n",
       "      <td>1.1251</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>1.1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3261</td>\n",
       "      <td>1.1443</td>\n",
       "      <td>1.2600</td>\n",
       "      <td>1.1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3455</td>\n",
       "      <td>1.1664</td>\n",
       "      <td>1.2692</td>\n",
       "      <td>1.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4985</td>\n",
       "      <td>1.3407</td>\n",
       "      <td>1.3654</td>\n",
       "      <td>1.2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0338</td>\n",
       "      <td>1.9246</td>\n",
       "      <td>1.6279</td>\n",
       "      <td>1.6433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0           0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1           0.1451 0.0462  0.1381   0.0028   0.0221     0.0240   0.0000   \n",
       "2           0.1453 0.0158  0.1383   0.0246   0.0427     0.0336   0.0000   \n",
       "3           0.1453 0.0045  0.1382   0.0137   0.0115     0.0118   0.0000   \n",
       "4           0.1453 0.0006  0.1383   0.0117   0.0018     0.0006   0.0000   \n",
       "5           0.1449 0.0336  0.1377   0.0085   0.0359     0.0393   0.0000   \n",
       "10          0.1448 0.0236  0.1370   0.0037   0.0011     0.0013   0.0000   \n",
       "20          0.1476 0.0106  0.1411   0.0031   0.0055     0.0050   0.0000   \n",
       "\n",
       "    reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0       0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1       0.0000       0.0000       0.0000         0.0000               1.2730   \n",
       "2       0.0000       0.0000       0.0000         0.0000               1.2871   \n",
       "3       0.0000       0.0000       0.0000         0.0000               1.3095   \n",
       "4       0.0000       0.0000       0.0000         0.0000               1.3261   \n",
       "5       0.0000       0.0000       0.0000         0.0000               1.3455   \n",
       "10      0.0000       0.0000       0.0000         0.0000               1.4985   \n",
       "20      0.0000       0.0000       0.0000         0.0000               2.0338   \n",
       "\n",
       "    action_delta  context_diff_to_real  context_delta  \n",
       "0         0.0000                0.5364         0.0000  \n",
       "1         1.0816                1.2032         1.0708  \n",
       "2         1.0995                1.2186         1.0893  \n",
       "3         1.1251                1.2390         1.1124  \n",
       "4         1.1443                1.2600         1.1368  \n",
       "5         1.1664                1.2692         1.1500  \n",
       "10        1.3407                1.3654         1.2810  \n",
       "20        1.9246                1.6279         1.6433  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.05$$\n",
    "$$n_{epochs} = 10$$\n",
    "$$BatchSize=150$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: -1.7198: 100%|██████████| 10/10 [00:00<00:00, 101.46it/s]\n",
      "Epoch [10/10], Loss: 98.9210: 100%|██████████| 10/10 [00:00<00:00, 104.40it/s]\n",
      "Epoch [10/10], Loss: -3.0079: 100%|██████████| 10/10 [00:00<00:00, 52.63it/s]\n",
      "Epoch [10/10], Loss: 91.4421: 100%|██████████| 10/10 [00:00<00:00, 50.28it/s]\n",
      "Epoch [10/10], Loss: -4.7455: 100%|██████████| 10/10 [00:00<00:00, 35.61it/s]\n",
      "Epoch [10/10], Loss: 89.6301: 100%|██████████| 10/10 [00:00<00:00, 32.14it/s]\n",
      "Epoch [10/10], Loss: -5.4052: 100%|██████████| 10/10 [00:00<00:00, 26.85it/s]\n",
      "Epoch [10/10], Loss: 85.7448: 100%|██████████| 10/10 [00:00<00:00, 25.09it/s]\n"
     ]
    }
   ],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], train_dataset, batch_size+100, num_epochs=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>ipw_var</th>\n",
       "      <th>reg_dm_var</th>\n",
       "      <th>conv_dm_var</th>\n",
       "      <th>conv_dr_var</th>\n",
       "      <th>conv_sndr_var</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4746</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>1.3637</td>\n",
       "      <td>1.2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.1998</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7363</td>\n",
       "      <td>1.6121</td>\n",
       "      <td>1.5940</td>\n",
       "      <td>1.5158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.1355</td>\n",
       "      <td>2.0325</td>\n",
       "      <td>1.8826</td>\n",
       "      <td>1.8401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.1391</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5199</td>\n",
       "      <td>2.4330</td>\n",
       "      <td>2.1781</td>\n",
       "      <td>2.1835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  ipw_var  \\\n",
       "0          0.1815 0.0035  0.1541   0.0128   0.0317     0.0333   0.0000   \n",
       "1          0.1430 0.0497  0.1361   0.0009   0.0148     0.0294   0.0000   \n",
       "2          0.1449 0.1010  0.1397   0.0187   0.1998     0.1172   0.0000   \n",
       "3          0.1450 0.1332  0.1399   0.0072   0.1406     0.1160   0.0000   \n",
       "4          0.1486 0.0274  0.1391   0.0125   0.0855     0.1512   0.0000   \n",
       "\n",
       "   reg_dm_var  conv_dm_var  conv_dr_var  conv_sndr_var  action_diff_to_real  \\\n",
       "0      0.0000       0.0000       0.0000         0.0000               0.3386   \n",
       "1      0.0000       0.0000       0.0000         0.0000               1.4746   \n",
       "2      0.0000       0.0000       0.0000         0.0000               1.7363   \n",
       "3      0.0000       0.0000       0.0000         0.0000               2.1355   \n",
       "4      0.0000       0.0000       0.0000         0.0000               2.5199   \n",
       "\n",
       "   action_delta  context_diff_to_real  context_delta  \n",
       "0        0.0000                0.5364         0.0000  \n",
       "1        1.3141                1.3637         1.2353  \n",
       "2        1.6121                1.5940         1.5158  \n",
       "3        2.0325                1.8826         1.8401  \n",
       "4        2.4330                2.1781         2.1835  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
