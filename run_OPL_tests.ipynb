{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# import gym\n",
    "# import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")  # TF32 = big speedup on Ada\n",
    "\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "# from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward,\n",
    "    get_weights_info\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    LinearCFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    train,\n",
    "    validation_loop, \n",
    "    cv_score_model\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_results(\n",
    "    our_x, \n",
    "    our_a, \n",
    "    emb_x, \n",
    "    emb_a, \n",
    "    original_x, \n",
    "    original_a, \n",
    "    dataset, \n",
    "    val_data, \n",
    "    original_policy_prob, \n",
    "    neighberhoodmodel, \n",
    "    regression_model, \n",
    "    dm\n",
    "):\n",
    "    policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "    policy_reward = calc_reward(dataset, policy)\n",
    "    eval_metrics = eval_policy(neighberhoodmodel, val_data, original_policy_prob, policy)\n",
    "    action_diff_to_real = np.sqrt(np.mean((emb_a - our_a) ** 2))\n",
    "    action_delta = np.sqrt(np.mean((original_a - our_a) ** 2))\n",
    "    context_diff_to_real = np.sqrt(np.mean((emb_x - our_x) ** 2))\n",
    "    context_delta = np.sqrt(np.mean((original_x - our_x) ** 2))\n",
    "\n",
    "    row = np.concatenate([\n",
    "        np.atleast_1d(policy_reward),\n",
    "        np.atleast_1d(eval_metrics),\n",
    "        np.atleast_1d(action_diff_to_real),\n",
    "        np.atleast_1d(action_delta),\n",
    "        np.atleast_1d(context_diff_to_real),\n",
    "        np.atleast_1d(context_delta)\n",
    "    ])\n",
    "    reg_dm = dm.estimate_policy_value(policy[val_data['x_idx']], regression_model.predict(val_data['x']))\n",
    "    reg_results = np.array([reg_dm])\n",
    "    conv_results = np.array([row])\n",
    "    return get_opl_results_dict(reg_results, conv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "    num_runs,\n",
    "    num_neighbors,\n",
    "    train_sizes,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    val_size=2000,\n",
    "    n_trials=10,    \n",
    "    prev_best_params=None\n",
    "):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    def T(x):\n",
    "        return torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    def _mean_dict(dicts):\n",
    "        \"\"\"\n",
    "        Robust mean over a list of dicts with numeric/scalar/1D-array values.\n",
    "        Returns a single dict with elementwise means.\n",
    "        \"\"\"\n",
    "        if not dicts:\n",
    "            return {}\n",
    "        keys = dicts[0].keys()\n",
    "        out = {}\n",
    "        for k in keys:\n",
    "            vals = [d[k] for d in dicts if k in d]\n",
    "            # try to convert each to np.array and average\n",
    "            arrs = [np.asarray(v) for v in vals]\n",
    "            # broadcast to same shape if scalars/1D\n",
    "            stacked = np.stack(arrs, axis=0)\n",
    "            out[k] = np.mean(stacked, axis=0)\n",
    "        return out\n",
    "\n",
    "    # ===== unpack dataset (keep originals safe) =====\n",
    "    our_x_orig, our_a_orig = our_x, our_a\n",
    "    emb_x, emb_a = emb_x, emb_a\n",
    "    original_x, original_a = original_x, original_a\n",
    "    n_users, n_actions, emb_dim = n_users, n_actions, emb_dim\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "    best_hyperparams_by_size = {}\n",
    "    last_best_params = prev_best_params if prev_best_params is not None else None\n",
    "\n",
    "    # ===== baseline (sample size = 0) using get_trial_results =====\n",
    "    pi_0 = softmax(our_x_orig @ our_a_orig.T, axis=1)\n",
    "    original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "\n",
    "    simulation_data = create_simulation_data_from_pi(\n",
    "        dataset, pi_0, val_size, random_state=0\n",
    "    )\n",
    "\n",
    "    # use same data for train/val just to generate the baseline row\n",
    "    train_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size), our_x_orig)\n",
    "    val_data   = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size), our_x_orig)\n",
    "\n",
    "    regression_model = RegressionModel(\n",
    "        n_actions=n_actions, action_context=our_x_orig,\n",
    "        base_model=LogisticRegression(random_state=12345)\n",
    "    )\n",
    "\n",
    "    regression_model.fit(train_data['x'], train_data['a'], train_data['r'])\n",
    "\n",
    "    neighberhoodmodel = NeighborhoodModel(\n",
    "        train_data['x_idx'], train_data['a'],\n",
    "        our_a_orig, our_x_orig, train_data['r'],\n",
    "        num_neighbors=num_neighbors\n",
    "    )\n",
    "\n",
    "    # baseline row produced via get_trial_results\n",
    "    results[0] = get_trial_results(\n",
    "        our_x_orig, our_a_orig, emb_x, emb_a, original_x, original_a,\n",
    "        dataset, val_data, original_policy_prob,\n",
    "        neighberhoodmodel, regression_model, dm\n",
    "    )\n",
    "\n",
    "    # ===== main loop over training sizes =====\n",
    "    for train_size in train_sizes:\n",
    "\n",
    "        # we’ll collect per-run trial dicts generated by get_trial_results\n",
    "        trial_dicts_this_size = []\n",
    "        best_hyperparams_by_size[train_size] = {}\n",
    "\n",
    "        # --- prepare a resampling for Optuna’s objective (shared loaders built per-run inside objective) ---\n",
    "        # We’ll do Optuna per-run (fresh resample + search), then final fit with best params, then get_trial_results.\n",
    "\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            # --- resample for this run ---\n",
    "            pi_0 = softmax(our_x_orig @ our_a_orig.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                dataset, pi_0, train_size + val_size,\n",
    "                random_state=(run + 1) * (train_size + 17)\n",
    "            )\n",
    "\n",
    "            idx_train = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx_train, our_x_orig)\n",
    "            val_idx   = np.arange(val_size) + train_size\n",
    "            val_data  = get_train_data(n_actions, val_size, simulation_data, val_idx, our_x_orig)\n",
    "\n",
    "            num_workers = 4 if torch.cuda.is_available() else 0\n",
    "\n",
    "            cf_dataset = CustomCFDataset(\n",
    "                train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "            )\n",
    "\n",
    "            val_dataset = CustomCFDataset(\n",
    "                val_data['x_idx'], val_data['a'], val_data['r'], original_policy_prob\n",
    "            )\n",
    "\n",
    "            # val_loader = DataLoader(\n",
    "            #     val_dataset, batch_size=val_size, shuffle=False,\n",
    "            #     pin_memory=torch.cuda.is_available(),\n",
    "            #     num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            # )\n",
    "\n",
    "\n",
    "            # --- Optuna objective bound to this run's data ---\n",
    "            def objective(trial):\n",
    "                lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "                trial_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "                trial_num_neighbors = trial.suggest_int(\"num_neighbors\", 3, 15)\n",
    "                lr_decay = trial.suggest_float(\"lr_decay\", 0.8, 1.0)\n",
    "\n",
    "                trial_neigh_model = NeighborhoodModel(\n",
    "                    train_data['x_idx'], train_data['a'],\n",
    "                    our_a_orig, our_x_orig, train_data['r'],\n",
    "                    num_neighbors=trial_num_neighbors\n",
    "                )\n",
    "\n",
    "                trial_scores_all = torch.as_tensor(\n",
    "                    trial_neigh_model.predict(all_user_indices),\n",
    "                    device=device, dtype=torch.float32\n",
    "                )\n",
    "\n",
    "                trial_model = LinearCFModel(\n",
    "                    n_users, n_actions, emb_dim,\n",
    "                    initial_user_embeddings=T(our_x_orig),\n",
    "                    initial_actions_embeddings=T(our_a_orig)\n",
    "                ).to(device)\n",
    "\n",
    "                assert (not torch.cuda.is_available()) or next(trial_model.parameters()).is_cuda\n",
    "\n",
    "                final_train_loader = DataLoader(\n",
    "                    cf_dataset, batch_size=trial_batch_size, shuffle=True,\n",
    "                    pin_memory=torch.cuda.is_available(),\n",
    "                    num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "                )\n",
    "\n",
    "                current_lr = lr\n",
    "                for epoch in range(epochs):\n",
    "                    if epoch > 0:\n",
    "                        current_lr *= lr_decay\n",
    "                        \n",
    "                    train(\n",
    "                        trial_model, final_train_loader, trial_scores_all,\n",
    "                        criterion=SNDRPolicyLoss(), num_epochs=1, lr=current_lr, device=str(device)\n",
    "                    )\n",
    "\n",
    "                trial_x, trial_a = trial_model.get_params()\n",
    "                trial_x = trial_x.detach().cpu().numpy()\n",
    "                trial_a = trial_a.detach().cpu().numpy()\n",
    "\n",
    "                pi_i = softmax(trial_x @ trial_a.T, axis=1)\n",
    "\n",
    "                # print(get_weights_info(pi_i, original_policy_prob))\n",
    "                # validation reward for selection\n",
    "                return cv_score_model(val_data, trial_scores_all, pi_i)\n",
    "\n",
    "\n",
    "            # --- run Optuna for this run ---\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            \n",
    "            if last_best_params is not None:\n",
    "                study.enqueue_trial(last_best_params)\n",
    "\n",
    "            study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            last_best_params = best_params  # optional warm-start to next run\n",
    "            best_hyperparams_by_size[train_size][run] = {\n",
    "                \"params\": best_params,\n",
    "                \"reward\": study.best_value\n",
    "            }\n",
    "\n",
    "\n",
    "            # --- final training with best params on this run’s data ---\n",
    "            regression_model = RegressionModel(\n",
    "                n_actions=n_actions, action_context=our_x_orig,\n",
    "                base_model=LogisticRegression(random_state=12345)\n",
    "            )\n",
    "            regression_model.fit(\n",
    "                train_data['x'], train_data['a'], train_data['r'],\n",
    "                original_policy_prob[train_data['x_idx'], train_data['a']].squeeze()\n",
    "            )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a_orig, our_x_orig, train_data['r'],\n",
    "                num_neighbors=best_params['num_neighbors']\n",
    "            )\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            model = LinearCFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x_orig),\n",
    "                initial_actions_embeddings=T(our_a_orig)\n",
    "            ).to(device)\n",
    "            assert (not torch.cuda.is_available()) or next(model.parameters()).is_cuda\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            current_lr = best_params['lr']\n",
    "            for epoch in range(best_params['num_epochs']):\n",
    "                if epoch > 0:\n",
    "                    current_lr *= best_params['lr_decay']\n",
    "                train(\n",
    "                    model, train_loader, scores_all,\n",
    "                    criterion=SNDRPolicyLoss(), num_epochs=1, lr=current_lr, device=str(device)\n",
    "                )\n",
    "\n",
    "            # learned embeddings (do NOT overwrite originals)\n",
    "            learned_x_t, learned_a_t = model.get_params()\n",
    "            learned_x = learned_x_t.detach().cpu().numpy()\n",
    "            learned_a = learned_a_t.detach().cpu().numpy()\n",
    "\n",
    "            # --- produce the per-run result via get_trial_results ---\n",
    "            trial_res = get_trial_results(\n",
    "                learned_x, learned_a,          # learned (policy) embeddings\n",
    "                emb_x, emb_a,                  # ground-truth embedding refs\n",
    "                original_x, original_a,        # original clean refs\n",
    "                dataset,\n",
    "                val_data,                      # use this run's val split\n",
    "                original_policy_prob,\n",
    "                neighberhoodmodel,\n",
    "                regression_model,\n",
    "                dm\n",
    "            )\n",
    "\n",
    "            trial_dicts_this_size.append(trial_res)\n",
    "\n",
    "            # memory hygiene\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # === aggregate per-run results (mean) and store under this train_size ===\n",
    "        results[train_size] = _mean_dict(trial_dicts_this_size)\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index'), best_hyperparams_by_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Item CTR: 0.07066414727263938\n",
      "Optimal greedy CTR: 0.09999926940951757\n",
      "Optimal Stochastic CTR: 0.09995326955796031\n",
      "Our Initial CTR: 0.08610747363354625\n"
     ]
    }
   ],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.1\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "batch_size = 200\n",
    "num_neighbors = 6\n",
    "n_trials_for_optuna = 10\n",
    "num_rounds_list = [500, 1000, 2000, 10000, 20000]\n",
    "# num_rounds_list = [20000]\n",
    "\n",
    "\n",
    "# Manually define your best parameters\n",
    "best_params_to_use = {\n",
    "    \"lr\": 0.0095,  # Learning rate\n",
    "    \"num_epochs\": 5,  # Number of training epochs\n",
    "    \"batch_size\": 64,  # Batch size for training\n",
    "    \"num_neighbors\": 8,  # Number of neighbors for neighborhood model\n",
    "    \"lr_decay\": 0.85  # Learning rate decay factor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of num_rounds_list: [500, 1000, 2000, 10000, 20000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-11 18:54:30,352] A new study created in memory with name: no-name-bd3d0adb-2e99-40fd-988e-459f5f2f5a66\n",
      "Best trial: 0. Best value: 0.0856866:  10%|█         | 1/10 [00:02<00:23,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.5243697746036257), 'ess': np.float64(4979.857516611228), 'max_wi': np.float64(5.42511752938691), 'min_wi': np.float64(0.01962761801119335)}\n",
      "Cross-validated error: 0.008975978922104751\n",
      "[I 2025-10-11 18:54:32,983] Trial 0 finished with value: 0.0856866120785302 and parameters: {'lr': 0.0095, 'num_epochs': 5, 'batch_size': 64, 'num_neighbors': 8, 'lr_decay': 0.85}. Best is trial 0 with value: 0.0856866120785302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.0904157:  20%|██        | 2/10 [00:04<00:16,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.005872964376626676), 'ess': np.float64(9998.956446285789), 'max_wi': np.float64(1.028189840753053), 'min_wi': np.float64(0.9671353831446523)}\n",
      "Cross-validated error: 0.010125484558585925\n",
      "[I 2025-10-11 18:54:34,599] Trial 1 finished with value: 0.0904157242591587 and parameters: {'lr': 0.00018709859912402802, 'num_epochs': 7, 'batch_size': 256, 'num_neighbors': 13, 'lr_decay': 0.9372089833344932}. Best is trial 1 with value: 0.0904157242591587.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0918008:  30%|███       | 3/10 [00:05<00:12,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.05332732258328814), 'ess': np.float64(9914.243972146633), 'max_wi': np.float64(1.2786191387730608), 'min_wi': np.float64(0.770492536029325)}\n",
      "Cross-validated error: 0.010468006645507729\n",
      "[I 2025-10-11 18:54:36,226] Trial 2 finished with value: 0.09180080882331923 and parameters: {'lr': 0.0009832292778037953, 'num_epochs': 8, 'batch_size': 128, 'num_neighbors': 6, 'lr_decay': 0.9166093307143252}. Best is trial 2 with value: 0.09180080882331923.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0918008:  40%|████      | 4/10 [00:07<00:10,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.0014367616341164368), 'ess': np.float64(9999.937162340399), 'max_wi': np.float64(1.0062946593360196), 'min_wi': np.float64(0.9930561406377443)}\n",
      "Cross-validated error: 0.010194265484445663\n",
      "[I 2025-10-11 18:54:37,791] Trial 3 finished with value: 0.09076465301861127 and parameters: {'lr': 0.0001434749010738803, 'num_epochs': 3, 'batch_size': 512, 'num_neighbors': 10, 'lr_decay': 0.9228945262208109}. Best is trial 2 with value: 0.09180080882331923.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.0920474:  50%|█████     | 5/10 [00:09<00:08,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.0031308780847601), 'ess': np.float64(9999.701540969014), 'max_wi': np.float64(1.0138645997112785), 'min_wi': np.float64(0.9850072177442966)}\n",
      "Cross-validated error: 0.010521598883516017\n",
      "[I 2025-10-11 18:54:39,353] Trial 4 finished with value: 0.09204742781275198 and parameters: {'lr': 0.0001443272615268543, 'num_epochs': 6, 'batch_size': 512, 'num_neighbors': 7, 'lr_decay': 0.9989489873440737}. Best is trial 4 with value: 0.09204742781275198.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.0920474:  60%|██████    | 6/10 [00:10<00:06,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.011870625323873603), 'ess': np.float64(9995.749171692998), 'max_wi': np.float64(1.0567141247476532), 'min_wi': np.float64(0.9399745357143868)}\n",
      "Cross-validated error: 0.01048897519594782\n",
      "[I 2025-10-11 18:54:40,977] Trial 5 finished with value: 0.09184863332652407 and parameters: {'lr': 0.0005774132980352496, 'num_epochs': 5, 'batch_size': 256, 'num_neighbors': 3, 'lr_decay': 0.8496933057061515}. Best is trial 4 with value: 0.09204742781275198.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.0920474:  70%|███████   | 7/10 [00:12<00:04,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.0012095743184314946), 'ess': np.float64(9999.955764572174), 'max_wi': np.float64(1.0056424239255024), 'min_wi': np.float64(0.9932442325438832)}\n",
      "Cross-validated error: 0.010112819850751903\n",
      "[I 2025-10-11 18:54:42,624] Trial 6 finished with value: 0.09045400505517123 and parameters: {'lr': 0.00020584108797785374, 'num_epochs': 1, 'batch_size': 256, 'num_neighbors': 12, 'lr_decay': 0.8558585654691033}. Best is trial 4 with value: 0.09204742781275198.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.0920474:  80%|████████  | 8/10 [00:13<00:03,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.024693670307783248), 'ess': np.float64(9981.37317479054), 'max_wi': np.float64(1.1295923616190813), 'min_wi': np.float64(0.8887776129375236)}\n",
      "Cross-validated error: 0.010135584874367581\n",
      "[I 2025-10-11 18:54:44,303] Trial 7 finished with value: 0.09052758633798341 and parameters: {'lr': 0.0005295398710204488, 'num_epochs': 4, 'batch_size': 64, 'num_neighbors': 10, 'lr_decay': 0.8101628418554027}. Best is trial 4 with value: 0.09204742781275198.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.0920474:  90%|█████████ | 9/10 [00:15<00:01,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.002256598073497488), 'ess': np.float64(9999.844780708445), 'max_wi': np.float64(1.010696556399499), 'min_wi': np.float64(0.9865985929055855)}\n",
      "Cross-validated error: 0.01048839528657389\n",
      "[I 2025-10-11 18:54:45,883] Trial 8 finished with value: 0.09180112565771675 and parameters: {'lr': 0.00013900262708385646, 'num_epochs': 2, 'batch_size': 128, 'num_neighbors': 3, 'lr_decay': 0.8174947625080651}. Best is trial 4 with value: 0.09204742781275198.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.0920474: 100%|██████████| 10/10 [00:17<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.349324864002174), 'ess': np.float64(7104.1847892717415), 'max_wi': np.float64(3.8727956792412686), 'min_wi': np.float64(0.10712411157975206)}\n",
      "Cross-validated error: 0.009427994483654492\n",
      "[I 2025-10-11 18:54:47,550] Trial 9 finished with value: 0.08770407568959147 and parameters: {'lr': 0.005215192549880918, 'num_epochs': 5, 'batch_size': 64, 'num_neighbors': 10, 'lr_decay': 0.9043090530110316}. Best is trial 4 with value: 0.09204742781275198.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-10-11 18:54:50,038] A new study created in memory with name: no-name-49070b65-3129-43e9-ad7d-b9c17ac17374\n",
      "Best trial: 0. Best value: 0.0844128:  10%|█         | 1/10 [00:02<00:22,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.004676878981958937), 'ess': np.float64(9999.339925905444), 'max_wi': np.float64(1.0195179426605883), 'min_wi': np.float64(0.9783698953331155)}\n",
      "Cross-validated error: 0.00867939228362645\n",
      "[I 2025-10-11 18:54:52,584] Trial 0 finished with value: 0.08441276020108382 and parameters: {'lr': 0.0001443272615268543, 'num_epochs': 6, 'batch_size': 512, 'num_neighbors': 7, 'lr_decay': 0.9989489873440737}. Best is trial 0 with value: 0.08441276020108382.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.085097:  20%|██        | 2/10 [00:05<00:21,  2.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.0071551603716698735), 'ess': np.float64(9998.400668263364), 'max_wi': np.float64(1.0381303079844417), 'min_wi': np.float64(0.9708183454881437)}\n",
      "Cross-validated error: 0.008832689529478823\n",
      "[I 2025-10-11 18:54:55,350] Trial 1 finished with value: 0.08509701951782245 and parameters: {'lr': 0.00015436758037102436, 'num_epochs': 4, 'batch_size': 128, 'num_neighbors': 11, 'lr_decay': 0.8636353165769655}. Best is trial 1 with value: 0.08509701951782245.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0892348:  30%|███       | 3/10 [00:08<00:18,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.43214185903619523), 'ess': np.float64(5911.670677036117), 'max_wi': np.float64(4.876918918311484), 'min_wi': np.float64(0.060521387910777644)}\n",
      "Cross-validated error: 0.009806552140314173\n",
      "[I 2025-10-11 18:54:58,064] Trial 2 finished with value: 0.08923475626623324 and parameters: {'lr': 0.00962507130082993, 'num_epochs': 3, 'batch_size': 128, 'num_neighbors': 3, 'lr_decay': 0.9147941106089159}. Best is trial 2 with value: 0.08923475626623324.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0892348:  40%|████      | 4/10 [00:10<00:16,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.0022327022846622296), 'ess': np.float64(9999.849551025778), 'max_wi': np.float64(1.0095927526605486), 'min_wi': np.float64(0.9900409189294476)}\n",
      "Cross-validated error: 0.008905382832946373\n",
      "[I 2025-10-11 18:55:00,826] Trial 3 finished with value: 0.08545989888097782 and parameters: {'lr': 0.0001156701055532078, 'num_epochs': 4, 'batch_size': 512, 'num_neighbors': 13, 'lr_decay': 0.9328331754045914}. Best is trial 2 with value: 0.08923475626623324.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0892348:  50%|█████     | 5/10 [00:13<00:13,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.09161580021902524), 'ess': np.float64(9753.350151256796), 'max_wi': np.float64(1.3977536904425931), 'min_wi': np.float64(0.6045518275920806)}\n",
      "Cross-validated error: 0.00893230038555786\n",
      "[I 2025-10-11 18:55:03,536] Trial 4 finished with value: 0.08557672936672825 and parameters: {'lr': 0.008631267424009164, 'num_epochs': 2, 'batch_size': 512, 'num_neighbors': 13, 'lr_decay': 0.8714811742043391}. Best is trial 2 with value: 0.08923475626623324.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0892348:  60%|██████    | 6/10 [00:16<00:11,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gini': np.float64(0.260578892311878), 'ess': np.float64(8225.043873766894), 'max_wi': np.float64(2.6565327985495317), 'min_wi': np.float64(0.25472151116614433)}\n",
      "Cross-validated error: 0.009465633371307864\n",
      "[I 2025-10-11 18:55:06,539] Trial 5 finished with value: 0.0877977913192468 and parameters: {'lr': 0.004965652129458273, 'num_epochs': 6, 'batch_size': 256, 'num_neighbors': 13, 'lr_decay': 0.9720007454543466}. Best is trial 2 with value: 0.08923475626623324.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.0892348:  60%|██████    | 6/10 [00:17<00:11,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-10-11 18:55:07,784] Trial 6 failed with parameters: {'lr': 0.005695439892982343, 'num_epochs': 2, 'batch_size': 512, 'num_neighbors': 3, 'lr_decay': 0.8320351523709059} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/roee/Documents/code/OPC/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_621058/4006468808.py\", line 144, in objective\n",
      "    trial_neigh_model = NeighborhoodModel(\n",
      "                        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/roee/Documents/code/OPC/models.py\", line 37, in __init__\n",
      "    self.fit(action_emb, context_emb, actions, context, rewards)\n",
      "  File \"/home/roee/Documents/code/OPC/models.py\", line 44, in fit\n",
      "    self.calculate_scores()\n",
      "  File \"/home/roee/Documents/code/OPC/models.py\", line 63, in calculate_scores\n",
      "    self.scores = self.context_convolve(context)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/roee/Documents/code/OPC/models.py\", line 98, in context_convolve\n",
      "    eta_all = self.convolve(all_actions, all_context)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/roee/Documents/code/OPC/models.py\", line 80, in convolve\n",
      "    chunk_top_k_idx = np.argpartition(tot_cosine, -self.num_neighbors, axis=1)[:, -self.num_neighbors:]\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/roee/Documents/code/OPC/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py\", line 962, in argpartition\n",
      "    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/roee/Documents/code/OPC/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py\", line 57, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-11 18:55:07,786] Trial 6 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValue of num_rounds_list:\u001b[39m\u001b[33m\"\u001b[39m, num_rounds_list)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Run the optimization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df4, best_hyperparams_by_size = \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials_for_optuna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_best_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_params_to_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Print best hyperparameters for each training size\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 196\u001b[39m, in \u001b[36mtrainer_trial\u001b[39m\u001b[34m(num_runs, num_neighbors, train_sizes, dataset, batch_size, val_size, n_trials, prev_best_params)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m last_best_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    194\u001b[39m     study.enqueue_trial(last_best_params)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m best_params = study.best_params\n\u001b[32m    199\u001b[39m last_best_params = best_params  \u001b[38;5;66;03m# optional warm-start to next run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/.venv/lib/python3.12/site-packages/optuna/study/study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mtrainer_trial.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    141\u001b[39m trial_num_neighbors = trial.suggest_int(\u001b[33m\"\u001b[39m\u001b[33mnum_neighbors\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m15\u001b[39m)\n\u001b[32m    142\u001b[39m lr_decay = trial.suggest_float(\u001b[33m\"\u001b[39m\u001b[33mlr_decay\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.8\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m trial_neigh_model = \u001b[43mNeighborhoodModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx_idx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mour_a_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mour_x_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial_num_neighbors\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m trial_scores_all = torch.as_tensor(\n\u001b[32m    151\u001b[39m     trial_neigh_model.predict(all_user_indices),\n\u001b[32m    152\u001b[39m     device=device, dtype=torch.float32\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m trial_model = LinearCFModel(\n\u001b[32m    156\u001b[39m     n_users, n_actions, emb_dim,\n\u001b[32m    157\u001b[39m     initial_user_embeddings=T(our_x_orig),\n\u001b[32m    158\u001b[39m     initial_actions_embeddings=T(our_a_orig)\n\u001b[32m    159\u001b[39m ).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/models.py:37\u001b[39m, in \u001b[36mNeighborhoodModel.__init__\u001b[39m\u001b[34m(self, context, actions, action_emb, context_emb, rewards, num_neighbors, chunksize, gamma)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.num_neighbors = num_neighbors\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.chunksize = chunksize\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/models.py:44\u001b[39m, in \u001b[36mNeighborhoodModel.fit\u001b[39m\u001b[34m(self, action_emb, context_emb, actions, context, rewards)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.context = np.int32(context)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mself\u001b[39m.reward = np.float32(rewards)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/models.py:63\u001b[39m, in \u001b[36mNeighborhoodModel.calculate_scores\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_scores\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     62\u001b[39m     context = np.arange(\u001b[38;5;28mself\u001b[39m.context_similarity.shape[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28mself\u001b[39m.scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext_convolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/models.py:98\u001b[39m, in \u001b[36mNeighborhoodModel.context_convolve\u001b[39m\u001b[34m(self, test_context)\u001b[39m\n\u001b[32m     95\u001b[39m all_actions = np.arange(\u001b[38;5;28mself\u001b[39m.action_similarity.shape[\u001b[32m0\u001b[39m]).reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) @ np.ones((\u001b[32m1\u001b[39m, test_context.shape[\u001b[32m0\u001b[39m]))\n\u001b[32m     96\u001b[39m all_actions = all_actions.T.flatten()\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m eta_all = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m eta_all = eta_all.reshape(test_context.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.action_similarity.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m eta_all\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/models.py:80\u001b[39m, in \u001b[36mNeighborhoodModel.convolve\u001b[39m\u001b[34m(self, test_actions, test_context)\u001b[39m\n\u001b[32m     76\u001b[39m tot_cosine = \u001b[38;5;28mself\u001b[39m.gamma * cosine_actions + (\u001b[32m1\u001b[39m - \u001b[38;5;28mself\u001b[39m.gamma) * cosine_context\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m cosine_context, cosine_actions\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m chunk_top_k_idx = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtot_cosine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:, -\u001b[38;5;28mself\u001b[39m.num_neighbors:]\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Get similarity and rewards for each example in the chunk\u001b[39;00m\n\u001b[32m     83\u001b[39m similarity = tot_cosine[np.arange(chunk_end - chunk_start)[:, \u001b[38;5;28;01mNone\u001b[39;00m], chunk_top_k_idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:962\u001b[39m, in \u001b[36margpartition\u001b[39m\u001b[34m(a, kth, axis, kind, order)\u001b[39m\n\u001b[32m    876\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34margpartition\u001b[39m(a, kth, axis=-\u001b[32m1\u001b[39m, kind=\u001b[33m'\u001b[39m\u001b[33mintroselect\u001b[39m\u001b[33m'\u001b[39m, order=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    878\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[32m    880\u001b[39m \u001b[33;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    960\u001b[39m \n\u001b[32m    961\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margpartition\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/OPC/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Value of num_rounds_list:\", num_rounds_list)\n",
    "\n",
    "# Run the optimization\n",
    "df4, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=10000, n_trials=n_trials_for_optuna, prev_best_params=best_params_to_use)\n",
    "\n",
    "# Print best hyperparameters for each training size\n",
    "print(\"\\n=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\")\n",
    "for train_size, params in best_hyperparams_by_size.items():\n",
    "    print(f\"\\nTraining Size: {train_size}\")\n",
    "    # print(f\"Best Reward: {params['reward']:.6f}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, value in params['params'].items():\n",
    "        print(f\"  {param_name}: {value}\")\n",
    "print(\"===========================\\n\")\n",
    "\n",
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.08610747</td>\n",
       "      <td>0.08611765</td>\n",
       "      <td>0.08751241</td>\n",
       "      <td>0.09178279</td>\n",
       "      <td>0.09119538</td>\n",
       "      <td>0.08897455</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.08610803</td>\n",
       "      <td>0.08518550</td>\n",
       "      <td>0.10581378</td>\n",
       "      <td>0.12150808</td>\n",
       "      <td>0.11026441</td>\n",
       "      <td>0.06799217</td>\n",
       "      <td>0.75692486</td>\n",
       "      <td>0.00025455</td>\n",
       "      <td>0.87627243</td>\n",
       "      <td>0.00012422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.08654208</td>\n",
       "      <td>0.08515550</td>\n",
       "      <td>0.08791228</td>\n",
       "      <td>0.09011070</td>\n",
       "      <td>0.09101994</td>\n",
       "      <td>0.09363095</td>\n",
       "      <td>0.76531032</td>\n",
       "      <td>0.12496686</td>\n",
       "      <td>0.87972439</td>\n",
       "      <td>0.05553527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.08734578</td>\n",
       "      <td>0.10292059</td>\n",
       "      <td>0.08114754</td>\n",
       "      <td>0.08605725</td>\n",
       "      <td>0.09373308</td>\n",
       "      <td>0.10895540</td>\n",
       "      <td>0.78380759</td>\n",
       "      <td>0.21309293</td>\n",
       "      <td>0.89112363</td>\n",
       "      <td>0.09057986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.08616457</td>\n",
       "      <td>0.08526466</td>\n",
       "      <td>0.08415045</td>\n",
       "      <td>0.08838887</td>\n",
       "      <td>0.08738445</td>\n",
       "      <td>0.08521387</td>\n",
       "      <td>0.75946979</td>\n",
       "      <td>0.05993718</td>\n",
       "      <td>0.87631002</td>\n",
       "      <td>0.02535974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>0.08658768</td>\n",
       "      <td>0.10000923</td>\n",
       "      <td>0.08665215</td>\n",
       "      <td>0.09042937</td>\n",
       "      <td>0.09040613</td>\n",
       "      <td>0.09037048</td>\n",
       "      <td>0.76479957</td>\n",
       "      <td>0.09941219</td>\n",
       "      <td>0.88121865</td>\n",
       "      <td>0.04027442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0          0.08610747 0.08611765 0.08751241 0.09178279 0.09119538 0.08897455   \n",
       "500        0.08610803 0.08518550 0.10581378 0.12150808 0.11026441 0.06799217   \n",
       "1000       0.08654208 0.08515550 0.08791228 0.09011070 0.09101994 0.09363095   \n",
       "2000       0.08734578 0.10292059 0.08114754 0.08605725 0.09373308 0.10895540   \n",
       "10000      0.08616457 0.08526466 0.08415045 0.08838887 0.08738445 0.08521387   \n",
       "20000      0.08658768 0.10000923 0.08665215 0.09042937 0.09040613 0.09037048   \n",
       "\n",
       "       action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0               0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "500             0.75692486    0.00025455            0.87627243     0.00012422  \n",
       "1000            0.76531032    0.12496686            0.87972439     0.05553527  \n",
       "2000            0.78380759    0.21309293            0.89112363     0.09057986  \n",
       "10000           0.75946979    0.05993718            0.87631002     0.02535974  \n",
       "20000           0.76479957    0.09941219            0.88121865     0.04027442  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.08610747</td>\n",
       "      <td>0.10697090</td>\n",
       "      <td>0.09051612</td>\n",
       "      <td>0.09112201</td>\n",
       "      <td>0.09452505</td>\n",
       "      <td>0.10672373</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.08705604</td>\n",
       "      <td>0.09221834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08299331</td>\n",
       "      <td>0.08198609</td>\n",
       "      <td>0.07599146</td>\n",
       "      <td>0.79170973</td>\n",
       "      <td>0.24615559</td>\n",
       "      <td>0.88427728</td>\n",
       "      <td>0.08661758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.08939145</td>\n",
       "      <td>0.11301958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08679147</td>\n",
       "      <td>0.09052395</td>\n",
       "      <td>0.10668506</td>\n",
       "      <td>1.01853061</td>\n",
       "      <td>0.76340735</td>\n",
       "      <td>0.91464524</td>\n",
       "      <td>0.19321758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.09251861</td>\n",
       "      <td>0.10603409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09028676</td>\n",
       "      <td>0.15397776</td>\n",
       "      <td>0.10628439</td>\n",
       "      <td>1.73862067</td>\n",
       "      <td>1.70789298</td>\n",
       "      <td>0.99652312</td>\n",
       "      <td>0.34170287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.09268524</td>\n",
       "      <td>0.09704712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09829317</td>\n",
       "      <td>0.09539229</td>\n",
       "      <td>0.09257621</td>\n",
       "      <td>2.18938809</td>\n",
       "      <td>2.22344507</td>\n",
       "      <td>1.03555944</td>\n",
       "      <td>0.40151858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>0.09264639</td>\n",
       "      <td>0.09493701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09216787</td>\n",
       "      <td>0.09134166</td>\n",
       "      <td>0.09056984</td>\n",
       "      <td>2.21242505</td>\n",
       "      <td>2.24879912</td>\n",
       "      <td>1.03520993</td>\n",
       "      <td>0.40065441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0          0.08610747 0.10697090 0.09051612 0.09112201 0.09452505 0.10672373   \n",
       "500        0.08705604 0.09221834        NaN 0.08299331 0.08198609 0.07599146   \n",
       "1000       0.08939145 0.11301958        NaN 0.08679147 0.09052395 0.10668506   \n",
       "2000       0.09251861 0.10603409        NaN 0.09028676 0.15397776 0.10628439   \n",
       "10000      0.09268524 0.09704712        NaN 0.09829317 0.09539229 0.09257621   \n",
       "20000      0.09264639 0.09493701        NaN 0.09216787 0.09134166 0.09056984   \n",
       "\n",
       "       action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0               0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "500             0.79170973    0.24615559            0.88427728     0.08661758  \n",
       "1000            1.01853061    0.76340735            0.91464524     0.19321758  \n",
       "2000            1.73862067    1.70789298            0.99652312     0.34170287  \n",
       "10000           2.18938809    2.22344507            1.03555944     0.40151858  \n",
       "20000           2.21242505    2.24879912            1.03520993     0.40065441  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
