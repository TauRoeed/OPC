{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/code\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# import gym\n",
    "# import recogym\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")  # TF32 = big speedup on Ada\n",
    "\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "import optuna\n",
    "# from memory_profiler import profile\n",
    "\n",
    "\n",
    "from estimators import (\n",
    "    DirectMethod as DM\n",
    ")\n",
    "\n",
    "from simulation_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    get_opl_results_dict,\n",
    "    CustomCFDataset,\n",
    "    calc_reward\n",
    ")\n",
    "\n",
    "from models import (    \n",
    "    CFModel,\n",
    "    NeighborhoodModel,\n",
    "    BPRModel, \n",
    "    RegressionModel\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    fit_bpr,\n",
    "    train,\n",
    "    validation_loop\n",
    " )\n",
    "\n",
    "from custom_losses import (\n",
    "    SNDRPolicyLoss,\n",
    "    BPRLoss\n",
    "    )\n",
    "\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trainer_trial` Function\n",
    "\n",
    "This function runs policy learning experiments using offline bandit data and evaluates various estimators.\n",
    "\n",
    "### Parameters\n",
    "- **num_runs** (int): Number of experimental runs per training size\n",
    "- **num_neighbors** (int): Number of neighbors to consider in the neighborhood model\n",
    "- **num_rounds_list** (list): List of training set sizes to evaluate\n",
    "- **dataset** (dict): Contains dataset information including embeddings, action probabilities, and reward probabilities\n",
    "- **batch_size** (int): Batch size for training the policy model\n",
    "- **num_epochs** (int): Number of training epochs for each experiment\n",
    "- **lr** (float, default=0.001): Learning rate for the optimizer\n",
    "\n",
    "### Process Flow\n",
    "1. Initializes result structures and retrieval models\n",
    "2. For each training size in `num_rounds_list`:\n",
    "   - Creates a uniform logging policy and simulates data\n",
    "   - Generates training data for offline learning\n",
    "   - Fits regression and neighborhood models for reward estimation\n",
    "   - Initializes and trains a counterfactual policy model\n",
    "   - Evaluates policy performance using various estimators\n",
    "   - Collects metrics on policy reward and embedding quality\n",
    "\n",
    "### Returns\n",
    "- **DataFrame**: Results table with rows indexed by training size and columns for various metrics:\n",
    "  - `policy_rewards`: True expected reward of the learned policy\n",
    "  - Various estimator errors (`ipw`, `reg_dm`, `conv_dm`, `conv_dr`, `conv_sndr`)\n",
    "  - Variance metrics for each estimator\n",
    "  - Embedding quality metrics comparing learned representations to ground truth\n",
    "\n",
    "### Implementation Notes\n",
    "- Uses uniform random logging policy for collecting offline data\n",
    "- Employs Self-Normalized Doubly Robust (SNDR) policy learning\n",
    "- Measures embedding quality via RMSE to original/ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "    num_runs,\n",
    "    num_neighbors,\n",
    "    num_rounds_list,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    val_size=2000,\n",
    "    n_trials=10,    \n",
    "    prev_best_params=None\n",
    "):\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "\n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "\n",
    "    all_user_indices = np.arange(n_users, dtype=np.int64)\n",
    "\n",
    "    def T(x):\n",
    "        return torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    best_hyperparams_by_size = {}\n",
    "    best_reward = -float('inf')\n",
    "    overall_best_params = {}\n",
    "\n",
    "    last_best_params = prev_best_params if prev_best_params is not None else None\n",
    "\n",
    "     # ---- Add baseline row for sample size = 0 ----\n",
    "    pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "    original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "    # Use a dummy simulation for baseline\n",
    "    simulation_data = create_simulation_data_from_pi(\n",
    "        dataset, pi_0, val_size, random_state=0\n",
    "    )\n",
    "\n",
    "    train_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size), our_x)\n",
    "    val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size), our_x)\n",
    "\n",
    "    regression_model = RegressionModel(\n",
    "        n_actions=n_actions, action_context=our_x,\n",
    "        base_model=LogisticRegression(random_state=12345)\n",
    "    )\n",
    "    regression_model.fit(train_data['x'], train_data['a'], train_data['r'])\n",
    "\n",
    "    neighberhoodmodel = NeighborhoodModel(\n",
    "        train_data['x_idx'], train_data['a'],\n",
    "        our_a, our_x, train_data['r'],\n",
    "        num_neighbors=num_neighbors\n",
    "    )\n",
    "    scores_all = torch.as_tensor(\n",
    "        neighberhoodmodel.predict(all_user_indices),\n",
    "        device=device, dtype=torch.float32\n",
    "    )\n",
    "    model = CFModel(\n",
    "        n_users, n_actions, emb_dim,\n",
    "        initial_user_embeddings=T(our_x),\n",
    "        initial_actions_embeddings=T(our_a)\n",
    "    ).to(device)\n",
    "\n",
    "    policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "    policy_reward = calc_reward(dataset, policy)\n",
    "    eval_metrics = eval_policy(neighberhoodmodel, val_data, original_policy_prob, policy)\n",
    "    action_diff_to_real = np.sqrt(np.mean((emb_a - our_a) ** 2))\n",
    "    action_delta = np.sqrt(np.mean((original_a - our_a) ** 2))\n",
    "    context_diff_to_real = np.sqrt(np.mean((emb_x - our_x) ** 2))\n",
    "    context_delta = np.sqrt(np.mean((original_x - our_x) ** 2))\n",
    "\n",
    "    row = np.concatenate([\n",
    "        np.atleast_1d(policy_reward),\n",
    "        np.atleast_1d(eval_metrics),\n",
    "        np.atleast_1d(action_diff_to_real),\n",
    "        np.atleast_1d(action_delta),\n",
    "        np.atleast_1d(context_diff_to_real),\n",
    "        np.atleast_1d(context_delta)\n",
    "    ])\n",
    "    reg_dm = dm.estimate_policy_value(policy[val_data['x_idx']], regression_model.predict(val_data['x']))\n",
    "    reg_results = np.array([reg_dm])\n",
    "    conv_results = np.array([row])\n",
    "    results[0] = get_opl_results_dict(reg_results, conv_results)\n",
    "\n",
    "    # ---- Main training size loop ----\n",
    "    for train_size in num_rounds_list:\n",
    "        # Generate initial data for Optuna search\n",
    "\n",
    "        pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "        original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "        simulation_data = create_simulation_data_from_pi(\n",
    "            dataset, pi_0, train_size + val_size,\n",
    "            random_state=train_size\n",
    "        )\n",
    "\n",
    "        idx = np.arange(train_size)\n",
    "        train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "        val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "        num_workers = 4 if torch.cuda.is_available() else 0\n",
    "        cf_dataset = CustomCFDataset(\n",
    "            train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "        )\n",
    "\n",
    "        val_dataset = CustomCFDataset(\n",
    "            val_data['x_idx'], val_data['a'], val_data['r'], original_policy_prob\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=val_size, shuffle=False,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "        )\n",
    "\n",
    "        # Define Optuna objective inside the loop so it can access train_data and cf_dataset\n",
    "        def objective(trial):\n",
    "            lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "            epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "            trial_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "            trial_num_neighbors = trial.suggest_int(\"num_neighbors\", 3, 15)\n",
    "            lr_decay = trial.suggest_float(\"lr_decay\", 0.8, 1.0)\n",
    "\n",
    "            trial_neigh_model = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=trial_num_neighbors\n",
    "            )\n",
    "\n",
    "            trial_scores_all = torch.as_tensor(\n",
    "                trial_neigh_model.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            trial_model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "\n",
    "            assert (not torch.cuda.is_available()) or next(trial_model.parameters()).is_cuda\n",
    "\n",
    "            final_train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=trial_batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            current_lr = lr\n",
    "            for epoch in range(epochs):\n",
    "                if epoch > 0:\n",
    "                    current_lr *= lr_decay\n",
    "\n",
    "                train(\n",
    "                    trial_model, final_train_loader, trial_neigh_model, trial_scores_all,\n",
    "                    criterion=SNDRPolicyLoss(), num_epochs=1, lr=current_lr, device=str(device)\n",
    "                )\n",
    "\n",
    "            # trial_x_t, trial_a_t = trial_model.get_params()\n",
    "            # trial_x = trial_x_t.detach().cpu().numpy()\n",
    "            # trial_a = trial_a_t.detach().cpu().numpy()\n",
    "            # trial_policy = np.expand_dims(softmax(trial_x @ trial_a.T, axis=1), -1)\n",
    "            # trial_policy_reward = calc_reward(dataset, trial_policy)\n",
    "            trial_policy_reward = validation_loop(trial_model, val_loader, trial_neigh_model, trial_scores_all, device=str(device))\n",
    "            return trial_policy_reward\n",
    "\n",
    "        # ---- Hyperparam search ----\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        if last_best_params is not None:\n",
    "            study.enqueue_trial(last_best_params)\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_reward_for_size = study.best_value\n",
    "        best_hyperparams_by_size[train_size] = {\n",
    "            \"params\": best_params,\n",
    "            \"reward\": best_reward_for_size\n",
    "        }\n",
    "        last_best_params = best_params\n",
    "\n",
    "        # ---- Final evaluation loop ----\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                dataset, pi_0, train_size + val_size,\n",
    "                random_state=(run + 1) * train_size\n",
    "            )\n",
    "            \n",
    "            idx = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            val_data = get_train_data(n_actions, val_size, simulation_data, np.arange(val_size) + train_size, our_x)\n",
    "\n",
    "            regression_model = RegressionModel(\n",
    "                n_actions=n_actions, action_context=our_x,\n",
    "                base_model=LogisticRegression(random_state=12345)\n",
    "            )\n",
    "\n",
    "            regression_model.fit(\n",
    "                train_data['x'], train_data['a'], train_data['r'],\n",
    "                original_policy_prob[train_data['x_idx'], train_data['a']].squeeze()\n",
    "            )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                train_data['x_idx'], train_data['a'],\n",
    "                our_a, our_x, train_data['r'],\n",
    "                num_neighbors=best_params['num_neighbors']\n",
    "            )\n",
    "            scores_all = torch.as_tensor(\n",
    "                neighberhoodmodel.predict(all_user_indices),\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            model = CFModel(\n",
    "                n_users, n_actions, emb_dim,\n",
    "                initial_user_embeddings=T(our_x),\n",
    "                initial_actions_embeddings=T(our_a)\n",
    "            ).to(device)\n",
    "\n",
    "            assert (not torch.cuda.is_available()) or next(model.parameters()).is_cuda\n",
    "\n",
    "            cf_dataset = CustomCFDataset(\n",
    "                train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob\n",
    "            )\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                cf_dataset, batch_size=batch_size, shuffle=True,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                num_workers=num_workers, persistent_workers=bool(num_workers)\n",
    "            )\n",
    "\n",
    "            current_lr = best_params['lr']\n",
    "            for epoch in range(best_params['num_epochs']):\n",
    "                if epoch > 0:\n",
    "                    current_lr *= best_params['lr_decay']\n",
    "                train(\n",
    "                    model, train_loader, neighberhoodmodel, scores_all,\n",
    "                    criterion=SNDRPolicyLoss(),\n",
    "                    num_epochs=1, lr=current_lr,\n",
    "                    device=str(device)\n",
    "                )\n",
    "\n",
    "            # val_value = validation_loop(model, val_loader, neighberhoodmodel, scores_all=scores_all, device=str(device))\n",
    "            \n",
    "            our_x_t, our_a_t = model.get_params()\n",
    "            our_a, our_x = our_a_t.detach().cpu().numpy(), our_x_t.detach().cpu().numpy()\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "            policy_reward = calc_reward(dataset, policy)\n",
    "            eval_metrics = eval_policy(neighberhoodmodel, train_data, original_policy_prob, policy)\n",
    "            action_diff_to_real = np.sqrt(np.mean((emb_a - our_a) ** 2))\n",
    "            action_delta = np.sqrt(np.mean((original_a - our_a) ** 2))\n",
    "            context_diff_to_real = np.sqrt(np.mean((emb_x - our_x) ** 2))\n",
    "            context_delta = np.sqrt(np.mean((original_x - our_x) ** 2))\n",
    "\n",
    "            row = np.concatenate([\n",
    "                np.atleast_1d(policy_reward),\n",
    "                np.atleast_1d(eval_metrics),\n",
    "                np.atleast_1d(action_diff_to_real),\n",
    "                np.atleast_1d(action_delta),\n",
    "                np.atleast_1d(context_diff_to_real),\n",
    "                np.atleast_1d(context_delta)\n",
    "            ])\n",
    "            conv_results.append(row)\n",
    "            our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "        results[train_size] = get_opl_results_dict(reg_results, conv_results)\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index'), best_hyperparams_by_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$\n",
    "\n",
    "Our parameters for the dataset will be\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "\n",
    "to learn a new policy from $\\pi$ we will sample from:\n",
    "$$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * \\pi_{random}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Item CTR: 0.07066414727263938\n",
      "Optimal greedy CTR: 0.09999926940951757\n",
      "Optimal Stochastic CTR: 0.09995326955796031\n",
      "Our Initial CTR: 0.08610747363354625\n"
     ]
    }
   ],
   "source": [
    "dataset_params = dict(\n",
    "                    n_actions= 500,\n",
    "                    n_users = 500,\n",
    "                    emb_dim = 16,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.6, # this is the epsilon for the noise in the ground truth policy representation\n",
    "                    ctr = 0.1\n",
    "                    )\n",
    "\n",
    "train_dataset = generate_dataset(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 5\n",
    "batch_size = 200\n",
    "num_neighbors = 6\n",
    "n_trials_for_optuna = 10\n",
    "num_rounds_list = [500, 1000, 2000, 10000, 20000]\n",
    "\n",
    "# Manually define your best parameters\n",
    "best_params_to_use = {\n",
    "    \"lr\": 0.0095,  # Learning rate\n",
    "    \"num_epochs\": 5,  # Number of training epochs\n",
    "    \"batch_size\": 64,  # Batch size for training\n",
    "    \"num_neighbors\": 8,  # Number of neighbors for neighborhood model\n",
    "    \"lr_decay\": 0.85  # Learning rate decay factor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "$$emb = 0.7 * gt + 0.3 * noise$$\n",
    "$$lr = 0.005$$\n",
    "$$n_{epochs} = 1$$\n",
    "$$BatchSize=50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of num_rounds_list: [500, 1000, 2000, 10000, 20000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue of num_rounds_list:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_rounds_list)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Run the optimization\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df4, best_hyperparams_by_size \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials_for_optuna\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprev_best_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params_to_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print best hyperparameters for each training size\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 56\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[1;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size, val_size, n_trials, prev_best_params)\u001b[0m\n\u001b[0;32m     50\u001b[0m regression_model \u001b[38;5;241m=\u001b[39m RegressionModel(\n\u001b[0;32m     51\u001b[0m     n_actions\u001b[38;5;241m=\u001b[39mn_actions, action_context\u001b[38;5;241m=\u001b[39mour_x,\n\u001b[0;32m     52\u001b[0m     base_model\u001b[38;5;241m=\u001b[39mLogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m)\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m regression_model\u001b[38;5;241m.\u001b[39mfit(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 56\u001b[0m neighberhoodmodel \u001b[38;5;241m=\u001b[39m \u001b[43mNeighborhoodModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mour_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mour_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_neighbors\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m scores_all \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\n\u001b[0;32m     62\u001b[0m     neighberhoodmodel\u001b[38;5;241m.\u001b[39mpredict(all_user_indices),\n\u001b[0;32m     63\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     65\u001b[0m model \u001b[38;5;241m=\u001b[39m CFModel(\n\u001b[0;32m     66\u001b[0m     n_users, n_actions, emb_dim,\n\u001b[0;32m     67\u001b[0m     initial_user_embeddings\u001b[38;5;241m=\u001b[39mT(our_x),\n\u001b[0;32m     68\u001b[0m     initial_actions_embeddings\u001b[38;5;241m=\u001b[39mT(our_a)\n\u001b[0;32m     69\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\models.py:37\u001b[0m, in \u001b[0;36mNeighborhoodModel.__init__\u001b[1;34m(self, context, actions, action_emb, context_emb, rewards, num_neighbors, chunksize, gamma)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_neighbors \u001b[38;5;241m=\u001b[39m num_neighbors\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;241m=\u001b[39m chunksize\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\models.py:44\u001b[0m, in \u001b[0;36mNeighborhoodModel.fit\u001b[1;34m(self, action_emb, context_emb, actions, context, rewards)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mint32(context)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(rewards)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\models.py:63\u001b[0m, in \u001b[0;36mNeighborhoodModel.calculate_scores\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_scores\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     62\u001b[0m     context \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_similarity\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_convolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\models.py:98\u001b[0m, in \u001b[0;36mNeighborhoodModel.context_convolve\u001b[1;34m(self, test_context)\u001b[0m\n\u001b[0;32m     95\u001b[0m all_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_similarity\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, test_context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     96\u001b[0m all_actions \u001b[38;5;241m=\u001b[39m all_actions\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m---> 98\u001b[0m eta_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m eta_all \u001b[38;5;241m=\u001b[39m eta_all\u001b[38;5;241m.\u001b[39mreshape(test_context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_similarity\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eta_all\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\models.py:75\u001b[0m, in \u001b[0;36mNeighborhoodModel.convolve\u001b[1;34m(self, test_actions, test_context)\u001b[0m\n\u001b[0;32m     72\u001b[0m chunk_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(chunk_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize, B)\n\u001b[0;32m     74\u001b[0m cosine_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_similarity[np\u001b[38;5;241m.\u001b[39mint32(test_context[chunk_start:chunk_end])][:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext]\n\u001b[1;32m---> 75\u001b[0m cosine_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_similarity\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mchunk_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     76\u001b[0m tot_cosine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m cosine_actions \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma) \u001b[38;5;241m*\u001b[39m cosine_context\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m cosine_context, cosine_actions\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Value of num_rounds_list:\", num_rounds_list)\n",
    "\n",
    "# Run the optimization\n",
    "df4, best_hyperparams_by_size = trainer_trial(num_runs, num_neighbors, num_rounds_list, train_dataset, batch_size, val_size=2000, n_trials=n_trials_for_optuna,prev_best_params=best_params_to_use)\n",
    "\n",
    "# Print best hyperparameters for each training size\n",
    "print(\"\\n=== BEST HYPERPARAMETERS BY TRAINING SIZE ===\")\n",
    "for train_size, params in best_hyperparams_by_size.items():\n",
    "    print(f\"\\nTraining Size: {train_size}\")\n",
    "    print(f\"Best Reward: {params['reward']:.6f}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, value in params['params'].items():\n",
    "        print(f\"  {param_name}: {value}\")\n",
    "print(\"===========================\\n\")\n",
    "\n",
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.08610747</td>\n",
       "      <td>0.10697090</td>\n",
       "      <td>0.09051612</td>\n",
       "      <td>0.09112201</td>\n",
       "      <td>0.09452505</td>\n",
       "      <td>0.10672373</td>\n",
       "      <td>0.75692870</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.87627132</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.08705604</td>\n",
       "      <td>0.09221834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08299331</td>\n",
       "      <td>0.08198609</td>\n",
       "      <td>0.07599146</td>\n",
       "      <td>0.79170973</td>\n",
       "      <td>0.24615559</td>\n",
       "      <td>0.88427728</td>\n",
       "      <td>0.08661758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.08939145</td>\n",
       "      <td>0.11301958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08679147</td>\n",
       "      <td>0.09052395</td>\n",
       "      <td>0.10668506</td>\n",
       "      <td>1.01853061</td>\n",
       "      <td>0.76340735</td>\n",
       "      <td>0.91464524</td>\n",
       "      <td>0.19321758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.09251861</td>\n",
       "      <td>0.10603409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09028676</td>\n",
       "      <td>0.15397776</td>\n",
       "      <td>0.10628439</td>\n",
       "      <td>1.73862067</td>\n",
       "      <td>1.70789298</td>\n",
       "      <td>0.99652312</td>\n",
       "      <td>0.34170287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.09268524</td>\n",
       "      <td>0.09704712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09829317</td>\n",
       "      <td>0.09539229</td>\n",
       "      <td>0.09257621</td>\n",
       "      <td>2.18938809</td>\n",
       "      <td>2.22344507</td>\n",
       "      <td>1.03555944</td>\n",
       "      <td>0.40151858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>0.09264639</td>\n",
       "      <td>0.09493701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09216787</td>\n",
       "      <td>0.09134166</td>\n",
       "      <td>0.09056984</td>\n",
       "      <td>2.21242505</td>\n",
       "      <td>2.24879912</td>\n",
       "      <td>1.03520993</td>\n",
       "      <td>0.40065441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       policy_rewards        ipw     reg_dm    conv_dm    conv_dr  conv_sndr  \\\n",
       "0          0.08610747 0.10697090 0.09051612 0.09112201 0.09452505 0.10672373   \n",
       "500        0.08705604 0.09221834        NaN 0.08299331 0.08198609 0.07599146   \n",
       "1000       0.08939145 0.11301958        NaN 0.08679147 0.09052395 0.10668506   \n",
       "2000       0.09251861 0.10603409        NaN 0.09028676 0.15397776 0.10628439   \n",
       "10000      0.09268524 0.09704712        NaN 0.09829317 0.09539229 0.09257621   \n",
       "20000      0.09264639 0.09493701        NaN 0.09216787 0.09134166 0.09056984   \n",
       "\n",
       "       action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "0               0.75692870    0.00000000            0.87627132     0.00000000  \n",
       "500             0.79170973    0.24615559            0.88427728     0.08661758  \n",
       "1000            1.01853061    0.76340735            0.91464524     0.19321758  \n",
       "2000            1.73862067    1.70789298            0.99652312     0.34170287  \n",
       "10000           2.18938809    2.22344507            1.03555944     0.40151858  \n",
       "20000           2.21242505    2.24879912            1.03520993     0.40065441  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance metrics\n",
    "df4[['policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr', 'action_diff_to_real', 'action_delta', 'context_diff_to_real', 'context_delta']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
