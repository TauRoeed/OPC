{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "from abc import ABCMeta\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.dataset import linear_reward_function, logistic_reward_function\n",
    "# from obp.dataset import SyntheticBanditDatasetWithActionEmbeds\n",
    "from obp.policy import IPWLearner\n",
    "\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation,\n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPW,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    "    SelfNormalizedDoublyRobust as SNDR\n",
    ")\n",
    "\n",
    "from my_utils import (\n",
    "    NeighborhoodModel,\n",
    "    eval_policy,\n",
    "    # create_simluation_data_from_pi,\n",
    "    # sample_policy_actions\n",
    ")\n",
    "random_state=12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val_data= 4000\n",
    "n_test_data = 5\n",
    "\n",
    "n_actions= 150\n",
    "n_users = 150\n",
    "beta = -1\n",
    "n_def_actions= 0.0\n",
    "reward_std = 2.5\n",
    "random_state= 12345\n",
    "\n",
    "num_runs = 5 # number of simulations\n",
    "max_iter = 25 # number of epochs\n",
    "emb_dim = 5\n",
    "\n",
    "random_ = check_random_state(random_state)\n",
    "num_rounds_list = [3, 6, 10, 15, 20]\n",
    "sigma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general user tendencies\n",
    "eps = 0.4\n",
    "emb_a = random_.normal(size=(n_actions, emb_dim))\n",
    "noise_a = random_.normal(size=(emb_dim))\n",
    "our_a = (1-eps) * emb_a + eps * noise_a\n",
    "\n",
    "original_a = our_a.copy()\n",
    "\n",
    "emb_x = random_.normal(size=(n_users, emb_dim))\n",
    "noise_x = random_.normal(size=(emb_dim)) \n",
    "our_x = (1-eps) * emb_x + eps * noise_x\n",
    "\n",
    "original_x = our_x.copy()\n",
    "\n",
    "# M_h = random_.normal(size=(emb_dim, emb_dim))\n",
    "# b_h = random_.normal(size=(1, n_actions))\n",
    "\n",
    "# score = emb_x @ emb_a.T\n",
    "# score = emb_x @ M_h @ emb_a.T\n",
    "\n",
    "score = emb_x @ emb_a.T\n",
    "score = random_.normal(score, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_x_a = (1 / (5.0 + np.exp(-score))) > random_.random(size=score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14715555555555557"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_x_a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simluation_data_from_pi(pi: np.ndarray, q_x_a: np.ndarray, n_users: np.int, n_actions: np.int, random_state: int = 12345):\n",
    "    random_ = check_random_state(random_state)\n",
    "    simulation_data = {'actions':np.zeros((n_actions, n_users), dtype=np.int32), \n",
    "                       'users': np.zeros((n_actions, n_users), dtype=np.int32), \n",
    "                       'reward':np.zeros((n_actions, n_users)),\n",
    "                       'pscore':np.zeros((n_actions, n_users))}\n",
    "    simulation_data['pi_0'] = pi\n",
    "    actions = []\n",
    "    for i in range(n_users):\n",
    "        user_actions = random_.choice(np.arange(n_actions), size=n_actions, p=pi[i], replace=False)\n",
    "        actions.append(np.array(user_actions))\n",
    "\n",
    "    actions = np.vstack(actions)\n",
    "    for i in range(n_actions):\n",
    "        simulation_data['actions'][i] = actions[:, i]\n",
    "        simulation_data['users'][i] = np.arange(n_users)\n",
    "        simulation_data['reward'][i] = np.squeeze(q_x_a[np.arange(n_users), simulation_data['actions'][i]])\n",
    "        simulation_data['pscore'][i] = np.squeeze(pi[np.arange(n_users), simulation_data['actions'][i]])\n",
    "    \n",
    "    simulation_data['q_x_a'] = q_x_a\n",
    "\n",
    "    return simulation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_data = create_simluation_data_from_pi(np.ones_like(q_x_a)/(n_actions), q_x_a, n_users, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_actions, embedding_dim, \n",
    "                 initial_user_embeddings=None, initial_actions_embeddings=None):\n",
    "\n",
    "        super(CFModel, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.num_users = num_users\n",
    "\n",
    "        \n",
    "        # Initialize user and actions embeddings\n",
    "        if initial_user_embeddings is None:\n",
    "            self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        else:\n",
    "            # If initial embeddings are provided, set them as the embeddings\n",
    "            self.user_embeddings = nn.Embedding.from_pretrained(initial_user_embeddings, freeze=False)\n",
    "        \n",
    "        if initial_actions_embeddings is None:\n",
    "            self.actions_embeddings = nn.Embedding(num_actions, embedding_dim)\n",
    "        else:\n",
    "            # If initial embeddings are provided, set them as the embeddings\n",
    "            self.actions_embeddings = nn.Embedding.from_pretrained(initial_actions_embeddings, freeze=False)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.actions_embeddings(torch.arange(self.num_actions)), self.user_embeddings(torch.arange(self.num_users))\n",
    "        \n",
    "    def forward(self, user_ids):\n",
    "        # Get embeddings for users and actions\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "        actions_embedding = self.actions_embeddings\n",
    "        \n",
    "        # Calculate dot product between user and actions embeddings\n",
    "        scores = user_embedding @ actions_embedding(torch.arange(self.num_actions)).T\n",
    "        \n",
    "        # Apply softmax to get the predicted probability distribution\n",
    "        return F.softmax(scores, dim=1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCFDataset(Dataset):\n",
    "    def __init__(self, user_idx, action_idx, rewards, original_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            np_arrays (list of np.ndarray): List of numpy arrays\n",
    "        \"\"\"\n",
    "        self.user_idx = user_idx\n",
    "        self.action_idx = action_idx\n",
    "        self.rewards = rewards\n",
    "        self.original_prob = original_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rewards)\n",
    "\n",
    "    def __getitem__(self, idx):   \n",
    "        # Convert list to tensor\n",
    "        user = torch.tensor(self.user_idx[idx].squeeze())\n",
    "        action =  torch.tensor(self.action_idx[idx].squeeze())\n",
    "        reward = torch.tensor(self.rewards[idx].squeeze(), dtype=torch.double)\n",
    "        action_dist = torch.tensor(self.original_prob[user].squeeze())\n",
    "        \n",
    "        return user, action, reward, action_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_policy_gradient(\n",
    "    self,\n",
    "    x: torch.Tensor,\n",
    "    a: torch.Tensor,\n",
    "    r: torch.Tensor,\n",
    "    phi_a: torch.Tensor,\n",
    "    pscore_c: torch.Tensor,\n",
    "    f_hat: torch.Tensor,\n",
    "    pi: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    current_pi = pi.detach()\n",
    "    log_prob = torch.log(pi + self.log_eps)\n",
    "    idx = torch.arange(a.shape[0], dtype=torch.long)\n",
    "\n",
    "    f_hat_factual = f_hat[idx, a]\n",
    "    iw = current_pi[idx, phi_a[a]] / pscore_c\n",
    "    self.max_iw_.append(iw.numpy().max())\n",
    "    estimated_policy_grad_arr = iw * (r - f_hat_factual) * log_prob[idx, phi_a[a]]\n",
    "\n",
    "    f_hat_c = torch.zeros((a.shape[0], self.num_clusters))\n",
    "    for c in range(self.num_clusters):\n",
    "        if (phi_a == c).sum() > 0:\n",
    "            f_hat_c[:, c] = f_hat[:, phi_a == c].max(1)[0]\n",
    "        else:\n",
    "            f_hat_c[:, c] = 0.0\n",
    "    estimated_policy_grad_arr += torch.sum(f_hat_c * current_pi * log_prob, dim=1)\n",
    "\n",
    "    return estimated_policy_grad_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "        iw = policy_prob[torch.arange(n), original_policy_actions] / pscore\n",
    "        \n",
    "        q_hat_at_position = scores[torch.arange(n), :]\n",
    "        q_hat_factual = scores[torch.arange(n), original_policy_actions]\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), :]\n",
    "        log_pi = torch.log(policy_prob[torch.arange(n), original_policy_actions])\n",
    "        \n",
    "        # DM\n",
    "        estimated_rewards = ((q_hat_at_position * pi_e_at_position).sum(axis=1) / (pi_e_at_position.sum(axis=1))).squeeze()\n",
    "\n",
    "        # DR + IPS / NORM\n",
    "        estimated_rewards = estimated_rewards + (iw @ (original_policy_rewards - q_hat_factual.squeeze())) / iw.sum()\n",
    "\n",
    "        # reinforce trick step\n",
    "        reinforce_grad = estimated_rewards * log_pi\n",
    "        \n",
    "        return -reinforce_grad.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_17:00.csv\n"
     ]
    }
   ],
   "source": [
    "result_path = Path(f\"./result/{datetime.now().strftime('%Y-%m-%d')}/train_data\")\n",
    "result_path.mkdir(parents=True, exist_ok=True)\n",
    "result_file_name = f\"result_{datetime.now().strftime('%H:00')}.csv\"\n",
    "curve_file_name = f\"curve_{datetime.now().strftime('%H:00')}.csv\"\n",
    "print(result_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dict(\n",
    "        num_data=n_test_data*n_users,\n",
    "        num_actions=n_actions,\n",
    "        x=our_x[simulation_data['users'][:n_test_data].flatten()],\n",
    "        a=simulation_data['actions'][:n_test_data].flatten(),\n",
    "        r=simulation_data['reward'][:n_test_data].flatten(),\n",
    "        x_idx=simulation_data['users'][:n_test_data].flatten(),\n",
    "        pi_0=simulation_data['pi_0'],\n",
    "        pscore=simulation_data['pscore'][:n_test_data].flatten(),\n",
    "        q_x_a=simulation_data['q_x_a'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the training function\n",
    "def train(model, train_loader, criterion, optimizer, neighborhood_model, num_epochs=1):\n",
    "    \n",
    "    tq = tqdm(range(num_epochs))\n",
    "    for epoch in tq:\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        # correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for user_idx, action_idx, rewards, original_prob in train_loader:\n",
    "            # Move data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                user_idx, action_idx, rewards, original_prob = user_idx.cuda(), action_idx.cuda(), rewards.cuda(), original_prob.cuda()\n",
    "                model.cuda()\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            policy = model(user_idx)\n",
    "            pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "            \n",
    "            scores = torch.tensor(neighborhood_model.predict(user_idx))\n",
    "            \n",
    "            loss = criterion(pscore, scores, policy, rewards, action_idx.type(torch.long))\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            action_emb, context_emb = model.get_params()\n",
    "            neighborhood_model.update(action_emb.detach().numpy(), context_emb.detach().numpy())\n",
    "            \n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            # _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += 1\n",
    "            # correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "            # Print statistics after each epoch\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            tq.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(n_actions, train_size, sim_data, idx, emb_x):\n",
    "   return dict(num_data=train_size,\n",
    "                num_actions=n_actions,\n",
    "                x=emb_x[sim_data['users'][idx].flatten()],\n",
    "                a=sim_data['actions'][idx].flatten(),\n",
    "                r=sim_data['reward'][idx].flatten(),\n",
    "                x_idx=sim_data['users'][idx].flatten(),\n",
    "                pi_0=sim_data['pi_0'],\n",
    "                pscore=sim_data['pscore'][idx].flatten(),\n",
    "                q_x_a=simulation_data['q_x_a']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def no_learning_trial(original_policy_prob, simulation_data, num_runs, num_neighbors, num_rounds_list, our_x, our_a):\n",
    "    results = {}\n",
    "    dm = DM()\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            idx = np.arange(train_size) + n_test_data\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            \n",
    "            regression_model = RegressionModel(\n",
    "                                                n_actions=n_actions,\n",
    "                                                action_context=emb_x,\n",
    "                                                base_model=LogisticRegression(random_state=12345)\n",
    "                                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "            regression_model.fit(train_data['x'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx'], train_data['a']].squeeze())\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = dict(\n",
    "                                policy_rewards=np.mean(conv_results[: ,0]),\n",
    "                                ipw=np.mean(conv_results[: ,3]),\n",
    "                                reg_dm=np.mean(reg_results),\n",
    "                                conv_dm=np.mean(conv_results[: ,1]), \n",
    "                                conv_dr=np.mean(conv_results[: ,2]),\n",
    "                                conv_sndr=np.mean(conv_results[: ,4]),\n",
    "                            )\n",
    "    return pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "\n",
    "def trainer_trial(original_policy_prob, simulation_data, num_runs, num_neighbors, num_rounds_list, our_x, our_a, batch_size):\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            idx = np.arange(train_size) + n_test_data\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            \n",
    "            regression_model = RegressionModel(\n",
    "                                                n_actions=n_actions,\n",
    "                                                action_context=emb_x,\n",
    "                                                base_model=LogisticRegression(random_state=12345)\n",
    "                                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "            regression_model.fit(train_data['x'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx'], train_data['a']].squeeze())\n",
    "\n",
    "            model = CFModel(n_users, n_actions, emb_dim, initial_user_embeddings=torch.tensor(our_x), initial_actions_embeddings=torch.tensor(our_a))\n",
    "            dataset =  CustomCFDataset(train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx']])\n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            adam = optim.Adam(model.parameters())\n",
    "            loss_fn = PolicyLoss()\n",
    "            \n",
    "            train(model, train_loader, loss_fn, adam, neighberhoodmodel)\n",
    "\n",
    "            our_a, our_x = model.get_params()\n",
    "            our_a, our_x = our_a.detach().numpy(), our_x.detach().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            \n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.mean(np.abs(emb_a-our_a)), np.mean(np.abs(original_a-our_a))])\n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = dict(\n",
    "                                    policy_rewards=np.mean(conv_results[: ,0]),\n",
    "                                    ipw=np.mean(conv_results[: ,3]),\n",
    "                                    reg_dm=np.mean(reg_results),\n",
    "                                    conv_dm=np.mean(conv_results[: ,1]), \n",
    "                                    conv_dr=np.mean(conv_results[: ,2]),\n",
    "                                    conv_sndr=np.mean(conv_results[: ,4]),\n",
    "                                    diff_to_real=np.mean(conv_results[: ,5]),\n",
    "                                    diff_from_start=np.mean(conv_results[: ,6])\n",
    "                                    )\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_policy_prob = np.expand_dims(np.ones_like(q_x_a) / (n_actions), -1)\n",
    "# dm = DM()\n",
    "# results = {}\n",
    "num_runs = 1\n",
    "batch_size = 50\n",
    "num_neighbors = 5\n",
    "\n",
    "df1 = no_learning_trial(original_policy_prob, simulation_data, num_runs, num_neighbors, num_rounds_list, our_x, our_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.148071</td>\n",
       "      <td>0.112947</td>\n",
       "      <td>0.117393</td>\n",
       "      <td>0.155586</td>\n",
       "      <td>0.158020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.148071</td>\n",
       "      <td>0.129069</td>\n",
       "      <td>0.131508</td>\n",
       "      <td>0.157163</td>\n",
       "      <td>0.158798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.148071</td>\n",
       "      <td>0.135941</td>\n",
       "      <td>0.145807</td>\n",
       "      <td>0.158226</td>\n",
       "      <td>0.159018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.148071</td>\n",
       "      <td>0.140199</td>\n",
       "      <td>0.149973</td>\n",
       "      <td>0.158513</td>\n",
       "      <td>0.159058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.148071</td>\n",
       "      <td>0.146485</td>\n",
       "      <td>0.158516</td>\n",
       "      <td>0.162328</td>\n",
       "      <td>0.162571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards       ipw    reg_dm   conv_dm   conv_dr  conv_sndr\n",
       "3         0.186667  0.148071  0.112947  0.117393  0.155586   0.158020\n",
       "6         0.186667  0.148071  0.129069  0.131508  0.157163   0.158798\n",
       "10        0.186667  0.148071  0.135941  0.145807  0.158226   0.159018\n",
       "15        0.186667  0.148071  0.140199  0.149973  0.158513   0.159058\n",
       "20        0.186667  0.148071  0.146485  0.158516  0.162328   0.162571"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.7058:   0%|          | 0/1 [32:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (50) must match the size of tensor b (150) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_policy_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mour_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mour_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df2\n",
      "Cell \u001b[1;32mIn[15], line 82\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[1;34m(original_policy_prob, simulation_data, num_runs, num_neighbors, num_rounds_list, our_x, our_a, batch_size)\u001b[0m\n\u001b[0;32m     79\u001b[0m adam \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     80\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m PolicyLoss()\n\u001b[1;32m---> 82\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighberhoodmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m our_a, our_x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_params()\n\u001b[0;32m     85\u001b[0m our_a, our_x \u001b[38;5;241m=\u001b[39m our_a\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), our_x\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, neighborhood_model, num_epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m pscore \u001b[38;5;241m=\u001b[39m original_prob[torch\u001b[38;5;241m.\u001b[39marange(user_idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), action_idx\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mlong)]\n\u001b[0;32m     24\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(neighborhood_model\u001b[38;5;241m.\u001b[39mpredict(user_idx))\n\u001b[1;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[23], line 18\u001b[0m, in \u001b[0;36mPolicyLoss.forward\u001b[1;34m(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions)\u001b[0m\n\u001b[0;32m     15\u001b[0m estimated_rewards \u001b[38;5;241m=\u001b[39m ((q_hat_at_position \u001b[38;5;241m*\u001b[39m pi_e_at_position)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (pi_e_at_position\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)))\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     16\u001b[0m estimated_rewards \u001b[38;5;241m=\u001b[39m estimated_rewards \u001b[38;5;241m+\u001b[39m (iw \u001b[38;5;241m@\u001b[39m (original_policy_rewards \u001b[38;5;241m-\u001b[39m q_hat_factual)) \u001b[38;5;241m/\u001b[39m iw\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 18\u001b[0m reinforce_grad \u001b[38;5;241m=\u001b[39m \u001b[43mestimated_rewards\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog_pi_at_position\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# return estimated_rewards\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# value = self.ope_fn.estimate_policy_value(original_policy_rewards, original_policy_actions, original_policy_prob, scores, pscore=pscore)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mreinforce_grad\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (50) must match the size of tensor b (150) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "df2 = trainer_trial(original_policy_prob, simulation_data, num_runs, num_neighbors, num_rounds_list, our_x, our_a, batch_size)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original_policy_prob = np.expand_dims(softmax(original_x @ original_a.T, axis=1), -1)\n",
    "# original_policy_prob = np.expand_dims(np.ones_like(q_x_a) / (n_actions), -1)\n",
    "# dm = DM()\n",
    "# results = {}\n",
    "# num_runs = 1\n",
    "# batch_size = 50\n",
    "# num_neighbors = 4\n",
    "\n",
    "# for train_size in num_rounds_list:\n",
    "#     reg_results, conv_results = [], []\n",
    "#     for run in range(num_runs):\n",
    "\n",
    "#         idx = np.arange(train_size) + n_test_data\n",
    "#         train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x, q_x_a)\n",
    "        \n",
    "#         regression_model = RegressionModel(\n",
    "#                                             n_actions=n_actions,\n",
    "#                                             action_context=emb_x,\n",
    "#                                             base_model=LogisticRegression(random_state=12345)\n",
    "#                                             )\n",
    "        \n",
    "#         neighberhoodmodel = NeighborhoodModel(\n",
    "#                                                 train_data['x_idx'],\n",
    "#                                                 train_data['a'], \n",
    "#                                                 our_a,\n",
    "#                                                 our_x, \n",
    "#                                                 train_data['r'], \n",
    "#                                                 num_neighbors=num_neighbors\n",
    "#                                             )\n",
    "        \n",
    "#         regression_model.fit(train_data['x'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx'], train_data['a']].squeeze())\n",
    "\n",
    "#         # model = CFModel(n_users, n_actions, emb_dim, initial_user_embeddings=torch.tensor(our_x), initial_actions_embeddings=torch.tensor(our_a))\n",
    "#         # dataset =  CustomCFDataset(train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx']])\n",
    "#         # train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#         # adam = optim.Adam(model.parameters())\n",
    "#         # loss_fn = PolicyLoss()\n",
    "        \n",
    "#         # train(model, train_loader, loss_fn, adam, neighberhoodmodel)\n",
    "\n",
    "#         # our_a, our_x = model.get_params()\n",
    "#         # our_a, our_x = our_a.detach().numpy(), our_x.detach().numpy()\n",
    "\n",
    "#         # print(np.mean(np.abs(emb_a-our_a)))\n",
    "#         # print(np.mean(np.abs(original_a-our_a)))\n",
    "\n",
    "#         policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "#         reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "#         reg_results.append(reg_dm)\n",
    "\n",
    "#         conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "        \n",
    "#         # conv_results[-1] = np.append(conv_results[-1], [np.mean(np.abs(emb_a-our_a)), np.mean(np.abs(original_a-our_a))])\n",
    "#         # our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "#     reg_results = np.array(reg_results)\n",
    "#     conv_results = np.array(conv_results)\n",
    "\n",
    "#     results[train_size] = dict(\n",
    "#                                 # reg_rewards=np.mean(reg_results[: ,0]),\n",
    "#                                 # reg_dm=np.mean(reg_results[: ,1]), \n",
    "#                                 # reg_dr=np.mean(reg_results[: ,2]),\n",
    "#                                 # reg_ipw=np.mean(reg_results[: ,3]),\n",
    "#                                 # reg_sndr=np.mean(reg_results[: ,4]),\n",
    "#                                 # reg_var_dm=np.std(reg_results[: ,1] - reg_results[: ,0]),\n",
    "#                                 # reg_var_dr=np.std(reg_results[: ,2] - reg_results[: ,0]),\n",
    "#                                 # reg_var_ipw=np.std(reg_results[: ,3] - reg_results[: ,0]),\n",
    "#                                 # reg_var_sndr=np.std(reg_results[: ,4] - reg_results[: ,0]),\n",
    "#                                 policy_rewards=np.mean(conv_results[: ,0]),\n",
    "#                                 ipw=np.mean(conv_results[: ,3]),\n",
    "#                                 reg_dm=np.mean(reg_results),\n",
    "\n",
    "#                                 conv_dm=np.mean(conv_results[: ,1]), \n",
    "#                                 conv_dr=np.mean(conv_results[: ,2]),\n",
    "#                                 conv_sndr=np.mean(conv_results[: ,4]),\n",
    "#                                 # diff_to_real=np.mean(conv_results[: ,5]),\n",
    "#                                 # diff_from_start=np.mean(conv_results[: ,6])\n",
    "#                                 # conv_var_dm=np.std(conv_results[: ,1] - conv_results[: ,0]),\n",
    "#                                 # conv_var_dr=np.std(conv_results[: ,2] - conv_results[: ,0]),\n",
    "#                                 # conv_var_ipw=np.std(conv_results[: ,3] - conv_results[: ,0]),\n",
    "#                                 # conv_var_sndr=np.std(conv_results[: ,4] - conv_results[: ,0]),\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[43mresults\u001b[49m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.149066</td>\n",
       "      <td>0.153500</td>\n",
       "      <td>0.209685</td>\n",
       "      <td>0.209908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.152306</td>\n",
       "      <td>0.164966</td>\n",
       "      <td>0.199016</td>\n",
       "      <td>0.199152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.159112</td>\n",
       "      <td>0.173460</td>\n",
       "      <td>0.198981</td>\n",
       "      <td>0.199082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.145156</td>\n",
       "      <td>0.159052</td>\n",
       "      <td>0.200623</td>\n",
       "      <td>0.200788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.144494</td>\n",
       "      <td>0.158721</td>\n",
       "      <td>0.191275</td>\n",
       "      <td>0.191405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards       ipw    reg_dm   conv_dm   conv_dr  conv_sndr\n",
       "3             0.56  0.199051  0.149066  0.153500  0.209685   0.209908\n",
       "6             0.56  0.199051  0.152306  0.164966  0.199016   0.199152\n",
       "10            0.56  0.199051  0.159112  0.173460  0.198981   0.199082\n",
       "15            0.56  0.199051  0.145156  0.159052  0.200623   0.200788\n",
       "20            0.56  0.199051  0.144494  0.158721  0.191275   0.191405"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'diff_to_real'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'diff_to_real'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetting closer to real emb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiff_to_real\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiff_to_real\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'diff_to_real'"
     ]
    }
   ],
   "source": [
    "df['getting closer to real emb'] = df['diff_to_real'].max() - df['diff_to_real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(y=['getting closer to real emb', 'diff_from_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
