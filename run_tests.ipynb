{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "from abc import ABCMeta\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.dataset import linear_reward_function, logistic_reward_function\n",
    "# from obp.dataset import SyntheticBanditDatasetWithActionEmbeds\n",
    "from obp.policy import IPWLearner\n",
    "\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation,\n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPW,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    "    SelfNormalizedDoublyRobust as SNDR\n",
    ")\n",
    "\n",
    "from my_utils import (\n",
    "    NeighborhoodModel,\n",
    "    eval_policy,\n",
    "    # create_simluation_data_from_pi,\n",
    "    # sample_policy_actions\n",
    ")\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(params):\n",
    "    emb_a = random_.normal(size=(params[\"n_actions\"], params[\"emb_dim\"]))\n",
    "    noise_a = random_.normal(size=(params[\"emb_dim\"]))\n",
    "    our_a = (1-params[\"eps\"]) * emb_a + params[\"eps\"] * noise_a\n",
    "\n",
    "    original_a = our_a.copy()\n",
    "\n",
    "    emb_x = random_.normal(size=(params[\"n_users\"], params[\"emb_dim\"]))\n",
    "    noise_x = random_.normal(size=(params[\"emb_dim\"])) \n",
    "    our_x = (1-params[\"eps\"]) * emb_x + params[\"eps\"] * noise_x\n",
    "    original_x = our_x.copy()\n",
    "\n",
    "    score = emb_x @ emb_a.T\n",
    "    score = random_.normal(score, scale=params[\"sigma\"])\n",
    "    q_x_a = (1 / (5.0 + np.exp(-score)))\n",
    "\n",
    "    return dict(\n",
    "                emb_a=emb_a,\n",
    "                our_a=our_a,\n",
    "                original_a=original_a,\n",
    "                emb_x=emb_x,\n",
    "                our_x=our_x,\n",
    "                original_x=original_x,\n",
    "                q_x_a=q_x_a,\n",
    "                n_actions=params[\"n_actions\"],\n",
    "                n_users=params[\"n_users\"],\n",
    "                emb_dim=params[\"emb_dim\"]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simluation_data_from_pi(pi: np.ndarray, q_x_a: np.ndarray, n_users: np.int, n_actions: np.int, random_state: int = 12345):\n",
    "    random_ = check_random_state(random_state)\n",
    "    simulation_data = {'actions':np.zeros((n_actions, n_users), dtype=np.int32), \n",
    "                       'users': np.zeros((n_actions, n_users), dtype=np.int32), \n",
    "                       'reward':np.zeros((n_actions, n_users)),\n",
    "                       'pscore':np.zeros((n_actions, n_users))}\n",
    "    \n",
    "    reward = q_x_a  > random_.random(size=q_x_a.shape)\n",
    "    simulation_data['pi_0'] = pi\n",
    "    actions = []\n",
    "    for i in range(n_users):\n",
    "        user_actions = random_.choice(np.arange(n_actions), size=n_actions, p=pi[i], replace=False)\n",
    "        actions.append(np.array(user_actions))\n",
    "\n",
    "    actions = np.vstack(actions)\n",
    "    for i in range(n_actions):\n",
    "        simulation_data['actions'][i] = actions[:, i]\n",
    "        simulation_data['users'][i] = np.arange(n_users)\n",
    "        simulation_data['reward'][i] = np.squeeze(reward[np.arange(n_users), simulation_data['actions'][i]])\n",
    "        simulation_data['pscore'][i] = np.squeeze(pi[np.arange(n_users), simulation_data['actions'][i]])\n",
    "    \n",
    "    simulation_data['q_x_a'] = reward\n",
    "    return simulation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset, simulation_data, n_test_data):\n",
    "    return dict(\n",
    "        num_data=n_test_data*dataset[\"n_users\"],\n",
    "        num_actions=dataset[\"n_actions\"],\n",
    "        x=dataset[\"our_x\"][simulation_data['users'][:n_test_data].flatten()],\n",
    "        a=simulation_data['actions'][:n_test_data].flatten(),\n",
    "        r=simulation_data['reward'][:n_test_data].flatten(),\n",
    "        x_idx=simulation_data['users'][:n_test_data].flatten(),\n",
    "        pi_0=simulation_data['pi_0'],\n",
    "        pscore=simulation_data['pscore'][:n_test_data].flatten(),\n",
    "        q_x_a=simulation_data['q_x_a'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(n_actions, train_size, sim_data, idx, emb_x):\n",
    "   return dict(\n",
    "                num_data=train_size,\n",
    "                num_actions=n_actions,\n",
    "                x=emb_x[sim_data['users'][idx].flatten()],\n",
    "                a=sim_data['actions'][idx].flatten(),\n",
    "                r=sim_data['reward'][idx].flatten(),\n",
    "                x_idx=sim_data['users'][idx].flatten(),\n",
    "                pi_0=sim_data['pi_0'],\n",
    "                pscore=sim_data['pscore'][idx].flatten(),\n",
    "                q_x_a=sim_data['q_x_a']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_data = 5\n",
    "\n",
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    sigma = 0.1,\n",
    "                    eps = 0.4\n",
    "                    )\n",
    "\n",
    "dataset = generate_dataset(dataset_params)\n",
    "\n",
    "# num_rounds_list = [3, 6, 10, 15, 20]\n",
    "num_rounds_list = [15, 20]\n",
    "# num_rounds_list = [3, 6]\n",
    "\n",
    "num_runs = 5 # number of simulations\n",
    "max_iter = 25 # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_actions, embedding_dim, \n",
    "                 initial_user_embeddings=None, initial_actions_embeddings=None):\n",
    "\n",
    "        super(CFModel, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.num_users = num_users\n",
    "\n",
    "        \n",
    "        # Initialize user and actions embeddings\n",
    "        if initial_user_embeddings is None:\n",
    "            self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        else:\n",
    "            # If initial embeddings are provided, set them as the embeddings\n",
    "            self.user_embeddings = nn.Embedding.from_pretrained(initial_user_embeddings, freeze=False)\n",
    "        \n",
    "        if initial_actions_embeddings is None:\n",
    "            self.actions_embeddings = nn.Embedding(num_actions, embedding_dim)\n",
    "        else:\n",
    "            # If initial embeddings are provided, set them as the embeddings\n",
    "            self.actions_embeddings = nn.Embedding.from_pretrained(initial_actions_embeddings, freeze=False)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.actions_embeddings(torch.arange(self.num_actions)), self.user_embeddings(torch.arange(self.num_users))\n",
    "        \n",
    "    def forward(self, user_ids):\n",
    "        # Get embeddings for users and actions\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "        actions_embedding = self.actions_embeddings\n",
    "        \n",
    "        # Calculate dot product between user and actions embeddings\n",
    "        scores = user_embedding @ actions_embedding(torch.arange(self.num_actions)).T\n",
    "        \n",
    "        # Apply softmax to get the predicted probability distribution\n",
    "        return F.softmax(scores, dim=1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCFDataset(Dataset):\n",
    "    def __init__(self, user_idx, action_idx, rewards, original_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            np_arrays (list of np.ndarray): List of numpy arrays\n",
    "        \"\"\"\n",
    "        self.user_idx = user_idx\n",
    "        self.action_idx = action_idx\n",
    "        self.rewards = rewards\n",
    "        self.original_prob = original_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rewards)\n",
    "\n",
    "    def __getitem__(self, sample_idx):   \n",
    "        # Convert list to tensor\n",
    "        if sample_idx in np.arange(len(self.rewards)):\n",
    "            user = torch.tensor(self.user_idx[sample_idx].squeeze())\n",
    "            action =  torch.tensor(self.action_idx[sample_idx].squeeze())\n",
    "            reward = torch.tensor(self.rewards[sample_idx].squeeze(), dtype=torch.double)\n",
    "            action_dist = torch.tensor(self.original_prob[user].squeeze())\n",
    "            \n",
    "            return user, action, reward, action_dist\n",
    "        \n",
    "        user = torch.tensor(self.user_idx[0].squeeze())\n",
    "        action =  torch.tensor(self.action_idx[0].squeeze())\n",
    "        reward = torch.tensor(self.rewards[0].squeeze(), dtype=torch.double)\n",
    "        action_dist = torch.tensor(self.original_prob[user].squeeze())\n",
    "        \n",
    "        return user, action, reward, action_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyLoss(nn.Module):\n",
    "    def __init__(self, log_eps=1e-10):\n",
    "        super(PolicyLoss, self).__init__()\n",
    "        self.log_eps = log_eps\n",
    "\n",
    "    def forward(self, pscore, scores, policy_prob, original_policy_rewards, original_policy_actions):\n",
    "        n = original_policy_actions.shape[0]\n",
    "\n",
    "        # q_hat_at_position = scores[torch.arange(n), :]\n",
    "        # q_hat_factual = scores[torch.arange(n), original_policy_actions].squeeze()\n",
    "\n",
    "        pi_e_at_position = policy_prob[torch.arange(n), original_policy_actions].squeeze()\n",
    "        # log_pi = torch.log(pscore + self.log_eps).squeeze()\n",
    "        iw = pi_e_at_position / pscore\n",
    "        log_pi = torch.log(pi_e_at_position).squeeze()\n",
    "        \n",
    "        # DM\n",
    "        # estimated_rewards_DM = (q_hat_at_position * pi_e_at_position).sum(axis=1).squeeze()\n",
    "\n",
    "        # DR + IPS / NORM\n",
    "        # ips_reward_grad = ((iw @ (original_policy_rewards - q_hat_factual.squeeze())) * log_pi) / iw.mean()\n",
    "\n",
    "        # estimated_rewards =  ips_reward_grad + estimated_rewards_DM\n",
    "\n",
    "        # reinforce trick step\n",
    "        reinforce_grad = iw * original_policy_rewards * log_pi\n",
    "        \n",
    "        # r_hat_ipw = (iw * (original_policy_rewards - q_hat_factual))/ iw.mean()\n",
    "        # r_hat = r_hat_ipw + estimated_rewards_DM\n",
    "        \n",
    "        # reinforce_grad = r_hat * log_pi\n",
    "        \n",
    "        return -reinforce_grad.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_09:00.csv\n"
     ]
    }
   ],
   "source": [
    "result_path = Path(f\"./result/{datetime.now().strftime('%Y-%m-%d')}/train_data\")\n",
    "result_path.mkdir(parents=True, exist_ok=True)\n",
    "result_file_name = f\"result_{datetime.now().strftime('%H:00')}.csv\"\n",
    "curve_file_name = f\"curve_{datetime.now().strftime('%H:00')}.csv\"\n",
    "print(result_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_learning_trial(\n",
    "                      num_runs, \n",
    "                      num_neighbors, \n",
    "                      num_rounds_list, \n",
    "                      dataset\n",
    "                      ):\n",
    "    results = {}\n",
    "    dm = DM()\n",
    "    our_x, our_a, n_actions = dataset[\"our_x\"], dataset[\"our_a\"], dataset[\"n_actions\"]\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "            \n",
    "            pi_0 = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simluation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            dataset[\"n_users\"],\n",
    "                                                            dataset[\"n_actions\"],\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            \n",
    "            regression_model = RegressionModel(\n",
    "                                                n_actions=n_actions,\n",
    "                                                action_context=our_x,\n",
    "                                                base_model=LogisticRegression(random_state=12345)\n",
    "                                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "            regression_model.fit(train_data['x'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx'], train_data['a']].squeeze())\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = dict(\n",
    "                                policy_rewards=np.mean(conv_results[: ,0]),\n",
    "                                ipw=np.mean(conv_results[: ,3]),\n",
    "                                reg_dm=np.mean(reg_results),\n",
    "                                conv_dm=np.mean(conv_results[: ,1]), \n",
    "                                conv_dr=np.mean(conv_results[: ,2]),\n",
    "                                conv_sndr=np.mean(conv_results[: ,4]),\n",
    "                            )\n",
    "        \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = DM()\n",
    "# results = {}\n",
    "num_runs = 15\n",
    "batch_size = 50\n",
    "num_neighbors = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Noised Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = no_learning_trial(num_runs, num_neighbors, num_rounds_list, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1784</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.1721</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>0.1757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2089</td>\n",
       "      <td>0.1857</td>\n",
       "      <td>0.1437</td>\n",
       "      <td>0.1734</td>\n",
       "      <td>0.1816</td>\n",
       "      <td>0.1816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr\n",
       "15          0.2000 0.1784  0.1458   0.1721   0.1758     0.1757\n",
       "20          0.2089 0.1857  0.1437   0.1734   0.1816     0.1816"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = dataset.copy()\n",
    "temp_data['our_a'] = dataset[\"emb_a\"]\n",
    "temp_data['our_x'] = dataset[\"emb_x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = no_learning_trial(num_runs, num_neighbors, num_rounds_list, temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2124</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.1887</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>0.2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.2047</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.2066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr\n",
       "15          0.2124 0.2034  0.1454   0.1887   0.2079     0.2088\n",
       "20          0.2044 0.2047  0.1431   0.1913   0.2072     0.2066"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = random_.normal(size=(dataset_params[\"n_users\"], dataset_params[\"emb_dim\"]))\n",
    "new_a = random_.normal(size=(dataset_params[\"n_actions\"], dataset_params[\"emb_dim\"]))\n",
    "\n",
    "temp_data['our_a'] = new_a\n",
    "temp_data['our_x'] = new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = no_learning_trial(num_runs, num_neighbors, num_rounds_list, temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1373</td>\n",
       "      <td>0.1725</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>0.1640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>0.1396</td>\n",
       "      <td>0.1435</td>\n",
       "      <td>0.1441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr\n",
       "15          0.1373 0.1725  0.1456   0.1525   0.1635     0.1640\n",
       "20          0.1276 0.1406  0.1433   0.1396   0.1435     0.1441"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iterative_improvment = df3.copy()\n",
    "df_iterative_improvment['noise level'] = 10\n",
    "\n",
    "for i in range(1, 10):\n",
    "    temp_data['our_a'] = ((10-i) * new_a + i * dataset[\"emb_a\"]) /10\n",
    "    temp_data['our_x'] = ((10-i) * new_x + i * dataset[\"emb_x\"]) /10\n",
    "\n",
    "    rounds = [num_rounds_list[-1]]\n",
    "    df = no_learning_trial(num_runs, num_neighbors, rounds, temp_data)\n",
    "    df['noise level'] = 10 - i\n",
    "    df_iterative_improvment = pd.concat([df_iterative_improvment, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise level</th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>0.1396</td>\n",
       "      <td>0.1435</td>\n",
       "      <td>0.1441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>0.1412</td>\n",
       "      <td>0.1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>0.1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>0.1507</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>0.1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.1623</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>0.1581</td>\n",
       "      <td>0.1611</td>\n",
       "      <td>0.1611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>0.1659</td>\n",
       "      <td>0.1724</td>\n",
       "      <td>0.1724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1924</td>\n",
       "      <td>0.1803</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.1789</td>\n",
       "      <td>0.1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1960</td>\n",
       "      <td>0.1895</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2058</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.1868</td>\n",
       "      <td>0.1978</td>\n",
       "      <td>0.1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    noise level  policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr\n",
       "20           10          0.1276 0.1406  0.1433   0.1396   0.1435     0.1441\n",
       "20            9          0.1316 0.1398  0.1433   0.1404   0.1412     0.1414\n",
       "20            8          0.1458 0.1417  0.1433   0.1425   0.1421     0.1422\n",
       "20            7          0.1404 0.1465  0.1433   0.1461   0.1458     0.1458\n",
       "20            6          0.1640 0.1537  0.1433   0.1507   0.1513     0.1514\n",
       "20            5          0.1880 0.1623  0.1432   0.1581   0.1611     0.1611\n",
       "20            4          0.1898 0.1713  0.1432   0.1659   0.1724     0.1724\n",
       "20            3          0.1924 0.1803  0.1432   0.1742   0.1789     0.1790\n",
       "20            2          0.1960 0.1895  0.1432   0.1818   0.1875     0.1874\n",
       "20            1          0.2058 0.1979  0.1431   0.1868   0.1978     0.1975"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iterative_improvment.iloc[1:][['noise level', 'policy_rewards', 'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_data = 5\n",
    "\n",
    "new_dataset_params = dict(\n",
    "                    n_actions= 300,\n",
    "                    n_users = 300,\n",
    "                    emb_dim = 5,\n",
    "                    sigma = 0.1,\n",
    "                    eps = 0.4\n",
    "                    )\n",
    "\n",
    "new_dataset = generate_dataset(new_dataset_params)\n",
    "\n",
    "num_rounds_list = [3, 6, 10, 15, 20]\n",
    "num_runs = 2 # number of simulations\n",
    "max_iter = 25 # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.1796</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.1794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr\n",
       "20          0.2333 0.1796  0.1425   0.1708   0.1794     0.1794"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_learning_trial(num_runs, num_neighbors, rounds, new_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_runs = 3\n",
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the training function\n",
    "def train(model, train_loader, neighborhood_model, num_epochs=1, lr=0.0001):\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # here we can change the learning rate\n",
    "    criterion = PolicyLoss()\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    tq = tqdm(range(num_epochs))\n",
    "    for epoch in tq:\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for user_idx, action_idx, rewards, original_prob in train_loader:\n",
    "            # Move data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                user_idx = user_idx.to(device) \n",
    "                action_idx = action_idx.to(device)\n",
    "                rewards = rewards.to(device)\n",
    "                original_prob = original_prob.to(device) \n",
    "            \n",
    "            # Forward pass\n",
    "            policy = model(user_idx)\n",
    "            pscore = original_prob[torch.arange(user_idx.shape[0]), action_idx.type(torch.long)]\n",
    "            \n",
    "            scores = torch.tensor(neighborhood_model.predict(user_idx))\n",
    "            \n",
    "            loss = criterion(\n",
    "                              pscore,\n",
    "                              scores,\n",
    "                              policy, \n",
    "                              rewards, \n",
    "                              action_idx.type(torch.long), \n",
    "                              )\n",
    "            \n",
    "            # Zero the gradients Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()       \n",
    "            optimizer.step()\n",
    "            \n",
    "            # update neighborhood\n",
    "            action_emb, context_emb = model.get_params()\n",
    "            \n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "            # Print statistics after each epoch\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            tq.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "        neighborhood_model.update(action_emb.detach().numpy(), context_emb.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_trial(\n",
    "                  num_runs,\n",
    "                  num_neighbors,\n",
    "                  num_rounds_list,\n",
    "                  dataset,\n",
    "                  batch_size,\n",
    "                  num_epochs,\n",
    "                  lr=0.001\n",
    "                  ):\n",
    "    dm = DM()\n",
    "    results = {}\n",
    "\n",
    "    our_x, our_a = dataset[\"our_x\"], dataset[\"our_a\"]\n",
    "    emb_x, emb_a = dataset[\"emb_x\"], dataset[\"emb_a\"]\n",
    "    \n",
    "    original_x, original_a = dataset[\"original_x\"], dataset[\"original_a\"]\n",
    "    n_users, n_actions, emb_dim = dataset[\"n_users\"], dataset[\"n_actions\"], dataset[\"emb_dim\"]\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "\n",
    "            pi_0 = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            simulation_data = create_simluation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            dataset[\"n_users\"],\n",
    "                                                            dataset[\"n_actions\"],\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            \n",
    "            regression_model = RegressionModel(\n",
    "                                                n_actions=n_actions,\n",
    "                                                action_context=our_x,\n",
    "                                                base_model=LogisticRegression(random_state=12345)\n",
    "                                                )\n",
    "            \n",
    "            regression_model.fit(train_data['x'], \n",
    "                        train_data['a'],\n",
    "                        train_data['r'],\n",
    "                        original_policy_prob[train_data['x_idx'],\n",
    "                        train_data['a']].squeeze()\n",
    "                        )\n",
    "\n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            model = CFModel(\n",
    "                            n_users, \n",
    "                            n_actions, \n",
    "                            emb_dim, \n",
    "                            initial_user_embeddings=torch.tensor(our_x), \n",
    "                            initial_actions_embeddings=torch.tensor(our_a)\n",
    "                            )\n",
    "            \n",
    "            dataset =  CustomCFDataset(\n",
    "                                       train_data['x_idx'], \n",
    "                                       train_data['a'], \n",
    "                                       train_data['r'], \n",
    "                                       original_policy_prob[train_data['x_idx']]\n",
    "                                       )\n",
    "            \n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            train(model, train_loader, neighberhoodmodel, num_epochs=num_epochs, lr=lr)\n",
    "\n",
    "            our_a, our_x = model.get_params()\n",
    "            our_a, our_x = our_a.detach().numpy(), our_x.detach().numpy()\n",
    "\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.mean(np.abs(emb_a-our_a)), np.mean(np.abs(original_a-our_a))])\n",
    "            conv_results[-1] = np.append(conv_results[-1], [np.mean(np.abs(emb_x-our_x)), np.mean(np.abs(original_x-our_x))])\n",
    "            \n",
    "            our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = dict(\n",
    "                                    policy_rewards=np.mean(conv_results[: ,0]),\n",
    "                                    ipw=np.mean(conv_results[: ,3]),\n",
    "                                    reg_dm=np.mean(reg_results),\n",
    "                                    conv_dm=np.mean(conv_results[: ,1]), \n",
    "                                    conv_dr=np.mean(conv_results[: ,2]),\n",
    "                                    conv_sndr=np.mean(conv_results[: ,4]),\n",
    "                                    action_diff_to_real=np.mean(conv_results[: ,5]),\n",
    "                                    action_delta=np.mean(conv_results[: ,6]),\n",
    "                                    context_diff_to_real=np.mean(conv_results[: ,7]),\n",
    "                                    context_delta=np.mean(conv_results[: ,8])\n",
    "                                    )\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Reinforce grad (iw * log_p * R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.7502: 100%|██████████| 1/1 [00:05<00:00,  5.15s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df4 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 25\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[1;34m(num_runs, num_neighbors, num_rounds_list, dataset, batch_size, num_epochs, lr)\u001b[0m\n\u001b[0;32m     23\u001b[0m pi_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_x_a\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m/\u001b[39m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_actions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     24\u001b[0m original_policy_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(pi_0, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m simulation_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_simluation_data_from_pi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mpi_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mq_x_a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_users\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# test_data = get_test_data(dataset, simulation_data, n_test_data)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# idx = np.arange(train_size) + n_test_data\u001b[39;00m\n\u001b[0;32m     36\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(train_size)\n",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m, in \u001b[0;36mcreate_simluation_data_from_pi\u001b[1;34m(pi, q_x_a, n_users, n_actions, random_state)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_simluation_data_from_pi\u001b[39m(pi: np\u001b[38;5;241m.\u001b[39mndarray, q_x_a: np\u001b[38;5;241m.\u001b[39mndarray, n_users: np\u001b[38;5;241m.\u001b[39mint, n_actions: np\u001b[38;5;241m.\u001b[39mint, random_state: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12345\u001b[39m):\n\u001b[0;32m      2\u001b[0m     random_ \u001b[38;5;241m=\u001b[39m check_random_state(random_state)\n\u001b[1;32m----> 3\u001b[0m     simulation_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_users\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[0;32m      4\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mzeros((n_actions, n_users), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32), \n\u001b[0;32m      5\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39mzeros((n_actions, n_users)),\n\u001b[0;32m      6\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpscore\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39mzeros((n_actions, n_users))}\n\u001b[0;32m      8\u001b[0m     reward \u001b[38;5;241m=\u001b[39m q_x_a  \u001b[38;5;241m>\u001b[39m random_\u001b[38;5;241m.\u001b[39mrandom(size\u001b[38;5;241m=\u001b[39mq_x_a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      9\u001b[0m     simulation_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpi_0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pi\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "df4 = trainer_trial(num_runs, num_neighbors, num_rounds_list, dataset, batch_size+100, num_epochs=1, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.3591</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.6052</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.1462</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>0.1275</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>0.3591</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.6058</td>\n",
       "      <td>0.0048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.1589</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>0.6068</td>\n",
       "      <td>0.0097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1427</td>\n",
       "      <td>0.1549</td>\n",
       "      <td>0.1558</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.6069</td>\n",
       "      <td>0.0138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2267</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>0.1602</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.0183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "3           0.1867 0.1471  0.1130   0.1152   0.1563     0.1590   \n",
       "6           0.1933 0.1462  0.1288   0.1275   0.1548     0.1567   \n",
       "10          0.1933 0.1456  0.1357   0.1399   0.1576     0.1589   \n",
       "15          0.2000 0.1460  0.1401   0.1427   0.1549     0.1558   \n",
       "20          0.2267 0.1465  0.1465   0.1508   0.1596     0.1602   \n",
       "\n",
       "    action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "3                0.3591        0.0109                0.6052         0.0021  \n",
       "6                0.3591        0.0185                0.6058         0.0048  \n",
       "10               0.3580        0.0267                0.6068         0.0097  \n",
       "15               0.3578        0.0312                0.6069         0.0138  \n",
       "20               0.3548        0.0360                0.6071         0.0183  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less epochs, different lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.5877: 100%|██████████| 5/5 [00:12<00:00,  2.53s/it]\n",
      "Epoch [5/5], Loss: 0.6207: 100%|██████████| 5/5 [00:53<00:00, 10.63s/it]\n",
      "Epoch [5/5], Loss: 0.6579: 100%|██████████| 5/5 [02:32<00:00, 30.59s/it]\n",
      "Epoch [5/5], Loss: 0.6655: 100%|██████████| 5/5 [06:21<00:00, 76.34s/it]\n",
      "Epoch [5/5], Loss: 0.6819: 100%|██████████| 5/5 [11:16<00:00, 135.31s/it]\n"
     ]
    }
   ],
   "source": [
    "df5 = trainer_trial(num_runs, num_neighbors, num_rounds_list, dataset, batch_size+100, num_epochs=5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.1468</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.3592</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.6053</td>\n",
       "      <td>0.0026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.1561</td>\n",
       "      <td>0.1581</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.6059</td>\n",
       "      <td>0.0062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>0.1552</td>\n",
       "      <td>0.1564</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>0.6070</td>\n",
       "      <td>0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2133</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.6074</td>\n",
       "      <td>0.0153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2133</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.1612</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.3554</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.0184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "3           0.1933 0.1468  0.1130   0.1143   0.1554     0.1582   \n",
       "6           0.1933 0.1455  0.1288   0.1263   0.1561     0.1581   \n",
       "10          0.1933 0.1448  0.1357   0.1378   0.1552     0.1564   \n",
       "15          0.2133 0.1466  0.1401   0.1410   0.1545     0.1554   \n",
       "20          0.2133 0.1470  0.1465   0.1491   0.1612     0.1620   \n",
       "\n",
       "    action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "3                0.3592        0.0111                0.6053         0.0026  \n",
       "6                0.3585        0.0168                0.6059         0.0062  \n",
       "10               0.3578        0.0218                0.6070         0.0111  \n",
       "15               0.3569        0.0242                0.6074         0.0153  \n",
       "20               0.3554        0.0262                0.6077         0.0184  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.4526: 100%|██████████| 10/10 [00:20<00:00,  2.06s/it]\n",
      "Epoch [10/10], Loss: 0.4421: 100%|██████████| 10/10 [00:38<00:00,  3.81s/it]\n",
      "Epoch [10/10], Loss: 0.4193: 100%|██████████| 10/10 [01:08<00:00,  6.81s/it]\n",
      "Epoch [10/10], Loss: 0.3979: 100%|██████████| 10/10 [01:47<00:00, 10.77s/it]\n",
      "Epoch [10/10], Loss: 0.3802: 100%|██████████| 10/10 [02:43<00:00, 16.36s/it]\n"
     ]
    }
   ],
   "source": [
    "df6 = trainer_trial(num_runs, num_neighbors, num_rounds_list, dataset, batch_size+100, num_epochs=10, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "      <th>action_diff_to_real</th>\n",
       "      <th>action_delta</th>\n",
       "      <th>context_diff_to_real</th>\n",
       "      <th>context_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>0.1555</td>\n",
       "      <td>0.3555</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.6076</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.1274</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1436</td>\n",
       "      <td>0.1481</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>0.0993</td>\n",
       "      <td>0.6131</td>\n",
       "      <td>0.0368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1308</td>\n",
       "      <td>0.1344</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1391</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.1253</td>\n",
       "      <td>0.6220</td>\n",
       "      <td>0.0647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.3609</td>\n",
       "      <td>0.1405</td>\n",
       "      <td>0.6298</td>\n",
       "      <td>0.0886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1468</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>0.6368</td>\n",
       "      <td>0.1086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards    ipw  reg_dm  conv_dm  conv_dr  conv_sndr  \\\n",
       "3           0.2067 0.1410  0.1126   0.0978   0.1510     0.1555   \n",
       "6           0.1867 0.1335  0.1274   0.0981   0.1436     0.1481   \n",
       "10          0.2000 0.1308  0.1344   0.0978   0.1354     0.1391   \n",
       "15          0.1933 0.1408  0.1398   0.0926   0.1418     0.1458   \n",
       "20          0.2067 0.1465  0.1468   0.0942   0.1588     0.1614   \n",
       "\n",
       "    action_diff_to_real  action_delta  context_diff_to_real  context_delta  \n",
       "3                0.3555        0.0674                0.6076         0.0157  \n",
       "6                0.3575        0.0993                0.6131         0.0368  \n",
       "10               0.3596        0.1253                0.6220         0.0647  \n",
       "15               0.3609        0.1405                0.6298         0.0886  \n",
       "20               0.3585        0.1499                0.6368         0.1086  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13069560060859173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.5961: 100%|██████████| 10/10 [04:25<00:00, 26.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1316379049999357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.5186: 100%|██████████| 10/10 [26:42<00:00, 160.28s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.03 GiB for an array with shape (450000, 1800) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df6 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_policy_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mour_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mour_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 45\u001b[0m, in \u001b[0;36mtrainer_trial\u001b[1;34m(original_policy_prob, simulation_data, num_runs, num_neighbors, num_rounds_list, our_x, our_a, batch_size, num_epochs, lr)\u001b[0m\n\u001b[0;32m     42\u001b[0m reg_dm \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mestimate_policy_value(policy[test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]], regression_model\u001b[38;5;241m.\u001b[39mpredict(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     43\u001b[0m reg_results\u001b[38;5;241m.\u001b[39mappend(reg_dm)\n\u001b[1;32m---> 45\u001b[0m conv_results\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighberhoodmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_policy_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(conv_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(emb_a\u001b[38;5;241m-\u001b[39mour_a)), np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(original_a\u001b[38;5;241m-\u001b[39mour_a))])\n\u001b[0;32m     48\u001b[0m our_a, our_x \u001b[38;5;241m=\u001b[39m original_a\u001b[38;5;241m.\u001b[39mcopy(), original_x\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\my_utils.py:105\u001b[0m, in \u001b[0;36meval_policy\u001b[1;34m(model, test_data, original_policy_prob, policy)\u001b[0m\n\u001b[0;32m    102\u001b[0m ipw \u001b[38;5;241m=\u001b[39m IPW()\n\u001b[0;32m    103\u001b[0m sndr \u001b[38;5;241m=\u001b[39m SNDR()\n\u001b[1;32m--> 105\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m policy \u001b[38;5;241m=\u001b[39m policy[test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m    108\u001b[0m actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39margmax(policy, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\my_utils.py:92\u001b[0m, in \u001b[0;36mNeighborhoodModel.predict\u001b[1;34m(self, test_context, chunksize)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# chunksize is for how concurrently we can run things.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start \u001b[38;5;241m<\u001b[39m test_context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m---> 92\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_convolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_context\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     93\u001b[0m     start \u001b[38;5;241m=\u001b[39m end\n\u001b[0;32m     94\u001b[0m     end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(end \u001b[38;5;241m+\u001b[39m chunksize, test_context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\my_utils.py:82\u001b[0m, in \u001b[0;36mNeighborhoodModel.context_convolve\u001b[1;34m(self, test_context)\u001b[0m\n\u001b[0;32m     79\u001b[0m all_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_similarity\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, test_context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     80\u001b[0m all_actions \u001b[38;5;241m=\u001b[39m all_actions\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m---> 82\u001b[0m eta_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m eta_all \u001b[38;5;241m=\u001b[39m eta_all\u001b[38;5;241m.\u001b[39mreshape(test_context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_similarity\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eta_all\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPC\\my_utils.py:66\u001b[0m, in \u001b[0;36mNeighborhoodModel.convolve\u001b[1;34m(self, test_actions, test_context)\u001b[0m\n\u001b[0;32m     63\u001b[0m cosine_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_similarity[np\u001b[38;5;241m.\u001b[39mint32(test_context)][:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext]\n\u001b[0;32m     64\u001b[0m cosine_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_similarity[np\u001b[38;5;241m.\u001b[39mint32(test_actions)][:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions]\n\u001b[1;32m---> 66\u001b[0m tot_cosine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m cosine_actions \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcosine_context\u001b[49m\n\u001b[0;32m     67\u001b[0m top_n_tot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(tot_cosine, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_neighbors:]\n\u001b[0;32m     68\u001b[0m similarity \u001b[38;5;241m=\u001b[39m tot_cosine[np\u001b[38;5;241m.\u001b[39marange(tot_cosine\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m], top_n_tot]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.03 GiB for an array with shape (450000, 1800) and data type float64"
     ]
    }
   ],
   "source": [
    "df7 = trainer_trial(num_runs, num_neighbors, num_rounds_list[:-3], dataset, batch_size+100, num_epochs=10, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf6\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df6' is not defined"
     ]
    }
   ],
   "source": [
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original_policy_prob = np.expand_dims(softmax(original_x @ original_a.T, axis=1), -1)\n",
    "# original_policy_prob = np.expand_dims(np.ones_like(q_x_a) / (n_actions), -1)\n",
    "# dm = DM()\n",
    "# results = {}\n",
    "# num_runs = 1\n",
    "# batch_size = 50\n",
    "# num_neighbors = 4\n",
    "\n",
    "# for train_size in num_rounds_list:\n",
    "#     reg_results, conv_results = [], []\n",
    "#     for run in range(num_runs):\n",
    "\n",
    "#         idx = np.arange(train_size) + n_test_data\n",
    "#         train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x, q_x_a)\n",
    "        \n",
    "#         regression_model = RegressionModel(\n",
    "#                                             n_actions=n_actions,\n",
    "#                                             action_context=emb_x,\n",
    "#                                             base_model=LogisticRegression(random_state=12345)\n",
    "#                                             )\n",
    "        \n",
    "#         neighberhoodmodel = NeighborhoodModel(\n",
    "#                                                 train_data['x_idx'],\n",
    "#                                                 train_data['a'], \n",
    "#                                                 our_a,\n",
    "#                                                 our_x, \n",
    "#                                                 train_data['r'], \n",
    "#                                                 num_neighbors=num_neighbors\n",
    "#                                             )\n",
    "        \n",
    "#         regression_model.fit(train_data['x'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx'], train_data['a']].squeeze())\n",
    "\n",
    "#         # model = CFModel(n_users, n_actions, emb_dim, initial_user_embeddings=torch.tensor(our_x), initial_actions_embeddings=torch.tensor(our_a))\n",
    "#         # dataset =  CustomCFDataset(train_data['x_idx'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx']])\n",
    "#         # train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#         # adam = optim.Adam(model.parameters())\n",
    "#         # loss_fn = PolicyLoss()\n",
    "        \n",
    "#         # train(model, train_loader, loss_fn, adam, neighberhoodmodel)\n",
    "\n",
    "#         # our_a, our_x = model.get_params()\n",
    "#         # our_a, our_x = our_a.detach().numpy(), our_x.detach().numpy()\n",
    "\n",
    "#         # print(np.mean(np.abs(emb_a-our_a)))\n",
    "#         # print(np.mean(np.abs(original_a-our_a)))\n",
    "\n",
    "#         policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "#         reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "#         reg_results.append(reg_dm)\n",
    "\n",
    "#         conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "        \n",
    "#         # conv_results[-1] = np.append(conv_results[-1], [np.mean(np.abs(emb_a-our_a)), np.mean(np.abs(original_a-our_a))])\n",
    "#         # our_a, our_x = original_a.copy(), original_x.copy()\n",
    "\n",
    "#     reg_results = np.array(reg_results)\n",
    "#     conv_results = np.array(conv_results)\n",
    "\n",
    "#     results[train_size] = dict(\n",
    "#                                 # reg_rewards=np.mean(reg_results[: ,0]),\n",
    "#                                 # reg_dm=np.mean(reg_results[: ,1]), \n",
    "#                                 # reg_dr=np.mean(reg_results[: ,2]),\n",
    "#                                 # reg_ipw=np.mean(reg_results[: ,3]),\n",
    "#                                 # reg_sndr=np.mean(reg_results[: ,4]),\n",
    "#                                 # reg_var_dm=np.std(reg_results[: ,1] - reg_results[: ,0]),\n",
    "#                                 # reg_var_dr=np.std(reg_results[: ,2] - reg_results[: ,0]),\n",
    "#                                 # reg_var_ipw=np.std(reg_results[: ,3] - reg_results[: ,0]),\n",
    "#                                 # reg_var_sndr=np.std(reg_results[: ,4] - reg_results[: ,0]),\n",
    "#                                 policy_rewards=np.mean(conv_results[: ,0]),\n",
    "#                                 ipw=np.mean(conv_results[: ,3]),\n",
    "#                                 reg_dm=np.mean(reg_results),\n",
    "\n",
    "#                                 conv_dm=np.mean(conv_results[: ,1]), \n",
    "#                                 conv_dr=np.mean(conv_results[: ,2]),\n",
    "#                                 conv_sndr=np.mean(conv_results[: ,4]),\n",
    "#                                 # diff_to_real=np.mean(conv_results[: ,5]),\n",
    "#                                 # diff_from_start=np.mean(conv_results[: ,6])\n",
    "#                                 # conv_var_dm=np.std(conv_results[: ,1] - conv_results[: ,0]),\n",
    "#                                 # conv_var_dr=np.std(conv_results[: ,2] - conv_results[: ,0]),\n",
    "#                                 # conv_var_ipw=np.std(conv_results[: ,3] - conv_results[: ,0]),\n",
    "#                                 # conv_var_sndr=np.std(conv_results[: ,4] - conv_results[: ,0]),\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[43mresults\u001b[49m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_rewards</th>\n",
       "      <th>ipw</th>\n",
       "      <th>reg_dm</th>\n",
       "      <th>conv_dm</th>\n",
       "      <th>conv_dr</th>\n",
       "      <th>conv_sndr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.149066</td>\n",
       "      <td>0.153500</td>\n",
       "      <td>0.209685</td>\n",
       "      <td>0.209908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.152306</td>\n",
       "      <td>0.164966</td>\n",
       "      <td>0.199016</td>\n",
       "      <td>0.199152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.159112</td>\n",
       "      <td>0.173460</td>\n",
       "      <td>0.198981</td>\n",
       "      <td>0.199082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.145156</td>\n",
       "      <td>0.159052</td>\n",
       "      <td>0.200623</td>\n",
       "      <td>0.200788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.144494</td>\n",
       "      <td>0.158721</td>\n",
       "      <td>0.191275</td>\n",
       "      <td>0.191405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    policy_rewards       ipw    reg_dm   conv_dm   conv_dr  conv_sndr\n",
       "3             0.56  0.199051  0.149066  0.153500  0.209685   0.209908\n",
       "6             0.56  0.199051  0.152306  0.164966  0.199016   0.199152\n",
       "10            0.56  0.199051  0.159112  0.173460  0.198981   0.199082\n",
       "15            0.56  0.199051  0.145156  0.159052  0.200623   0.200788\n",
       "20            0.56  0.199051  0.144494  0.158721  0.191275   0.191405"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'diff_to_real'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'diff_to_real'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetting closer to real emb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiff_to_real\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiff_to_real\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\roeed\\PycharmProjects\\OPE\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'diff_to_real'"
     ]
    }
   ],
   "source": [
    "df['getting closer to real emb'] = df['diff_to_real'].max() - df['diff_to_real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(y=['getting closer to real emb', 'diff_from_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
