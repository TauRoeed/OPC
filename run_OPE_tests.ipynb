{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# implementing OPE of the IPWLearner using synthetic bandit data\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "# from obp.ope import (\n",
    "    # OffPolicyEvaluation,\n",
    "    # RegressionModel,\n",
    "    # InverseProbabilityWeighting as IPW,\n",
    "    # DirectMethod as DM,\n",
    "    # DoublyRobust as DR,\n",
    "    # SelfNormalizedDoublyRobust as SNDR\n",
    "# )\n",
    "\n",
    "from from_saito import (\n",
    "    DirectMethod as DM,\n",
    ")\n",
    "\n",
    "\n",
    "from my_utils import (\n",
    "    eval_policy,\n",
    "    generate_dataset,\n",
    "    create_simulation_data_from_pi,\n",
    "    get_train_data,\n",
    "    NeighborhoodModel\n",
    ")\n",
    "random_state=12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(dataset, policy):\n",
    "    return np.array([np.sum(dataset['q_x_a'] * policy.squeeze(), axis=1).mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ope_results_dict(reg_results, conv_results):\n",
    "    reward = conv_results[:, 0]\n",
    "    return dict(\n",
    "                policy_rewards=reward.mean(),\n",
    "                ipw=np.mean(abs(conv_results[: ,3] - reward)),\n",
    "                reg_dm=np.mean(abs(reg_results - reward)),\n",
    "                conv_dm=np.mean(abs(conv_results[: ,1] - reward)),\n",
    "                conv_dr=np.mean(abs(conv_results[: ,2] - reward)),\n",
    "                conv_sndr=np.mean(abs(conv_results[: ,4] - reward)),\n",
    "                \n",
    "                ipw_p_err=np.mean(abs(conv_results[: ,3] - reward) / reward) * 100,\n",
    "                reg_dm_p_err=np.mean(abs(reg_results - reward) / reward) * 100,\n",
    "                conv_dm_p_err=np.mean(abs(conv_results[: ,1] - reward) / reward) * 100,\n",
    "                conv_dr_p_err=np.mean(abs(conv_results[: ,2] - reward) / reward) * 100,\n",
    "                conv_sndr_p_err=np.mean(abs(conv_results[: ,4] - reward) / reward) * 100,\n",
    "                \n",
    "                ipw_var=np.std(conv_results[: ,3]),\n",
    "                reg_dm_var=np.std(reg_results),\n",
    "                conv_dm_var=np.std(conv_results[: ,1]),\n",
    "                conv_dr_var=np.std(conv_results[: ,2]),\n",
    "                conv_sndr_var=np.std(conv_results[: ,4]),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    for val in [\"ipw\", \"reg_dm\", \"conv_dm\", \"conv_dr\", \"conv_sndr\"]:\n",
    "        ci = df[val + '_var']\n",
    "        ax.plot(df.index, df[val], label=val)\n",
    "        ax.fill_between(df.index, (df[val]-ci), (df[val]+ci), alpha=.1, linestyle='--')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Number of training data\")\n",
    "    plt.ylabel(\"Policy value error\")\n",
    "    plt.title(\"IPW vs DM vs DR vs SNDR Errors\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    df.plot(\n",
    "    y=[\"ipw_p_err\", \"reg_dm_p_err\", \"conv_dm_p_err\", \"conv_dr_p_err\", \"conv_sndr_p_err\"],\n",
    "    title=\"IPW vs DM vs DR vs SNDR Errors in percentage\",\n",
    "    xlabel=\"Number of training data\",\n",
    "    figsize=(10, 5),\n",
    "    marker=\"o\",\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_11:00.csv\n"
     ]
    }
   ],
   "source": [
    "result_path = Path(f\"./result/{datetime.now().strftime('%Y-%m-%d')}/train_data\")\n",
    "result_path.mkdir(parents=True, exist_ok=True)\n",
    "result_file_name = f\"result_{datetime.now().strftime('%H:00')}.csv\"\n",
    "curve_file_name = f\"curve_{datetime.now().strftime('%H:00')}.csv\"\n",
    "print(result_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run several simulations on a generated dataset, the dataset is generated like this:\n",
    "$$ \\text{We have users U and actions A } u_i \\sim N(0, I_{emb_dim}) \\ a_i \\sim N(0, I_{emb_dim})$$\n",
    "$$ p_{ij} = 1 / (5 + e^{-(u_i.T a_j)}) $$\n",
    "$$r_{ij} \\sim Bin(p_{ij})$$\n",
    "\n",
    "We have a policy $\\pi$\n",
    "and it's ground truth reward is calculated by\n",
    "$$R_{gt} = \\sum_{i}{\\sum_{j}{\\pi_{ij} * p_{ij}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_learning_trial(\n",
    "                      num_runs, \n",
    "                      num_neighbors, \n",
    "                      num_rounds_list, \n",
    "                      dataset\n",
    "                      ):\n",
    "    results = {}\n",
    "    dm = DM()\n",
    "    our_x, our_a, n_actions = dataset[\"our_x\"], dataset[\"our_a\"], dataset[\"n_actions\"]\n",
    "\n",
    "    for train_size in num_rounds_list:\n",
    "        reg_results, conv_results = [], []\n",
    "        for run in range(num_runs):\n",
    "            \n",
    "            greedy = np.ones_like(dataset[\"q_x_a\"])/(dataset[\"n_actions\"])\n",
    "            pi_0 = softmax(our_x @ our_a.T, axis=1)\n",
    "\n",
    "            pi_0 = 0.8 * pi_0 + 0.2 * greedy\n",
    "            simulation_data = create_simulation_data_from_pi(\n",
    "                                                            pi_0,\n",
    "                                                            dataset[\"q_x_a\"],\n",
    "                                                            dataset[\"n_users\"],\n",
    "                                                            dataset[\"n_actions\"],\n",
    "                                                            random_state=train_size*(run+1)\n",
    "                                                            )\n",
    "            \n",
    "            # test_data = get_test_data(dataset, simulation_data, n_test_data)\n",
    "            \n",
    "            # idx = np.arange(train_size) + n_test_data\n",
    "            idx = np.arange(train_size)\n",
    "            train_data = get_train_data(n_actions, train_size, simulation_data, idx, our_x)\n",
    "            \n",
    "            regression_model = RegressionModel(\n",
    "                                                n_actions=n_actions,\n",
    "                                                action_context=our_x,\n",
    "                                                base_model=LogisticRegression(random_state=12345)\n",
    "                                                )\n",
    "            \n",
    "            neighberhoodmodel = NeighborhoodModel(\n",
    "                                                    train_data['x_idx'],\n",
    "                                                    train_data['a'], \n",
    "                                                    our_a,\n",
    "                                                    our_x, \n",
    "                                                    train_data['r'], \n",
    "                                                    num_neighbors=num_neighbors\n",
    "                                                )\n",
    "            \n",
    "            original_policy_prob = np.expand_dims(pi_0, -1)\n",
    "            regression_model.fit(train_data['x'], train_data['a'], train_data['r'], original_policy_prob[train_data['x_idx'], train_data['a']].squeeze())\n",
    "            policy = np.expand_dims(softmax(our_x @ our_a.T, axis=1), -1)\n",
    "\n",
    "            # reg_dm = dm.estimate_policy_value(policy[test_data['x_idx']], regression_model.predict(test_data['x']))\n",
    "            reg_dm = dm.estimate_policy_value(policy[train_data['x_idx']], regression_model.predict(train_data['x']))\n",
    "\n",
    "            reg_results.append(reg_dm)\n",
    "\n",
    "            # conv_results.append(eval_policy(neighberhoodmodel, test_data, original_policy_prob[test_data['x_idx']], policy))\n",
    "            conv_results.append(eval_policy(neighberhoodmodel, train_data, original_policy_prob[train_data['x_idx']], policy))\n",
    "            conv_results[-1] = np.append(calc_reward(dataset, policy), conv_results[-1])\n",
    "\n",
    "        reg_results = np.array(reg_results)\n",
    "        conv_results = np.array(conv_results)\n",
    "\n",
    "        results[train_size] = get_ope_results_dict(reg_results, conv_results)\n",
    "        \n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_data = 5\n",
    "\n",
    "dataset_params = dict(\n",
    "                    n_actions= 150,\n",
    "                    n_users = 150,\n",
    "                    emb_dim = 5,\n",
    "                    # sigma = 0.1,\n",
    "                    eps = 0.2\n",
    "                    )\n",
    "\n",
    "dataset = generate_dataset(dataset_params)\n",
    "\n",
    "# num_rounds_list = [3]\n",
    "num_rounds_list = [1, 2, 3, 4, 5, 10, 20]\n",
    "\n",
    "# num_runs = 5 # number of simulations\n",
    "max_iter = 25 # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = DM()\n",
    "# results = {}\n",
    "num_runs = 10\n",
    "batch_size = 50\n",
    "num_neighbors = 6\n",
    "# num_rounds_list = [3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the parameters:\n",
    "$$EmbDim = 5$$\n",
    "$$NumActions= 150$$\n",
    "$$NumUsers = 150$$\n",
    "$$NeighborhoodSize = 6$$\n",
    "and we run 10 trials for stability and variance calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the policy $\\pi$\n",
    "We will use an initial policy : $$\\pi_{start} = (1-\\epsilon)*\\pi + \\epsilon * greedy$$ to sample examples from the data\n",
    "and then use ope tools and the convolution model to calculate $\\hat{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Noised Policy\n",
    "This is the first policy we will test out, the policy is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$emb = 0.8 * gt + 0.2 * noise$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_simluation_data_from_pi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mno_learning_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36mno_learning_trial\u001b[0;34m(num_runs, num_neighbors, num_rounds_list, dataset)\u001b[0m\n\u001b[1;32m     16\u001b[0m pi_0 \u001b[38;5;241m=\u001b[39m softmax(our_x \u001b[38;5;241m@\u001b[39m our_a\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m pi_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m pi_0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m greedy\n\u001b[0;32m---> 19\u001b[0m simulation_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_simluation_data_from_pi\u001b[49m(\n\u001b[1;32m     20\u001b[0m                                                 pi_0,\n\u001b[1;32m     21\u001b[0m                                                 dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_x_a\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     22\u001b[0m                                                 dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_users\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m                                                 dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_actions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m                                                 random_state\u001b[38;5;241m=\u001b[39mtrain_size\u001b[38;5;241m*\u001b[39m(run\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m                                                 )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# test_data = get_test_data(dataset, simulation_data, n_test_data)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# idx = np.arange(train_size) + n_test_data\u001b[39;00m\n\u001b[1;32m     30\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(train_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_simluation_data_from_pi' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = no_learning_trial(num_runs, num_neighbors, num_rounds_list, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$emb = gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = dataset.copy()\n",
    "temp_data['our_a'] = dataset[\"emb_a\"]\n",
    "temp_data['our_x'] = dataset[\"emb_x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = no_learning_trial(num_runs, num_neighbors, num_rounds_list, temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$emb = noise$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = random_.normal(size=(dataset_params[\"n_users\"], dataset_params[\"emb_dim\"]))\n",
    "new_a = random_.normal(size=(dataset_params[\"n_actions\"], dataset_params[\"emb_dim\"]))\n",
    "\n",
    "temp_data['our_a'] = new_a\n",
    "temp_data['our_x'] = new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = no_learning_trial(num_runs, num_neighbors, num_rounds_list, temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative improvement\n",
    "At each iteration the model embeddings get closer to the ground truth - $$emb = (1-\\epsilon_t) * gt + \\epsilon_t * noise$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iterative_improvment = df3.iloc[5:].copy()\n",
    "df_iterative_improvment['noise level'] = 10\n",
    "\n",
    "for i in range(1, 11):\n",
    "    temp_data['our_a'] = ((10-i) * new_a + i * dataset[\"emb_a\"]) /10\n",
    "    temp_data['our_x'] = ((10-i) * new_x + i * dataset[\"emb_x\"]) /10\n",
    "\n",
    "    rounds = [num_rounds_list[-1]]\n",
    "    df = no_learning_trial(num_runs, num_neighbors, rounds, temp_data)\n",
    "    df['noise level'] = 10 - i\n",
    "    df_iterative_improvment = pd.concat([df_iterative_improvment, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_for_display = ['noise level', 'policy_rewards', \n",
    "                    'ipw', 'reg_dm', 'conv_dm', 'conv_dr', 'conv_sndr',\n",
    "                    'ipw_var', 'reg_dm_var', 'conv_dm_var', 'conv_dr_var', 'conv_sndr_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iterative_improvment.iloc[1:][keys_for_display[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iterative_improvment.set_index('noise level', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iterative_improvment.iloc[1:].plot(\n",
    "    y=[\"ipw_p_err\",\t\"reg_dm_p_err\",\t\"conv_dm_p_err\", \"conv_dr_p_err\", \"conv_sndr_p_err\"],\n",
    "    title=\"IPW vs DM vs DR vs SNDR Errors\",\n",
    "    figsize=(10, 5),\n",
    "    marker=\"o\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opl-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
